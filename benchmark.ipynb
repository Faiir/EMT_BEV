{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging \n",
    "from custome_logger import setup_custom_logger\n",
    "logger = setup_custom_logger()\n",
    "logger.debug(\"test\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.benchmark as benchmark\n",
    "from timeit import default_timer as timer\n",
    "import warnings\n",
    "from mmcv import Config, DictAction\n",
    "from mmcv.runner import get_dist_info, init_dist\n",
    "from mmcv.runner import load_checkpoint, wrap_fp16_model\n",
    "from os import path as osp\n",
    "from mmdet3d.models import build_model\n",
    "from mmdet3d.datasets import build_dataset\n",
    "from mmcv.parallel import MMDataParallel\n",
    "from mmdet.datasets import (build_dataloader, build_dataset,\n",
    "                            replace_ImageToTensor)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects.mmdet3d_plugin\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "cfg = Config.fromfile(\n",
    "    r\"/home/niklas/ETM_BEV/BEVerse/projects/configs/beverse_tiny.py\"\n",
    ")\n",
    "\n",
    "# if args.cfg_options is not None:\n",
    "#     cfg.merge_from_dict(args.cfg_options)\n",
    "# import modules from string list.\n",
    "if cfg.get(\"custom_imports\", None):\n",
    "    from mmcv.utils import import_modules_from_strings\n",
    "\n",
    "    import_modules_from_strings(**cfg[\"custom_imports\"])\n",
    "\n",
    "# import modules from plguin/xx, registry will be updated\n",
    "if hasattr(cfg, \"plugin\"):\n",
    "    if cfg.plugin:\n",
    "        import importlib\n",
    "\n",
    "        if hasattr(cfg, \"plugin_dir\"):\n",
    "            plugin_dir = cfg.plugin_dir\n",
    "            _module_dir = os.path.dirname(plugin_dir)\n",
    "            _module_dir = _module_dir.split(\"/\")\n",
    "            _module_path = _module_dir[0]\n",
    "\n",
    "            for m in _module_dir[1:]:\n",
    "                _module_path = _module_path + \".\" + m\n",
    "            print(_module_path)\n",
    "            plg_lib = importlib.import_module(_module_path)\n",
    "        else:\n",
    "            # import dir is the dirpath for the config file\n",
    "            _module_dir = os.path.dirname(\n",
    "                r\"/home/niklas/ETM_BEV/BEVerse/projects/configs/beverse_tiny.py\"\n",
    "            )\n",
    "            _module_dir = _module_dir.split(\"/\")\n",
    "            _module_path = _module_dir[0]\n",
    "            for m in _module_dir[1:]:\n",
    "                _module_path = _module_path + \".\" + m\n",
    "            print(_module_path)\n",
    "            plg_lib = importlib.import_module(_module_path)\n",
    "            \n",
    "\n",
    "samples_per_gpu = 1\n",
    "if isinstance(cfg.data.test, dict):\n",
    "    cfg.data.test.test_mode = True\n",
    "    samples_per_gpu = cfg.data.test.pop(\"samples_per_gpu\", 1)\n",
    "    if samples_per_gpu > 1:\n",
    "        # Replace 'ImageToTensor' to 'DefaultFormatBundle'\n",
    "        cfg.data.test.pipeline = replace_ImageToTensor(cfg.data.test.pipeline)\n",
    "elif isinstance(cfg.data.test, list):\n",
    "    for ds_cfg in cfg.data.test:\n",
    "        ds_cfg.test_mode = True\n",
    "    samples_per_gpu = max(\n",
    "        [ds_cfg.pop(\"samples_per_gpu\", 1) for ds_cfg in cfg.data.test]\n",
    "    )\n",
    "    if samples_per_gpu > 1:\n",
    "        for ds_cfg in cfg.data.test:\n",
    "            ds_cfg.pipeline = replace_ImageToTensor(ds_cfg.pipeline)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def measure_time_host(\n",
    "    model: nn.Module,\n",
    "    input_tensor: torch.Tensor,\n",
    "    num_repeats: int = 100,\n",
    "    num_warmups: int = 10,\n",
    "    synchronize: bool = True,\n",
    "    continuous_measure: bool = True,\n",
    ") -> float:\n",
    "\n",
    "    for _ in range(num_warmups):\n",
    "        _ = model.forward(input_tensor)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    elapsed_time_ms = 0\n",
    "\n",
    "    if continuous_measure:\n",
    "        start = timer()\n",
    "        for _ in range(num_repeats):\n",
    "            _ = model.forward(input_tensor)\n",
    "        if synchronize:\n",
    "            torch.cuda.synchronize()\n",
    "        end = timer()\n",
    "        elapsed_time_ms = (end - start) * 1000\n",
    "\n",
    "    else:\n",
    "        for _ in range(num_repeats):\n",
    "            start = timer()\n",
    "            _ = model.forward(input_tensor)\n",
    "            if synchronize:\n",
    "                torch.cuda.synchronize()\n",
    "            end = timer()\n",
    "            elapsed_time_ms += (end - start) * 1000\n",
    "\n",
    "    return elapsed_time_ms / num_repeats\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def measure_time_device(\n",
    "    model: nn.Module,\n",
    "    input_tensor: torch.Tensor,\n",
    "    num_repeats: int = 100,\n",
    "    num_warmups: int = 10,\n",
    "    synchronize: bool = True,\n",
    "    continuous_measure: bool = True,\n",
    ") -> float:\n",
    "\n",
    "    for _ in range(num_warmups):\n",
    "        _ = model.forward(input_tensor)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    elapsed_time_ms = 0\n",
    "\n",
    "    if continuous_measure:\n",
    "        start_event = torch.cuda.Event(enable_timing=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True)\n",
    "        start_event.record()\n",
    "        for _ in range(num_repeats):\n",
    "            _ = model.forward(input_tensor)\n",
    "        end_event.record()\n",
    "        if synchronize:\n",
    "            # This has to be synchronized to compute the elapsed time.\n",
    "            # Otherwise, there will be runtime error.\n",
    "            torch.cuda.synchronize()\n",
    "        elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "\n",
    "    else:\n",
    "        for _ in range(num_repeats):\n",
    "            start_event = torch.cuda.Event(enable_timing=True)\n",
    "            end_event = torch.cuda.Event(enable_timing=True)\n",
    "            start_event.record()\n",
    "            _ = model.forward(input_tensor)\n",
    "            end_event.record()\n",
    "            if synchronize:\n",
    "                # This has to be synchronized to compute the elapsed time.\n",
    "                # Otherwise, there will be runtime error.\n",
    "                torch.cuda.synchronize()\n",
    "            elapsed_time_ms += start_event.elapsed_time(end_event)\n",
    "\n",
    "    return elapsed_time_ms / num_repeats\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_inference(model: nn.Module, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "    return model.forward(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/niklas/ETM_BEV/BEVerse\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def measure_time_host(\n",
    "    model: nn.Module,\n",
    "    input_tensor: torch.Tensor,\n",
    "    num_repeats: int = 100,\n",
    "    num_warmups: int = 10,\n",
    "    synchronize: bool = True,\n",
    "    continuous_measure: bool = True,\n",
    ") -> float:\n",
    "\n",
    "    for _ in range(num_warmups):\n",
    "        _ = model.forward(input_tensor)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    elapsed_time_ms = 0\n",
    "\n",
    "    if continuous_measure:\n",
    "        start = timer()\n",
    "        for _ in range(num_repeats):\n",
    "            _ = model.forward(input_tensor)\n",
    "        if synchronize:\n",
    "            torch.cuda.synchronize()\n",
    "        end = timer()\n",
    "        elapsed_time_ms = (end - start) * 1000\n",
    "\n",
    "    else:\n",
    "        for _ in range(num_repeats):\n",
    "            start = timer()\n",
    "            _ = model.forward(input_tensor)\n",
    "            if synchronize:\n",
    "                torch.cuda.synchronize()\n",
    "            end = timer()\n",
    "            elapsed_time_ms += (end - start) * 1000\n",
    "\n",
    "    return elapsed_time_ms / num_repeats\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def measure_time_device(\n",
    "    model: nn.Module,\n",
    "    input_tensor: torch.Tensor,\n",
    "    num_repeats: int = 100,\n",
    "    num_warmups: int = 10,\n",
    "    synchronize: bool = True,\n",
    "    continuous_measure: bool = True,\n",
    ") -> float:\n",
    "\n",
    "    for _ in range(num_warmups):\n",
    "        _ = model.forward(input_tensor)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    elapsed_time_ms = 0\n",
    "\n",
    "    if continuous_measure:\n",
    "        start_event = torch.cuda.Event(enable_timing=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True)\n",
    "        start_event.record()\n",
    "        for _ in range(num_repeats):\n",
    "            _ = model.forward(input_tensor)\n",
    "        end_event.record()\n",
    "        if synchronize:\n",
    "            # This has to be synchronized to compute the elapsed time.\n",
    "            # Otherwise, there will be runtime error.\n",
    "            torch.cuda.synchronize()\n",
    "        elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "\n",
    "    else:\n",
    "        for _ in range(num_repeats):\n",
    "            start_event = torch.cuda.Event(enable_timing=True)\n",
    "            end_event = torch.cuda.Event(enable_timing=True)\n",
    "            start_event.record()\n",
    "            _ = model.forward(input_tensor)\n",
    "            end_event.record()\n",
    "            if synchronize:\n",
    "                # This has to be synchronized to compute the elapsed time.\n",
    "                # Otherwise, there will be runtime error.\n",
    "                torch.cuda.synchronize()\n",
    "            elapsed_time_ms += start_event.elapsed_time(end_event)\n",
    "\n",
    "    return elapsed_time_ms / num_repeats\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_inference(model: nn.Module, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    # from model.forward because BEVerse differentiates between different input types - img lidar etc \n",
    "    return model.forward_dummy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = build_dataset(cfg.data.test)\n",
    "data_loader = build_dataloader(\n",
    "    dataset,\n",
    "    samples_per_gpu=samples_per_gpu,\n",
    "    workers_per_gpu=cfg.data.workers_per_gpu,\n",
    "    dist=False,\n",
    "    shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['img_metas', 'img_inputs', 'semantic_indices', 'semantic_map', 'future_egomotions', 'gt_bboxes_3d', 'gt_labels_3d', 'motion_segmentation', 'motion_instance', 'instance_centerness', 'instance_offset', 'instance_flow', 'has_invalid_frame', 'img_is_valid'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(cfg.model, test_cfg=cfg.get(\"test_cfg\"))\n",
    "wrap_fp16_model(model)\n",
    "model.cuda()\n",
    "model = MMDataParallel(model, device_ids=[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple test\n",
      "Started Logger\n",
      "simple_test_pts\n"
     ]
    }
   ],
   "source": [
    "motion_distribution_targets = {\n",
    "    # for motion prediction\n",
    "    'motion_segmentation': sample['motion_segmentation'][0],\n",
    "    'motion_instance': sample['motion_instance'][0],\n",
    "    'instance_centerness': sample['instance_centerness'][0],\n",
    "    'instance_offset': sample['instance_offset'][0],\n",
    "    'instance_flow': sample['instance_flow'][0],\n",
    "    'future_egomotion': sample['future_egomotions'][0],\n",
    "}\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    result = model(\n",
    "        return_loss=False,\n",
    "        rescale=True,\n",
    "        img_metas=sample['img_metas'],\n",
    "        img_inputs=sample['img_inputs'],\n",
    "        future_egomotions=sample['future_egomotions'],\n",
    "        motion_targets=motion_distribution_targets,\n",
    "        img_is_valid=sample['img_is_valid'][0],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple test\n",
    "\n",
    "batch?:  torch.Size([1, 3, 6, 3, 256, 704])\n",
    "\n",
    "B 1, S 3, N 6, C 3, imH 256, imW 704\n",
    "\n",
    "imgs  torch.Size([18, 3, 256, 704])\n",
    "\n",
    "after backbone:  2\n",
    "\n",
    "shape in list:  [torch.Size([18, 384, 16, 44]), torch.Size([18, 768, 8, 22])]\n",
    "\n",
    "after backbone with_img_neck:  torch.Size([18, 512, 16, 44])\n",
    "\n",
    "after transformation:  torch.Size([1, 3, 6, 512, 16, 44])\n",
    "\n",
    "after LLS :  torch.Size([1, 3, 64, 128, 128])\n",
    "\n",
    "after Temporal :  torch.Size([1, 64, 128, 128])\n",
    "\n",
    "simple_test_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t_BEV': 1663055717.0110373,\n",
       " 't_temporal': 1663055717.5791538,\n",
       " 't0': 1663055712.6830127,\n",
       " 't_end': 1663055746.884341}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"time_stats\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x, rots, trans, intrins, post_rots, post_trans = input\n",
    "\n",
    "'CAM_FRONT_RIGHT': {\n",
    "    'data_path': './data/nuscenes/samples/CAM_FRONT_RIGHT/n015-2018-10-08-15-36-50+0800__CAM_FRONT_RIGHT__1538984233520339.jpg', \n",
    "    'type': 'CAM_FRONT_RIGHT', \n",
    "    'sample_data_token': 'd6f89460954c43d39ed7c9ac91ab03d0', \n",
    "    'sensor2ego_translation': [1.5508477543, -0.493404796419, 1.49574800619], \n",
    "    'sensor2ego_rotation': [0.2060347966337182, -0.2026940577919598, 0.6824507824531167, -0.6713610884174485], 'ego2global_translation': [715.6566537856895, 1810.1516804263824, 0.0], \n",
    "    'ego2global_rotation': [0.8004835927391405, 0.00541081062084495, -0.0028680900818508943, -0.5993233809414961], 'timestamp': 1538984233520339, \n",
    "    'sensor2lidar_rotation': array([[ 0.55971752, -0.01057234,  0.82861603],\n",
    "       [-0.82828535,  0.02385392,  0.55979851],\n",
    "       [-0.02568412, -0.99965955,  0.00459454]]), \n",
    "    'sensor2lidar_translation': array([ 0.48135698,  0.51094805, -0.32997566]), \n",
    "    'cam_intrinsic': array([[1.26084744e+03, 0.00000000e+00, 8.07968245e+02],\n",
    "       [0.00000000e+00, 1.26084744e+03, 4.95334427e+02],\n",
    "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataContainer([[{'flip': False, 'pcd_horizontal_flip': False, 'pcd_vertical_flip': False, 'box_mode_3d': <Box3DMode.LIDAR: 0>, 'box_type_3d': <class 'mmdet3d.core.bbox.structures.lidar_box3d.LiDARInstance3DBoxes'>, 'sample_idx': 'b5989651183643369174912bc5641d3b', 'pcd_scale_factor': 1.0, 'pts_filename': './data/nuscenes/samples/LIDAR_TOP/n015-2018-10-08-15-36-50+0800__LIDAR_TOP__1538984233547259.pcd.bin', 'img_info': [{'CAM_FRONT': {'data_path': './data/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984233512470.jpg', 'type': 'CAM_FRONT', 'sample_data_token': '524d443c501a4f98a14508c3fb6f6de3', 'sensor2ego_translation': [1.70079118954, 0.0159456324149, 1.51095763913], 'sensor2ego_rotation': [0.4998015430569128, -0.5030316162024876, 0.4997798114386805, -0.49737083824542755], 'ego2global_translation': [715.6477969391926, 1810.182287996884, 0.0], 'ego2global_rotation': [0.8012964074773921, 0.00544170305513258, -0.0028663044812231933, -0.5982359396845913], 'timestamp': 1538984233512470, 'sensor2lidar_rotation': array([[ 0.99984938,  0.00665765, -0.01602791],\n",
       "       [ 0.01590371,  0.01830706,  0.99970592],\n",
       "       [ 0.00694912, -0.99981025,  0.01819842]]), 'sensor2lidar_translation': array([-0.03363291,  0.62479763, -0.31517726]), 'cam_intrinsic': array([[1.26641720e+03, 0.00000000e+00, 8.16267020e+02],\n",
       "       [0.00000000e+00, 1.26641720e+03, 4.91507066e+02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])}, 'CAM_FRONT_RIGHT': {'data_path': './data/nuscenes/samples/CAM_FRONT_RIGHT/n015-2018-10-08-15-36-50+0800__CAM_FRONT_RIGHT__1538984233520339.jpg', 'type': 'CAM_FRONT_RIGHT', 'sample_data_token': 'd6f89460954c43d39ed7c9ac91ab03d0', 'sensor2ego_translation': [1.5508477543, -0.493404796419, 1.49574800619], 'sensor2ego_rotation': [0.2060347966337182, -0.2026940577919598, 0.6824507824531167, -0.6713610884174485], 'ego2global_translation': [715.6566537856895, 1810.1516804263824, 0.0], 'ego2global_rotation': [0.8004835927391405, 0.00541081062084495, -0.0028680900818508943, -0.5993233809414961], 'timestamp': 1538984233520339, 'sensor2lidar_rotation': array([[ 0.55971752, -0.01057234,  0.82861603],\n",
       "       [-0.82828535,  0.02385392,  0.55979851],\n",
       "       [-0.02568412, -0.99965955,  0.00459454]]), 'sensor2lidar_translation': array([ 0.48135698,  0.51094805, -0.32997566]), 'cam_intrinsic': array([[1.26084744e+03, 0.00000000e+00, 8.07968245e+02],\n",
       "       [0.00000000e+00, 1.26084744e+03, 4.95334427e+02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])}, 'CAM_FRONT_LEFT': {'data_path': './data/nuscenes/samples/CAM_FRONT_LEFT/n015-2018-10-08-15-36-50+0800__CAM_FRONT_LEFT__1538984233504844.jpg', 'type': 'CAM_FRONT_LEFT', 'sample_data_token': 'd6986708c5084569bf7a636968070602', 'sensor2ego_translation': [1.52387798135, 0.494631336551, 1.50932822144], 'sensor2ego_rotation': [0.6757265034669446, -0.6736266522251881, 0.21214015046209478, -0.21122827103904068], 'ego2global_translation': [715.6390883664105, 1810.2118513524092, 0.0], 'ego2global_rotation': [0.8020719929312536, 0.005471594935842252, -0.0028645972209445187, -0.5971954235314489], 'timestamp': 1538984233504844, 'sensor2lidar_rotation': array([[ 0.56057632,  0.00250465, -0.82809898],\n",
       "       [ 0.82783997,  0.02349612,  0.56047205],\n",
       "       [ 0.0208609 , -0.99972079,  0.01109792]]), 'sensor2lidar_translation': array([-0.51404547,  0.41102881, -0.32481493]), 'cam_intrinsic': array([[1.27259795e+03, 0.00000000e+00, 8.26615493e+02],\n",
       "       [0.00000000e+00, 1.27259795e+03, 4.79751654e+02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])}, 'CAM_BACK': {'data_path': './data/nuscenes/samples/CAM_BACK/n015-2018-10-08-15-36-50+0800__CAM_BACK__1538984233537525.jpg', 'type': 'CAM_BACK', 'sample_data_token': 'fd183c135b1f41ea8eb7a3df78d0b1ff', 'sensor2ego_translation': [0.0283260309358, 0.00345136761476, 1.57910346144], 'sensor2ego_rotation': [0.5037872666382278, -0.49740249788611096, -0.4941850223835201, 0.5045496097725578], 'ego2global_translation': [715.675643040015, 1810.08500957178, 0.0], 'ego2global_rotation': [0.7986492558274089, 0.005347114582401329, -0.0028879623579548255, -0.6017660959254754], 'timestamp': 1538984233537525, 'sensor2lidar_rotation': array([[-0.99995084,  0.00986234, -0.00102836],\n",
       "       [ 0.00110126,  0.00738905, -0.99997209],\n",
       "       [-0.00985447, -0.99992407, -0.00739955]]), 'sensor2lidar_translation': array([-0.00420777, -0.94773981, -0.28427825]), 'cam_intrinsic': array([[809.22099057,   0.        , 829.21960033],\n",
       "       [  0.        , 809.22099057, 481.77842385],\n",
       "       [  0.        ,   0.        ,   1.        ]])}, 'CAM_BACK_LEFT': {'data_path': './data/nuscenes/samples/CAM_BACK_LEFT/n015-2018-10-08-15-36-50+0800__CAM_BACK_LEFT__1538984233547423.jpg', 'type': 'CAM_BACK_LEFT', 'sample_data_token': '4552459a83ac4259b7592c8d7c87248f', 'sensor2ego_translation': [1.03569100218, 0.484795032713, 1.59097014818], 'sensor2ego_rotation': [0.6924185592174665, -0.7031619420114925, -0.11648342771943819, 0.11203317912370753], 'ego2global_translation': [715.6861851787944, 1810.046666787832, 0.0], 'ego2global_rotation': [0.797548711559291, 0.005314979524262166, -0.002909620171228901, -0.6032241190413655], 'timestamp': 1538984233547423, 'sensor2lidar_rotation': array([[-0.31699263,  0.01989338, -0.94821935],\n",
       "       [ 0.94810196,  0.03285954, -0.316264  ],\n",
       "       [ 0.02486649, -0.99926198, -0.02927718]]), 'sensor2lidar_translation': array([-0.48305818,  0.09972816, -0.24976837]), 'cam_intrinsic': array([[1.25674148e+03, 0.00000000e+00, 7.92112574e+02],\n",
       "       [0.00000000e+00, 1.25674148e+03, 4.92775747e+02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])}, 'CAM_BACK_RIGHT': {'data_path': './data/nuscenes/samples/CAM_BACK_RIGHT/n015-2018-10-08-15-36-50+0800__CAM_BACK_RIGHT__1538984233527893.jpg', 'type': 'CAM_BACK_RIGHT', 'sample_data_token': 'b3e53998db124133bb9cd832d78d2b11', 'sensor2ego_translation': [1.0148780988, -0.480568219723, 1.56239545128], 'sensor2ego_rotation': [0.12280980120078765, -0.132400842670559, -0.7004305821388234, 0.690496031265798], 'ego2global_translation': [715.6651129914562, 1810.1224267298358, 0.0], 'ego2global_rotation': [0.7997019438584799, 0.005378627710295174, -0.0028707485661199133, -0.600366246682467], 'timestamp': 1538984233527893, 'sensor2lidar_rotation': array([[-0.3502702 , -0.0055997 ,  0.93663196],\n",
       "       [-0.93598044,  0.03985974, -0.34978825],\n",
       "       [-0.0353752 , -0.99918959, -0.01920289]]), 'sensor2lidar_translation': array([ 0.47471108,  0.00256451, -0.2754058 ]), 'cam_intrinsic': array([[1.25951374e+03, 0.00000000e+00, 8.07252905e+02],\n",
       "       [0.00000000e+00, 1.25951374e+03, 5.01195799e+02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])}}, {'CAM_FRONT': {'data_path': './data/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984233512470.jpg', 'type': 'CAM_FRONT', 'sample_data_token': '524d443c501a4f98a14508c3fb6f6de3', 'sensor2ego_translation': [1.70079118954, 0.0159456324149, 1.51095763913], 'sensor2ego_rotation': [0.4998015430569128, -0.5030316162024876, 0.4997798114386805, -0.49737083824542755], 'ego2global_translation': [715.6477969391926, 1810.182287996884, 0.0], 'ego2global_rotation': [0.8012964074773921, 0.00544170305513258, -0.0028663044812231933, -0.5982359396845913], 'timestamp': 1538984233512470, 'sensor2lidar_rotation': array([[ 0.99984938,  0.00665765, -0.01602791],\n",
       "       [ 0.01590371,  0.01830706,  0.99970592],\n",
       "       [ 0.00694912, -0.99981025,  0.01819842]]), 'sensor2lidar_translation': array([-0.03363291,  0.62479763, -0.31517726]), 'cam_intrinsic': array([[1.26641720e+03, 0.00000000e+00, 8.16267020e+02],\n",
       "       [0.00000000e+00, 1.26641720e+03, 4.91507066e+02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])}, 'CAM_FRONT_RIGHT': {'data_path': './data/nuscenes/samples/CAM_FRONT_RIGHT/n015-2018-10-08-15-36-50+0800__CAM_FRONT_RIGHT__1538984233520339.jpg', 'type': 'CAM_FRONT_RIGHT', 'sample_data_token': 'd6f89460954c43d39ed7c9ac91ab03d0', 'sensor2ego_translation': [1.5508477543, -0.493404796419, 1.49574800619], 'sensor2ego_rotation': [0.2060347966337182, -0.2026940577919598, 0.6824507824531167, -0.6713610884174485], 'ego2global_translation': [715.6566537856895, 1810.1516804263824, 0.0], 'ego2global_rotation': [0.8004835927391405, 0.00541081062084495, -0.0028680900818508943, -0.5993233809414961], 'timestamp': 1538984233520339, 'sensor2lidar_rotation': array([[ 0.55971752, -0.01057234,  0.82861603],\n",
       "       [-0.82828535,  0.02385392,  0.55979851],\n",
       "       [-0.02568412, -0.99965955,  0.00459454]]), 'sensor2lidar_translation': array([ 0.48135698,  0.51094805, -0.32997566]), 'cam_intrinsic': array([[1.26084744e+03, 0.00000000e+00, 8.07968245e+02],\n",
       "       [0.00000000e+00, 1.26084744e+03, 4.95334427e+02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])}, 'CAM_FRONT_LEFT': {'data_path': './data/nuscenes/samples/CAM_FRONT_LEFT/n015-2018-10-08-15-36-50+0800__CAM_FRONT_LEFT__1538984233504844.jpg', 'type': 'CAM_FRONT_LEFT', 'sample_data_token': 'd6986708c5084569bf7a636968070602', 'sensor2ego_translation': [1.52387798135, 0.494631336551, 1.50932822144], 'sensor2ego_rotation': [0.6757265034669446, -0.6736266522251881, 0.21214015046209478, -0.21122827103904068], 'ego2global_translation': [715.6390883664105, 1810.2118513524092, 0.0], 'ego2global_rotation': [0.8020719929312536, 0.005471594935842252, -0.0028645972209445187, -0.5971954235314489], 'timestamp': 1538984233504844, 'sensor2lidar_rotation': array([[ 0.56057632,  0.00250465, -0.82809898],\n",
       "       [ 0.82783997,  0.02349612,  0.56047205],\n",
       "       [ 0.0208609 , -0.99972079,  0.01109792]]), 'sensor2lidar_translation': array([-0.51404547,  0.41102881, -0.32481493]), 'cam_intrinsic': array([[1.27259795e+03, 0.00000000e+00, 8.26615493e+02],\n",
       "       [0.00000000e+00, 1.27259795e+03, 4.79751654e+02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])}, 'CAM_BACK': {'data_path': './data/nuscenes/samples/CAM_BACK/n015-2018-10-08-15-36-50+0800__CAM_BACK__1538984233537525.jpg', 'type': 'CAM_BACK', 'sample_data_token': 'fd183c135b1f41ea8eb7a3df78d0b1ff', 'sensor2ego_translation': [0.0283260309358, 0.00345136761476, 1.57910346144], 'sensor2ego_rotation': [0.5037872666382278, -0.49740249788611096, -0.4941850223835201, 0.5045496097725578], 'ego2global_translation': [715.675643040015, 1810.08500957178, 0.0], 'ego2global_rotation': [0.7986492558274089, 0.005347114582401329, -0.0028879623579548255, -0.6017660959254754], 'timestamp': 1538984233537525, 'sensor2lidar_rotation': array([[-0.99995084,  0.00986234, -0.00102836],\n",
       "       [ 0.00110126,  0.00738905, -0.99997209],\n",
       "       [-0.00985447, -0.99992407, -0.00739955]]), 'sensor2lidar_translation': array([-0.00420777, -0.94773981, -0.28427825]), 'cam_intrinsic': array([[809.22099057,   0.        , 829.21960033],\n",
       "       [  0.        , 809.22099057, 481.77842385],\n",
       "       [  0.        ,   0.        ,   1.        ]])}, 'CAM_BACK_LEFT': {'data_path': './data/nuscenes/samples/CAM_BACK_LEFT/n015-2018-10-08-15-36-50+0800__CAM_BACK_LEFT__1538984233547423.jpg', 'type': 'CAM_BACK_LEFT', 'sample_data_token': '4552459a83ac4259b7592c8d7c87248f', 'sensor2ego_translation': [1.03569100218, 0.484795032713, 1.59097014818], 'sensor2ego_rotation': [0.6924185592174665, -0.7031619420114925, -0.11648342771943819, 0.11203317912370753], 'ego2global_translation': [715.6861851787944, 1810.046666787832, 0.0], 'ego2global_rotation': [0.797548711559291, 0.005314979524262166, -0.002909620171228901, -0.6032241190413655], 'timestamp': 1538984233547423, 'sensor2lidar_rotation': array([[-0.31699263,  0.01989338, -0.94821935],\n",
       "       [ 0.94810196,  0.03285954, -0.316264  ],\n",
       "       [ 0.02486649, -0.99926198, -0.02927718]]), 'sensor2lidar_translation': array([-0.48305818,  0.09972816, -0.24976837]), 'cam_intrinsic': array([[1.25674148e+03, 0.00000000e+00, 7.92112574e+02],\n",
       "       [0.00000000e+00, 1.25674148e+03, 4.92775747e+02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])}, 'CAM_BACK_RIGHT': {'data_path': './data/nuscenes/samples/CAM_BACK_RIGHT/n015-2018-10-08-15-36-50+0800__CAM_BACK_RIGHT__1538984233527893.jpg', 'type': 'CAM_BACK_RIGHT', 'sample_data_token': 'b3e53998db124133bb9cd832d78d2b11', 'sensor2ego_translation': [1.0148780988, -0.480568219723, 1.56239545128], 'sensor2ego_rotation': [0.12280980120078765, -0.132400842670559, -0.7004305821388234, 0.690496031265798], 'ego2global_translation': [715.6651129914562, 1810.1224267298358, 0.0], 'ego2global_rotation': [0.7997019438584799, 0.005378627710295174, -0.0028707485661199133, -0.600366246682467], 'timestamp': 1538984233527893, 'sensor2lidar_rotation': array([[-0.3502702 , -0.0055997 ,  0.93663196],\n",
       "       [-0.93598044,  0.03985974, -0.34978825],\n",
       "       [-0.0353752 , -0.99918959, -0.01920289]]), 'sensor2lidar_translation': array([ 0.47471108,  0.00256451, -0.2754058 ]), 'cam_intrinsic': array([[1.25951374e+03, 0.00000000e+00, 8.07252905e+02],\n",
       "       [0.00000000e+00, 1.25951374e+03, 5.01195799e+02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])}}, {'CAM_FRONT': {'data_path': './data/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984233512470.jpg', 'type': 'CAM_FRONT', 'sample_data_token': '524d443c501a4f98a14508c3fb6f6de3', 'sensor2ego_translation': [1.70079118954, 0.0159456324149, 1.51095763913], 'sensor2ego_rotation': [0.4998015430569128, -0.5030316162024876, 0.4997798114386805, -0.49737083824542755], 'ego2global_translation': [715.6477969391926, 1810.182287996884, 0.0], 'ego2global_rotation': [0.8012964074773921, 0.00544170305513258, -0.0028663044812231933, -0.5982359396845913], 'timestamp': 1538984233512470, 'sensor2lidar_rotation': array([[ 0.99984938,  0.00665765, -0.01602791],\n",
       "       [ 0.01590371,  0.01830706,  0.99970592],\n",
       "       [ 0.00694912, -0.99981025,  0.01819842]]), 'sensor2lidar_translation': array([-0.03363291,  0.62479763, -0.31517726]), 'cam_intrinsic': array([[1.26641720e+03, 0.00000000e+00, 8.16267020e+02],\n",
       "       [0.00000000e+00, 1.26641720e+03, 4.91507066e+02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])}, 'CAM_FRONT_RIGHT': {'data_path': './data/nuscenes/samples/CAM_FRONT_RIGHT/n015-2018-10-08-15-36-50+0800__CAM_FRONT_RIGHT__1538984233520339.jpg', 'type': 'CAM_FRONT_RIGHT', 'sample_data_token': 'd6f89460954c43d39ed7c9ac91ab03d0', 'sensor2ego_translation': [1.5508477543, -0.493404796419, 1.49574800619], 'sensor2ego_rotation': [0.2060347966337182, -0.2026940577919598, 0.6824507824531167, -0.6713610884174485], 'ego2global_translation': [715.6566537856895, 1810.1516804263824, 0.0], 'ego2global_rotation': [0.8004835927391405, 0.00541081062084495, -0.0028680900818508943, -0.5993233809414961], 'timestamp': 1538984233520339, 'sensor2lidar_rotation': array([[ 0.55971752, -0.01057234,  0.82861603],\n",
       "       [-0.82828535,  0.02385392,  0.55979851],\n",
       "       [-0.02568412, -0.99965955,  0.00459454]]), 'sensor2lidar_translation': array([ 0.48135698,  0.51094805, -0.32997566]), 'cam_intrinsic': array([[1.26084744e+03, 0.00000000e+00, 8.07968245e+02],\n",
       "       [0.00000000e+00, 1.26084744e+03, 4.95334427e+02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])}, 'CAM_FRONT_LEFT': {'data_path': './data/nuscenes/samples/CAM_FRONT_LEFT/n015-2018-10-08-15-36-50+0800__CAM_FRONT_LEFT__1538984233504844.jpg', 'type': 'CAM_FRONT_LEFT', 'sample_data_token': 'd6986708c5084569bf7a636968070602', 'sensor2ego_translation': [1.52387798135, 0.494631336551, 1.50932822144], 'sensor2ego_rotation': [0.6757265034669446, -0.6736266522251881, 0.21214015046209478, -0.21122827103904068], 'ego2global_translation': [715.6390883664105, 1810.2118513524092, 0.0], 'ego2global_rotation': [0.8020719929312536, 0.005471594935842252, -0.0028645972209445187, -0.5971954235314489], 'timestamp': 1538984233504844, 'sensor2lidar_rotation': array([[ 0.56057632,  0.00250465, -0.82809898],\n",
       "       [ 0.82783997,  0.02349612,  0.56047205],\n",
       "       [ 0.0208609 , -0.99972079,  0.01109792]]), 'sensor2lidar_translation': array([-0.51404547,  0.41102881, -0.32481493]), 'cam_intrinsic': array([[1.27259795e+03, 0.00000000e+00, 8.26615493e+02],\n",
       "       [0.00000000e+00, 1.27259795e+03, 4.79751654e+02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])}, 'CAM_BACK': {'data_path': './data/nuscenes/samples/CAM_BACK/n015-2018-10-08-15-36-50+0800__CAM_BACK__1538984233537525.jpg', 'type': 'CAM_BACK', 'sample_data_token': 'fd183c135b1f41ea8eb7a3df78d0b1ff', 'sensor2ego_translation': [0.0283260309358, 0.00345136761476, 1.57910346144], 'sensor2ego_rotation': [0.5037872666382278, -0.49740249788611096, -0.4941850223835201, 0.5045496097725578], 'ego2global_translation': [715.675643040015, 1810.08500957178, 0.0], 'ego2global_rotation': [0.7986492558274089, 0.005347114582401329, -0.0028879623579548255, -0.6017660959254754], 'timestamp': 1538984233537525, 'sensor2lidar_rotation': array([[-0.99995084,  0.00986234, -0.00102836],\n",
       "       [ 0.00110126,  0.00738905, -0.99997209],\n",
       "       [-0.00985447, -0.99992407, -0.00739955]]), 'sensor2lidar_translation': array([-0.00420777, -0.94773981, -0.28427825]), 'cam_intrinsic': array([[809.22099057,   0.        , 829.21960033],\n",
       "       [  0.        , 809.22099057, 481.77842385],\n",
       "       [  0.        ,   0.        ,   1.        ]])}, 'CAM_BACK_LEFT': {'data_path': './data/nuscenes/samples/CAM_BACK_LEFT/n015-2018-10-08-15-36-50+0800__CAM_BACK_LEFT__1538984233547423.jpg', 'type': 'CAM_BACK_LEFT', 'sample_data_token': '4552459a83ac4259b7592c8d7c87248f', 'sensor2ego_translation': [1.03569100218, 0.484795032713, 1.59097014818], 'sensor2ego_rotation': [0.6924185592174665, -0.7031619420114925, -0.11648342771943819, 0.11203317912370753], 'ego2global_translation': [715.6861851787944, 1810.046666787832, 0.0], 'ego2global_rotation': [0.797548711559291, 0.005314979524262166, -0.002909620171228901, -0.6032241190413655], 'timestamp': 1538984233547423, 'sensor2lidar_rotation': array([[-0.31699263,  0.01989338, -0.94821935],\n",
       "       [ 0.94810196,  0.03285954, -0.316264  ],\n",
       "       [ 0.02486649, -0.99926198, -0.02927718]]), 'sensor2lidar_translation': array([-0.48305818,  0.09972816, -0.24976837]), 'cam_intrinsic': array([[1.25674148e+03, 0.00000000e+00, 7.92112574e+02],\n",
       "       [0.00000000e+00, 1.25674148e+03, 4.92775747e+02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])}, 'CAM_BACK_RIGHT': {'data_path': './data/nuscenes/samples/CAM_BACK_RIGHT/n015-2018-10-08-15-36-50+0800__CAM_BACK_RIGHT__1538984233527893.jpg', 'type': 'CAM_BACK_RIGHT', 'sample_data_token': 'b3e53998db124133bb9cd832d78d2b11', 'sensor2ego_translation': [1.0148780988, -0.480568219723, 1.56239545128], 'sensor2ego_rotation': [0.12280980120078765, -0.132400842670559, -0.7004305821388234, 0.690496031265798], 'ego2global_translation': [715.6651129914562, 1810.1224267298358, 0.0], 'ego2global_rotation': [0.7997019438584799, 0.005378627710295174, -0.0028707485661199133, -0.600366246682467], 'timestamp': 1538984233527893, 'sensor2lidar_rotation': array([[-0.3502702 , -0.0055997 ,  0.93663196],\n",
       "       [-0.93598044,  0.03985974, -0.34978825],\n",
       "       [-0.0353752 , -0.99918959, -0.01920289]]), 'sensor2lidar_translation': array([ 0.47471108,  0.00256451, -0.2754058 ]), 'cam_intrinsic': array([[1.25951374e+03, 0.00000000e+00, 8.07252905e+02],\n",
       "       [0.00000000e+00, 1.25951374e+03, 5.01195799e+02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])}}], 'lidar2ego_rots': tensor([[ 0.0020,  0.9997,  0.0242],\n",
       "        [-1.0000,  0.0022, -0.0058],\n",
       "        [-0.0059, -0.0242,  0.9997]], dtype=torch.float64), 'lidar2ego_trans': tensor([0.9437, 0.0000, 1.8402])}]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"img_metas\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future Egomotions:  torch.Size([1, 7, 6])\n",
      "img_is_valid:  torch.Size([1, 7])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "num_warmups = 100\n",
    "num_repeats = 1000\n",
    "# Change to C x 1 x 3 x 1600 x 900\n",
    "# 704×256 Tiny\n",
    "# 1408×512\n",
    "# B, S, N, C, imH, imW = imgs.shape\n",
    "img_inputs =  torch.rand(1,7,6,3,704,256).cuda()\n",
    "# (batch, seq, num_cam)\n",
    "future_egomotions = torch.zeros((batch_size, 7, 6)).type_as(img_inputs).cuda()\n",
    "img_is_valid = torch.ones((batch_size, 7)).type_as(img_inputs) > 0\n",
    "img_is_valid.cuda()\n",
    "print(\"Future Egomotions: \", future_egomotions.shape)\n",
    "print(\"img_is_valid: \", img_is_valid.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #single frame \n",
    "# future_egomotions = future_egomotions[:, :1]\n",
    "# img_is_valid = img_is_valid[:, :1]\n",
    "# print(\"Future Egomotions: \", future_egomotions.shape)\n",
    "# print(\"img_is_valid: \", img_is_valid.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n",
      "torch.Size([3])\n",
      "image meta: [{'box_type_3d': <class 'mmdet3d.core.bbox.structures.lidar_box3d.LiDARInstance3DBoxes'>, 'lidar2ego_rots': tensor([[-5.4280e-04,  9.9893e-01,  4.6229e-02],\n",
      "        [-1.0000e+00, -4.0569e-04, -2.9750e-03],\n",
      "        [-2.9531e-03, -4.6231e-02,  9.9893e-01]]), 'lidar2ego_trans': tensor([0.9858, 0.0000, 1.8402])}]\n"
     ]
    }
   ],
   "source": [
    "from mmdet3d.core.bbox.structures.box_3d_mode import LiDARInstance3DBoxes\n",
    "\n",
    "dummy_lidar2ego_rots = (\n",
    "    torch.tensor(\n",
    "        [\n",
    "            [-5.4280e-04, 9.9893e-01, 4.6229e-02],\n",
    "            [-1.0000e00, -4.0569e-04, -2.9750e-03],\n",
    "            [-2.9531e-03, -4.6231e-02, 9.9893e-01],\n",
    "        ]\n",
    "    )\n",
    "    .type_as(img_inputs)\n",
    "    .cpu()\n",
    ")\n",
    "dummy_lidar2ego_trans = (\n",
    "    torch.tensor([0.9858, 0.0000, 1.8402]).type_as(img_inputs).cpu()\n",
    ")\n",
    "print(dummy_lidar2ego_rots.shape)\n",
    "print(dummy_lidar2ego_trans.shape)\n",
    "img_metas = [\n",
    "    dict(\n",
    "        box_type_3d=LiDARInstance3DBoxes,\n",
    "        lidar2ego_rots=dummy_lidar2ego_rots,\n",
    "        lidar2ego_trans=dummy_lidar2ego_trans,\n",
    "    )\n",
    "]\n",
    "print(\"image meta:\", img_metas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'projects.mmdet3d_plugin.models.detectors.beverse.BEVerse'>\n"
     ]
    }
   ],
   "source": [
    "model = build_model(cfg.model, test_cfg=cfg.get(\"test_cfg\"))\n",
    "print(type(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch?:  torch.Size([1, 7, 6, 3, 704, 256])\n",
      "B 1, S 7, 6, C 3, imH 704, imW 256\n",
      "imgs  torch.Size([42, 3, 704, 256])\n",
      "after backbone:  2\n",
      "shape in list:  [torch.Size([42, 384, 44, 16]), torch.Size([42, 768, 22, 8])]\n",
      "after backbone with_img_neck:  torch.Size([42, 512, 44, 16])\n",
      "after transformation:  torch.Size([1, 7, 6, 512, 44, 16])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"Tensor\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m build_model(cfg\u001b[38;5;241m.\u001b[39mmodel, test_cfg\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_cfg\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      2\u001b[0m wrap_fp16_model(model)\n\u001b[0;32m----> 3\u001b[0m img_feats,time_dict  \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_img_feat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimg_metas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_metas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfuture_egomotion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuture_egomotions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimg_is_valid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_is_valid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(img_feats\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(time_dict)\n",
      "File \u001b[0;32m~/ETM_BEV/BEVerse/projects/mmdet3d_plugin/models/detectors/beverse.py:105\u001b[0m, in \u001b[0;36mBEVerse.extract_img_feat\u001b[0;34m(self, img, img_metas, future_egomotion, aug_transform, img_is_valid, count_time)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mafter transformation: \u001b[39m\u001b[39m\"\u001b[39m, x\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    104\u001b[0m \u001b[39m# lifting with LSS\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer([x] \u001b[39m+\u001b[39;49m img[\u001b[39m1\u001b[39;49m:])\n\u001b[1;32m    107\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39msynchronize()\n\u001b[1;32m    108\u001b[0m t_BEV \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"Tensor\") to list"
     ]
    }
   ],
   "source": [
    "model = build_model(cfg.model, test_cfg=cfg.get(\"test_cfg\")).cuda()\n",
    "wrap_fp16_model(model)\n",
    "img_feats,time_dict  = model.extract_img_feat(\n",
    "            img=img_inputs,\n",
    "            img_metas=img_metas,\n",
    "            future_egomotion=future_egomotions,\n",
    "            img_is_valid=img_is_valid,\n",
    "        )\n",
    "print(img_feats.shape)\n",
    "print(time_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0, 7, 6, 3, 704, 256])\n",
      "torch.Size([1, 7, 6, 3, 704, 256])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"Tensor\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m img_inputs \u001b[38;5;241m=\u001b[39m  torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m7\u001b[39m,\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m704\u001b[39m,\u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(img_inputs[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 6\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mtrans_output\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimg_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      7\u001b[0m test\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"Tensor\") to list"
     ]
    }
   ],
   "source": [
    "img_inputs =  torch.rand(1,7,6,3,704,256)\n",
    "print(img_inputs[1:].shape)\n",
    "trans_output = torch.rand(1, 7, 6, 512, 44, 16)\n",
    "img_inputs =  torch.rand(2,7,6,3,704,256)\n",
    "print(img_inputs[1:].shape)\n",
    "test = [trans_output] + img_inputs[1:]\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency Measurement Using CPU Timer...\n",
      "|Synchronization: True | Continuous Measurement: True | Latency: N/A     ms| \n",
      "Latency Measurement Using CUDA Timer...\n",
      "|Synchronization: True | Continuous Measurement: True | Latency: N/A     ms| \n",
      "|Synchronization: False| Continuous Measurement: True | Latency: N/A     ms| \n",
      "|Synchronization: True | Continuous Measurement: False| Latency: N/A     ms| \n",
      "|Synchronization: False| Continuous Measurement: False| Latency: N/A     ms| \n",
      "Latency Measurement Using PyTorch Benchmark...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dummy_input.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 95\u001b[0m\n\u001b[1;32m     85\u001b[0m num_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     86\u001b[0m timer \u001b[38;5;241m=\u001b[39m benchmark\u001b[38;5;241m.\u001b[39mTimer(\n\u001b[1;32m     87\u001b[0m     stmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_inference(model, input_tensor)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     88\u001b[0m     setup\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom __main__ import run_inference\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m     sub_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.utils.benchmark.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     93\u001b[0m )\n\u001b[0;32m---> 95\u001b[0m profile_result \u001b[38;5;241m=\u001b[39m \u001b[43mtimer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_repeats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# https://pytorch.org/docs/stable/_modules/torch/utils/benchmark/utils/common.html#Measurement\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLatency: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprofile_result\u001b[38;5;241m.\u001b[39mmean \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/torch/utils/benchmark/utils/timer.py:261\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39m\"\"\"Mirrors the semantics of timeit.Timer.timeit().\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \n\u001b[1;32m    256\u001b[0m \u001b[39mExecute the main statement (`stmt`) `number` times.\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[39mhttps://docs.python.org/3/library/timeit.html#timeit.Timer.timeit\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[39mwith\u001b[39;00m common\u001b[39m.\u001b[39mset_torch_threads(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_spec\u001b[39m.\u001b[39mnum_threads):\n\u001b[1;32m    260\u001b[0m     \u001b[39m# Warmup\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_timer\u001b[39m.\u001b[39;49mtimeit(number\u001b[39m=\u001b[39;49m\u001b[39mmax\u001b[39;49m(\u001b[39mint\u001b[39;49m(number \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m \u001b[39m100\u001b[39;49m), \u001b[39m2\u001b[39;49m))\n\u001b[1;32m    263\u001b[0m     \u001b[39mreturn\u001b[39;00m common\u001b[39m.\u001b[39mMeasurement(\n\u001b[1;32m    264\u001b[0m         number_per_run\u001b[39m=\u001b[39mnumber,\n\u001b[1;32m    265\u001b[0m         raw_times\u001b[39m=\u001b[39m[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timer\u001b[39m.\u001b[39mtimeit(number\u001b[39m=\u001b[39mnumber)],\n\u001b[1;32m    266\u001b[0m         task_spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_spec\n\u001b[1;32m    267\u001b[0m     )\n",
      "File \u001b[0;32m/usr/lib/python3.8/timeit.py:177\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    175\u001b[0m gc\u001b[39m.\u001b[39mdisable()\n\u001b[1;32m    176\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     timing \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner(it, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimer)\n\u001b[1;32m    178\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     \u001b[39mif\u001b[39;00m gcold:\n",
      "File \u001b[0;32m<timeit-src>:6\u001b[0m, in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn [15], line 89\u001b[0m, in \u001b[0;36mrun_inference\u001b[0;34m(model, input_tensor)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_inference\u001b[39m(model: nn\u001b[38;5;241m.\u001b[39mModule, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# from model.forward because BEVerse differentiates between different input types - img lidar etc \u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_dummy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ETM_BEV/BEVerse/projects/mmdet3d_plugin/models/detectors/beverse.py:209\u001b[0m, in \u001b[0;36mforward_dummy\u001b[0;34m(self, img_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39m\"\"\"Forward training function.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \n\u001b[1;32m    169\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39m    dict: Losses of different branches.\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    193\u001b[0m img_feats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextract_img_feat(\n\u001b[1;32m    194\u001b[0m     img\u001b[39m=\u001b[39mimg_inputs,\n\u001b[1;32m    195\u001b[0m     img_metas\u001b[39m=\u001b[39mimg_metas,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    198\u001b[0m     img_is_valid\u001b[39m=\u001b[39mimg_is_valid,\n\u001b[1;32m    199\u001b[0m )\n\u001b[1;32m    201\u001b[0m mtl_targets \u001b[39m=\u001b[39m {\n\u001b[1;32m    202\u001b[0m     \u001b[39m# for detection\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mgt_bboxes_3d\u001b[39m\u001b[39m\"\u001b[39m: gt_bboxes_3d,\n\u001b[1;32m    204\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mgt_labels_3d\u001b[39m\u001b[39m\"\u001b[39m: gt_labels_3d,\n\u001b[1;32m    205\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mgt_bboxes_ignore\u001b[39m\u001b[39m\"\u001b[39m: gt_bboxes_ignore,\n\u001b[1;32m    206\u001b[0m     \u001b[39m# for map segmentation\u001b[39;00m\n\u001b[1;32m    207\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39msemantic_seg\u001b[39m\u001b[39m\"\u001b[39m: semantic_indices,\n\u001b[1;32m    208\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39msemantic_map\u001b[39m\u001b[39m\"\u001b[39m: semantic_map,\n\u001b[0;32m--> 209\u001b[0m     \u001b[39m# for motion prediction\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmotion_segmentation\u001b[39m\u001b[39m\"\u001b[39m: motion_segmentation,\n\u001b[1;32m    211\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmotion_instance\u001b[39m\u001b[39m\"\u001b[39m: motion_instance,\n\u001b[1;32m    212\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minstance_centerness\u001b[39m\u001b[39m\"\u001b[39m: instance_centerness,\n\u001b[1;32m    213\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minstance_offset\u001b[39m\u001b[39m\"\u001b[39m: instance_offset,\n\u001b[1;32m    214\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minstance_flow\u001b[39m\u001b[39m\"\u001b[39m: instance_flow,\n\u001b[1;32m    215\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mfuture_egomotion\u001b[39m\u001b[39m\"\u001b[39m: future_egomotions,\n\u001b[1;32m    216\u001b[0m     \u001b[39m# for bev_augmentation\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39maug_transform\u001b[39m\u001b[39m\"\u001b[39m: aug_transform,\n\u001b[1;32m    218\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mimg_is_valid\u001b[39m\u001b[39m\"\u001b[39m: img_is_valid,\n\u001b[1;32m    219\u001b[0m }\n\u001b[1;32m    221\u001b[0m loss_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_pts_train(img_feats, img_metas, mtl_targets)\n\u001b[1;32m    223\u001b[0m \u001b[39mreturn\u001b[39;00m loss_dict\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/torch/serialization.py:594\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    592\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 594\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    595\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    596\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    598\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    599\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    231\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dummy_input.pt'"
     ]
    }
   ],
   "source": [
    "\n",
    "num_warmups = 100\n",
    "num_repeats = 1000\n",
    "# Change to C x 1 x 3 x 1600 x 900\n",
    "# 704×256 Tiny\n",
    "# 1408×512\n",
    "# B, T, N, C, imH, imW = imgs.shape\n",
    "input_shape = (1,3,6,3,1600,900)\n",
    "\n",
    "future_egomotions = torch.zeros((batch_size, 7, 6)).type_as(img_inputs)\n",
    "img_is_valid = torch.ones((batch_size, 7)).type_as(img_inputs) > 0\n",
    "print(\"Future Egomotions: \", future_egomotions.shape)\n",
    "print(\"img_is_valid: \", img_is_valid.shape)\n",
    "\n",
    "\n",
    "# imgs = imgs.view(B * S * N, C, imH, imW)\n",
    "\n",
    "model = build_model(cfg.model, test_cfg=cfg.get(\"test_cfg\"))\n",
    "wrap_fp16_model(model)\n",
    "# load_checkpoint(\n",
    "#     model,\n",
    "#     r\"/home/niklas/ETM_BEV/BEVerse/checkpoints/beverse_tiny.pth\",\n",
    "#     map_location=\"cpu\",\n",
    "# )\n",
    "# model = fuse_module(model)\n",
    "model.cuda(device)\n",
    "model.eval()\n",
    "# model = nn.Conv2d(in_channels=input_shape[1], out_channels=256, kernel_size=(5, 5))\n",
    "\n",
    "# Input tensor\n",
    "input_tensor = torch.rand(input_shape, device=device)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "print(\"Latency Measurement Using CPU Timer...\")\n",
    "for continuous_measure in [True]:\n",
    "    for synchronize in [True]:\n",
    "        try:\n",
    "            latency_ms = measure_time_host(\n",
    "                model=model,\n",
    "                input_tensor=input_tensor,\n",
    "                num_repeats=num_repeats,\n",
    "                num_warmups=num_warmups,\n",
    "                synchronize=synchronize,\n",
    "                continuous_measure=continuous_measure,\n",
    "            )\n",
    "            print(\n",
    "                f\"|\"\n",
    "                f\"Synchronization: {synchronize!s:5}| \"\n",
    "                f\"Continuous Measurement: {continuous_measure!s:5}| \"\n",
    "                f\"Latency: {latency_ms:.5f} ms| \"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"|\"\n",
    "                f\"Synchronization: {synchronize!s:5}| \"\n",
    "                f\"Continuous Measurement: {continuous_measure!s:5}| \"\n",
    "                f\"Latency: N/A     ms| \"\n",
    "            )\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "print(\"Latency Measurement Using CUDA Timer...\")\n",
    "for continuous_measure in [True, False]:\n",
    "    for synchronize in [True, False]:\n",
    "        try:\n",
    "            latency_ms = measure_time_device(\n",
    "                model=model,\n",
    "                input_tensor=input_tensor,\n",
    "                num_repeats=num_repeats,\n",
    "                num_warmups=num_warmups,\n",
    "                synchronize=synchronize,\n",
    "                continuous_measure=continuous_measure,\n",
    "            )\n",
    "            print(\n",
    "                f\"|\"\n",
    "                f\"Synchronization: {synchronize!s:5}| \"\n",
    "                f\"Continuous Measurement: {continuous_measure!s:5}| \"\n",
    "                f\"Latency: {latency_ms:.5f} ms| \"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"|\"\n",
    "                f\"Synchronization: {synchronize!s:5}| \"\n",
    "                f\"Continuous Measurement: {continuous_measure!s:5}| \"\n",
    "                f\"Latency: N/A     ms| \"\n",
    "            )\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "print(\"Latency Measurement Using PyTorch Benchmark...\")\n",
    "num_threads = 1\n",
    "timer = benchmark.Timer(\n",
    "    stmt=\"run_inference(model, input_tensor)\",\n",
    "    setup=\"from __main__ import run_inference\",\n",
    "    globals={\"model\": model, \"input_tensor\": input_tensor},\n",
    "    num_threads=num_threads,\n",
    "    label=\"Latency Measurement\",\n",
    "    sub_label=\"torch.utils.benchmark.\",\n",
    ")\n",
    "\n",
    "profile_result = timer.timeit(num_repeats)\n",
    "# https://pytorch.org/docs/stable/_modules/torch/utils/benchmark/utils/common.html#Measurement\n",
    "print(f\"Latency: {profile_result.mean * 1000:.5f} ms\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49a6cb26e152f15aca94d1d3fa9630fb57fb8fd83a336982cd2ebf9e9635e69c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
