{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging \n",
    "from custome_logger import setup_custom_logger\n",
    "logger = setup_custom_logger()\n",
    "logger.debug(\"test\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.benchmark as benchmark\n",
    "from timeit import default_timer as timer\n",
    "import warnings\n",
    "from mmcv import Config, DictAction\n",
    "from mmcv.runner import get_dist_info, init_dist\n",
    "from mmcv.runner import load_checkpoint, wrap_fp16_model\n",
    "from os import path as osp\n",
    "from mmdet3d.models import build_model\n",
    "from mmdet3d.datasets import build_dataset\n",
    "from mmcv.parallel import MMDataParallel\n",
    "from mmdet.datasets import (build_dataloader, build_dataset,\n",
    "                            replace_ImageToTensor)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "\n",
    "cfg = Config.fromfile(\n",
    "    r\"/home/niklas/ETM_BEV/BEVerse/projects/configs/beverse_tiny_exp.py\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'BEVerse',\n",
       " 'img_backbone': {'type': 'SwinTransformer',\n",
       "  'pretrained': 'https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth',\n",
       "  'pretrain_img_size': 224,\n",
       "  'embed_dims': 96,\n",
       "  'patch_size': 4,\n",
       "  'window_size': 7,\n",
       "  'mlp_ratio': 4,\n",
       "  'depths': [2, 2, 6, 2],\n",
       "  'num_heads': [3, 6, 12, 24],\n",
       "  'strides': (4, 2, 2, 2),\n",
       "  'out_indices': (2, 3),\n",
       "  'qkv_bias': True,\n",
       "  'qk_scale': None,\n",
       "  'patch_norm': True,\n",
       "  'drop_rate': 0.0,\n",
       "  'attn_drop_rate': 0.0,\n",
       "  'drop_path_rate': 0.0,\n",
       "  'use_abs_pos_embed': False,\n",
       "  'act_cfg': {'type': 'GELU'},\n",
       "  'norm_cfg': {'type': 'LN', 'requires_grad': True},\n",
       "  'pretrain_style': 'official',\n",
       "  'output_missing_index_as_none': False},\n",
       " 'img_neck': {'type': 'FPN_LSS', 'in_channels': 1152, 'inverse': True},\n",
       " 'transformer': {'type': 'TransformerLSS',\n",
       "  'grid_conf': {'xbound': [-51.2, 51.2, 0.2],\n",
       "   'ybound': [-51.2, 51.2, 0.2],\n",
       "   'zbound': [-10.0, 10.0, 20.0],\n",
       "   'dbound': [1.0, 60.0, 0.5]},\n",
       "  'input_dim': (256, 704),\n",
       "  'numC_input': 512,\n",
       "  'numC_Trans': 64},\n",
       " 'temporal_model': {'type': 'Temporal3DConvModel',\n",
       "  'grid_conf': {'xbound': [-51.2, 51.2, 0.2],\n",
       "   'ybound': [-51.2, 51.2, 0.2],\n",
       "   'zbound': [-10.0, 10.0, 20.0],\n",
       "   'dbound': [1.0, 60.0, 0.5]},\n",
       "  'receptive_field': 4,\n",
       "  'input_egopose': True,\n",
       "  'in_channels': 64,\n",
       "  'input_shape': (96, 167),\n",
       "  'with_skip_connect': True},\n",
       " 'pts_bbox_head': {'type': 'MultiTaskHead',\n",
       "  'in_channels': 64,\n",
       "  'out_channels': 256,\n",
       "  'grid_conf': {'xbound': [-51.2, 51.2, 0.2],\n",
       "   'ybound': [-51.2, 51.2, 0.2],\n",
       "   'zbound': [-10.0, 10.0, 20.0],\n",
       "   'dbound': [1.0, 60.0, 0.5]},\n",
       "  'det_grid_conf': {'xbound': [-51.2, 51.2, 0.2],\n",
       "   'ybound': [-51.2, 51.2, 0.2],\n",
       "   'zbound': [-10.0, 10.0, 20.0],\n",
       "   'dbound': [1.0, 60.0, 0.5]},\n",
       "  'map_grid_conf': {'xbound': [-30.0, 30.0, 0.15],\n",
       "   'ybound': [-15.0, 15.0, 0.15],\n",
       "   'zbound': [-10.0, 10.0, 20.0],\n",
       "   'dbound': [1.0, 60.0, 0.5]},\n",
       "  'motion_grid_conf': {'xbound': [-50.0, 50.0, 0.2],\n",
       "   'ybound': [-50.0, 50.0, 0.2],\n",
       "   'zbound': [-10.0, 10.0, 20.0],\n",
       "   'dbound': [1.0, 60.0, 0.5]},\n",
       "  'using_ego': True,\n",
       "  'task_enbale': {'3dod': True, 'map': True, 'motion': True},\n",
       "  'task_weights': {'3dod': 1.0, 'map': 10.0, 'motion': 1.0},\n",
       "  'bev_encode_block': 'Basic',\n",
       "  'cfg_3dod': {'type': 'CenterHeadv1',\n",
       "   'in_channels': 256,\n",
       "   'tasks': [{'num_class': 1, 'class_names': ['car']},\n",
       "    {'num_class': 2, 'class_names': ['truck', 'construction_vehicle']},\n",
       "    {'num_class': 2, 'class_names': ['bus', 'trailer']},\n",
       "    {'num_class': 1, 'class_names': ['barrier']},\n",
       "    {'num_class': 2, 'class_names': ['motorcycle', 'bicycle']},\n",
       "    {'num_class': 2, 'class_names': ['pedestrian', 'traffic_cone']}],\n",
       "   'common_heads': {'reg': (2, 2),\n",
       "    'height': (1, 2),\n",
       "    'dim': (3, 2),\n",
       "    'rot': (2, 2),\n",
       "    'vel': (2, 2)},\n",
       "   'share_conv_channel': 64,\n",
       "   'bbox_coder': {'type': 'CenterPointBBoxCoder',\n",
       "    'pc_range': [-51.2, -51.2],\n",
       "    'post_center_range': [-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],\n",
       "    'max_num': 500,\n",
       "    'score_threshold': 0.1,\n",
       "    'out_size_factor': 8,\n",
       "    'voxel_size': [0.1, 0.1],\n",
       "    'code_size': 9},\n",
       "   'separate_head': {'type': 'SeparateHead',\n",
       "    'init_bias': -2.19,\n",
       "    'final_kernel': 3},\n",
       "   'loss_cls': {'type': 'GaussianFocalLoss', 'reduction': 'mean'},\n",
       "   'loss_bbox': {'type': 'L1Loss', 'reduction': 'mean', 'loss_weight': 0.25},\n",
       "   'norm_bbox': True},\n",
       "  'cfg_map': {'type': 'MapHead',\n",
       "   'task_dict': {'semantic_seg': 4},\n",
       "   'in_channels': 256,\n",
       "   'class_weights': [1.0, 2.0, 2.0, 2.0],\n",
       "   'semantic_thresh': 0.25},\n",
       "  'cfg_motion': {'type': 'IterativeFlow',\n",
       "   'task_dict': {'segmentation': 2,\n",
       "    'instance_center': 1,\n",
       "    'instance_offset': 2,\n",
       "    'instance_flow': 2},\n",
       "   'in_channels': 256,\n",
       "   'grid_conf': {'xbound': [-50.0, 50.0, 0.2],\n",
       "    'ybound': [-50.0, 50.0, 0.2],\n",
       "    'zbound': [-10.0, 10.0, 20.0],\n",
       "    'dbound': [1.0, 60.0, 0.5]},\n",
       "   'class_weights': [1.0, 2.0],\n",
       "   'receptive_field': 4,\n",
       "   'n_future': 4,\n",
       "   'future_discount': 0.95,\n",
       "   'using_focal_loss': True,\n",
       "   'prob_latent_dim': 32,\n",
       "   'future_dim': 6,\n",
       "   'distribution_log_sigmas': [-5.0, 5.0],\n",
       "   'n_gru_blocks': 1,\n",
       "   'n_res_layers': 3,\n",
       "   'loss_weights': {'loss_motion_seg': 5.0,\n",
       "    'loss_motion_centerness': 1.0,\n",
       "    'loss_motion_offset': 1.0,\n",
       "    'loss_motion_flow': 1.0,\n",
       "    'loss_motion_prob': 10.0},\n",
       "   'using_spatial_prob': True,\n",
       "   'sample_ignore_mode': 'past_valid',\n",
       "   'posterior_with_label': False}},\n",
       " 'train_cfg': {'pts': {'point_cloud_range': [-51.2,\n",
       "    -51.2,\n",
       "    -5.0,\n",
       "    51.2,\n",
       "    51.2,\n",
       "    3.0],\n",
       "   'grid_size': [1024, 1024, 40],\n",
       "   'voxel_size': [0.1, 0.1, 0.2],\n",
       "   'out_size_factor': 8,\n",
       "   'dense_reg': 1,\n",
       "   'gaussian_overlap': 0.1,\n",
       "   'max_objs': 500,\n",
       "   'min_radius': 2,\n",
       "   'code_weights': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}},\n",
       " 'test_cfg': {'pts': {'pc_range': [-51.2, -51.2],\n",
       "   'post_center_limit_range': [-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],\n",
       "   'max_per_img': 500,\n",
       "   'max_pool_nms': False,\n",
       "   'min_radius': [4, 12, 10, 1, 0.85, 0.175],\n",
       "   'score_threshold': 0.1,\n",
       "   'out_size_factor': 8,\n",
       "   'voxel_size': [0.1, 0.1],\n",
       "   'pre_max_size': 1000,\n",
       "   'post_max_size': 83,\n",
       "   'nms_type': ['rotate', 'rotate', 'rotate', 'circle', 'rotate', 'rotate'],\n",
       "   'nms_thr': [0.2, 0.2, 0.2, 0.2, 0.2, 0.5],\n",
       "   'nms_rescale_factor': [1.0,\n",
       "    [0.7, 0.7],\n",
       "    [0.4, 0.55],\n",
       "    1.1,\n",
       "    [1.0, 1.0],\n",
       "    [4.5, 9.0]]}}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_cfg(\n",
    "    cfg,\n",
    "    n_future=3,\n",
    "    receptive_field=3,\n",
    "    resize_lim=(0.38, 0.55),\n",
    "    final_dim=(256, 704),\n",
    "    grid_conf={\n",
    "        \"xbound\": [-51.2, 51.2, 0.8],\n",
    "        \"ybound\": [-51.2, 51.2, 0.8],\n",
    "        \"zbound\": [-10.0, 10.0, 20.0],\n",
    "        \"dbound\": [1.0, 60.0, 1.0],\n",
    "    },\n",
    "    det_grid_conf={\n",
    "        \"xbound\": [-51.2, 51.2, 0.8],\n",
    "        \"ybound\": [-51.2, 51.2, 0.8],\n",
    "        \"zbound\": [-10.0, 10.0, 20.0],\n",
    "        \"dbound\": [1.0, 60.0, 1.0],\n",
    "    },\n",
    "    map_grid_conf={\n",
    "        \"xbound\": [-30.0, 30.0, 0.15],\n",
    "        \"ybound\": [-15.0, 15.0, 0.15],\n",
    "        \"zbound\": [-10.0, 10.0, 20.0],\n",
    "        \"dbound\": [1.0, 60.0, 1.0],\n",
    "    },\n",
    "    motion_grid_conf={\n",
    "        \"xbound\": [-50.0, 50.0, 0.5],\n",
    "        \"ybound\": [-50.0, 50.0, 0.5],\n",
    "        \"zbound\": [-10.0, 10.0, 20.0],\n",
    "        \"dbound\": [1.0, 60.0, 1.0],\n",
    "    },\n",
    "    t_input_shape=(128, 128),\n",
    "    point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],\n",
    "):\n",
    "    \n",
    "    cfg[\"det_grid_conf\"] = det_grid_conf\n",
    "    cfg[\"map_grid_conf\"] = map_grid_conf\n",
    "    cfg[\"motion_grid_conf\"] = motion_grid_conf\n",
    "    cfg[\"grid_conf\"] = det_grid_conf\n",
    "\n",
    "    cfg[\"model\"][\"temporal_model\"][\"input_shape\"] = t_input_shape\n",
    "\n",
    "    cfg[\"data\"][\"val\"][\"pipeline\"][0][\"data_aug_conf\"][\"resize_lim\"] = resize_lim\n",
    "    cfg[\"data\"][\"train\"][\"dataset\"][\"pipeline\"][0][\"data_aug_conf\"][\n",
    "        \"resize_lim\"\n",
    "    ] = resize_lim\n",
    "    cfg[\"data\"][\"val\"][\"pipeline\"][0][\"data_aug_conf\"][\"final_dim\"] = final_dim\n",
    "    cfg[\"data\"][\"train\"][\"dataset\"][\"pipeline\"][0][\"data_aug_conf\"][\n",
    "        \"final_dim\"\n",
    "    ] = final_dim\n",
    "\n",
    "    cfg[\"data\"][\"test\"][\"pipeline\"][0][\"data_aug_conf\"][\"resize_lim\"] = resize_lim\n",
    "    cfg[\"data\"][\"test\"][\"pipeline\"][0][\"data_aug_conf\"][\"final_dim\"] = final_dim\n",
    "\n",
    "    cfg[\"model\"][\"pts_bbox_head\"][\"cfg_motion\"][\n",
    "        \"grid_conf\"\n",
    "    ] = motion_grid_conf  # motion_grid\n",
    "    cfg[\"model\"][\"temporal_model\"][\"grid_conf\"] = grid_conf\n",
    "    cfg[\"model\"][\"transformer\"][\"grid_conf\"] = grid_conf\n",
    "    cfg[\"model\"][\"pts_bbox_head\"][\"grid_conf\"] = grid_conf\n",
    "    cfg[\"data\"][\"train\"][\"dataset\"][\"grid_conf\"] = motion_grid_conf\n",
    "    cfg[\"data\"][\"val\"][\"pipeline\"][3][\"grid_conf\"] = motion_grid_conf\n",
    "    cfg[\"data\"][\"test\"][\"pipeline\"][3][\"grid_conf\"] = motion_grid_conf\n",
    "    cfg[\"data\"][\"val\"][\"grid_conf\"] = grid_conf\n",
    "    cfg[\"data\"][\"test\"][\"grid_conf\"] = grid_conf\n",
    "\n",
    "    cfg[\"model\"][\"pts_bbox_head\"][\"det_grid_conf\"] = det_grid_conf\n",
    "\n",
    "    cfg[\"model\"][\"pts_bbox_head\"][\"map_grid_conf\"] = map_grid_conf\n",
    "    cfg[\"data\"][\"train\"][\"dataset\"][\"map_grid_conf\"] = map_grid_conf\n",
    "    cfg[\"data\"][\"test\"][\"pipeline\"][2][\"map_grid_conf\"] = map_grid_conf\n",
    "    cfg[\"data\"][\"val\"][\"pipeline\"][2][\"map_grid_conf\"] = map_grid_conf\n",
    "    cfg[\"data\"][\"test\"][\"map_grid_conf\"] = map_grid_conf\n",
    "    cfg[\"data\"][\"val\"][\"map_grid_conf\"] = map_grid_conf\n",
    "\n",
    "    cfg[\"model\"][\"pts_bbox_head\"][\"motion_grid_conf\"] = motion_grid_conf\n",
    "\n",
    "    cfg[\"data\"][\"test\"][\"pipeline\"][5][\"point_cloud_range\"] = point_cloud_range\n",
    "    cfg[\"data\"][\"train\"][\"pipeline\"][5][\n",
    "        \"point_cloud_range\"\n",
    "    ] = point_cloud_range  # point_cloud_range=None\n",
    "    cfg[\"data\"][\"train\"][\"pipeline\"][6][\n",
    "        \"point_cloud_range\"\n",
    "    ] = point_cloud_range  #'point_cloud_range =None\n",
    "    cfg[\"data\"][\"val\"][\"pipeline\"][5][\"point_cloud_range\"] = point_cloud_range\n",
    "\n",
    "    cfg[\"model\"][\"pts_bbox_head\"][\"cfg_motion\"][\"receptive_field\"] = receptive_field\n",
    "    cfg[\"data\"][\"train\"][\"dataset\"][\"receptive_field\"] = receptive_field\n",
    "    cfg[\"model\"][\"temporal_model\"][\"receptive_field\"] = receptive_field\n",
    "    cfg[\"data\"][\"test\"][\"receptive_field\"] = receptive_field\n",
    "    cfg[\"data\"][\"val\"][\"receptive_field\"] = receptive_field\n",
    "\n",
    "    cfg[\"data\"][\"val\"][\"future_frames\"] = n_future\n",
    "    cfg[\"model\"][\"pts_bbox_head\"][\"cfg_motion\"][\"n_future\"] = n_future\n",
    "    cfg[\"data\"][\"test\"][\"future_frames\"] = n_future\n",
    "    cfg[\"data\"][\"train\"][\"dataset\"][\"future_frames\"] = n_future\n",
    "    \n",
    "    cfg[\"data\"][\"train\"][\"dataset\"][\"pipeline\"][4][\"map_grid_conf\"] = map_grid_conf\n",
    "    cfg[\"data\"][\"train\"][\"dataset\"][\"pipeline\"][5][\"grid_conf\"] = motion_grid_conf\n",
    "    cfg[\"data\"][\"train\"][\"dataset\"][\"pipeline\"][7][\"point_cloud_range\"] = point_cloud_range\n",
    "\n",
    "    return cfg\n",
    "\n",
    "def import_modules_load_config(cfg_file=\"beverse_tiny.py\", samples_per_gpu=1):\n",
    "    cfg_path = r\"/home/niklas/ETM_BEV/BEVerse/projects/configs\"\n",
    "    cfg_path = os.path.join(cfg_path, cfg_file)\n",
    "\n",
    "    cfg = Config.fromfile(cfg_path)\n",
    "\n",
    "    # if args.cfg_options is not None:\n",
    "    #     cfg.merge_from_dict(args.cfg_options)\n",
    "    # import modules from string list.\n",
    "    if cfg.get(\"custom_imports\", None):\n",
    "        from mmcv.utils import import_modules_from_strings\n",
    "\n",
    "        import_modules_from_strings(**cfg[\"custom_imports\"])\n",
    "\n",
    "    # import modules from plguin/xx, registry will be updated\n",
    "    if hasattr(cfg, \"plugin\"):\n",
    "        if cfg.plugin:\n",
    "            import importlib\n",
    "\n",
    "            if hasattr(cfg, \"plugin_dir\"):\n",
    "                plugin_dir = cfg.plugin_dir\n",
    "                _module_dir = os.path.dirname(plugin_dir)\n",
    "                _module_dir = _module_dir.split(\"/\")\n",
    "                _module_path = _module_dir[0]\n",
    "\n",
    "                for m in _module_dir[1:]:\n",
    "                    _module_path = _module_path + \".\" + m\n",
    "                print(_module_path)\n",
    "                plg_lib = importlib.import_module(_module_path)\n",
    "            else:\n",
    "                # import dir is the dirpath for the config file\n",
    "                _module_dir = cfg_path\n",
    "                _module_dir = _module_dir.split(\"/\")\n",
    "                _module_path = _module_dir[0]\n",
    "                for m in _module_dir[1:]:\n",
    "                    _module_path = _module_path + \".\" + m\n",
    "                print(_module_path)\n",
    "                plg_lib = importlib.import_module(_module_path)\n",
    "\n",
    "    samples_per_gpu = 1\n",
    "    if isinstance(cfg.data.test, dict):\n",
    "        cfg.data.test.test_mode = True\n",
    "        samples_per_gpu = cfg.data.test.pop(\"samples_per_gpu\", 1)\n",
    "        if samples_per_gpu > 1:\n",
    "            # Replace 'ImageToTensor' to 'DefaultFormatBundle'\n",
    "            cfg.data.test.pipeline = replace_ImageToTensor(cfg.data.test.pipeline)\n",
    "    elif isinstance(cfg.data.test, list):\n",
    "        for ds_cfg in cfg.data.test:\n",
    "            ds_cfg.test_mode = True\n",
    "        samples_per_gpu = max(\n",
    "            [ds_cfg.pop(\"samples_per_gpu\", 1) for ds_cfg in cfg.data.test]\n",
    "        )\n",
    "        if samples_per_gpu > 1:\n",
    "            for ds_cfg in cfg.data.test:\n",
    "                ds_cfg.pipeline = replace_ImageToTensor(ds_cfg.pipeline)\n",
    "\n",
    "    return cfg\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def measure_time_host(\n",
    "    model: nn.Module,\n",
    "    input_tensor: torch.Tensor,\n",
    "    num_repeats: int = 100,\n",
    "    num_warmups: int = 10,\n",
    "    synchronize: bool = True,\n",
    "    continuous_measure: bool = True,\n",
    ") -> float:\n",
    "\n",
    "    for _ in range(num_warmups):\n",
    "        _ = model.forward(input_tensor)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    elapsed_time_ms = 0\n",
    "\n",
    "    if continuous_measure:\n",
    "        start = timer()\n",
    "        for _ in range(num_repeats):\n",
    "            _ = model.forward(input_tensor)\n",
    "        if synchronize:\n",
    "            torch.cuda.synchronize()\n",
    "        end = timer()\n",
    "        elapsed_time_ms = (end - start) * 1000\n",
    "\n",
    "    else:\n",
    "        for _ in range(num_repeats):\n",
    "            start = timer()\n",
    "            _ = model.forward(input_tensor)\n",
    "            if synchronize:\n",
    "                torch.cuda.synchronize()\n",
    "            end = timer()\n",
    "            elapsed_time_ms += (end - start) * 1000\n",
    "\n",
    "    return elapsed_time_ms / num_repeats\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def measure_time_device(\n",
    "    model: nn.Module,\n",
    "    input_tensor: torch.Tensor,\n",
    "    num_repeats: int = 100,\n",
    "    num_warmups: int = 10,\n",
    "    synchronize: bool = True,\n",
    "    continuous_measure: bool = True,\n",
    ") -> float:\n",
    "\n",
    "    for _ in range(num_warmups):\n",
    "        _ = model.forward(input_tensor)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    elapsed_time_ms = 0\n",
    "\n",
    "    if continuous_measure:\n",
    "        start_event = torch.cuda.Event(enable_timing=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True)\n",
    "        start_event.record()\n",
    "        for _ in range(num_repeats):\n",
    "            _ = model.forward(input_tensor)\n",
    "        end_event.record()\n",
    "        if synchronize:\n",
    "            # This has to be synchronized to compute the elapsed time.\n",
    "            # Otherwise, there will be runtime error.\n",
    "            torch.cuda.synchronize()\n",
    "        elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "\n",
    "    else:\n",
    "        for _ in range(num_repeats):\n",
    "            start_event = torch.cuda.Event(enable_timing=True)\n",
    "            end_event = torch.cuda.Event(enable_timing=True)\n",
    "            start_event.record()\n",
    "            _ = model.forward(input_tensor)\n",
    "            end_event.record()\n",
    "            if synchronize:\n",
    "                # This has to be synchronized to compute the elapsed time.\n",
    "                # Otherwise, there will be runtime error.\n",
    "                torch.cuda.synchronize()\n",
    "            elapsed_time_ms += start_event.elapsed_time(end_event)\n",
    "\n",
    "    return elapsed_time_ms / num_repeats\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_inference(model: nn.Module, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    # from model.forward because BEVerse differentiates between different input types - img lidar etc \n",
    "    return model.forward_dummy()\n",
    "\n",
    "def calculate_birds_eye_view_parameters(x_bounds, y_bounds, z_bounds):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "        x_bounds: Forward direction in the ego-car.\n",
    "        y_bounds: Sides\n",
    "        z_bounds: Height\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        bev_resolution: Bird's-eye view bev_resolution\n",
    "        bev_start_position Bird's-eye view first element\n",
    "        bev_dimension Bird's-eye view tensor spatial dimension\n",
    "    \"\"\"\n",
    "    bev_resolution = torch.tensor([row[2] for row in [x_bounds, y_bounds, z_bounds]])\n",
    "    bev_start_position = torch.tensor(\n",
    "        [row[0] + row[2] / 2.0 for row in [x_bounds, y_bounds, z_bounds]]\n",
    "    )\n",
    "    bev_dimension = torch.tensor(\n",
    "        [(row[1] - row[0]) / row[2] for row in [x_bounds, y_bounds, z_bounds]],\n",
    "        dtype=torch.long,\n",
    "    )\n",
    "\n",
    "    return bev_resolution, bev_start_position, bev_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_lims = [\n",
    "    (0.3, 0.45),  # fiery\n",
    "    (0.38, 0.55),  # desTINY\n",
    "    (0.82, 0.99),  # small\n",
    "    (1, 1),  # BEVDEt\n",
    "]\n",
    "\n",
    "final_dims = [(224, 480), (256, 704), (512, 1408), (900, 1600)]\n",
    "\n",
    "backbones = [\n",
    "    \"beverse_tiny.py\",\n",
    "    \"beverse_tiny.py\",\n",
    "    \"beverse_small.py\",\n",
    "    \"beverse_small.py\",\n",
    "]\n",
    "\n",
    "# future frames -> tiny settings\n",
    "future_frames_list = [4, 4, 4, 4, 5, 7, 10]\n",
    "receptive_field_list = [\n",
    "    3,\n",
    "    5,\n",
    "    8,\n",
    "    13,\n",
    "    4,\n",
    "    6,\n",
    "    9,\n",
    "]\n",
    "\n",
    "# grid_size = (\n",
    "#     point_cloud_range[3:] -  # type: ignore\n",
    "#     point_cloud_range[:3]) / voxel_size  # type: ignore\n",
    "\n",
    "point_cloud_range_base = [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]\n",
    "point_cloud_range_extended_fustrum = [-71.2, -71.2, -5.0, 71.2, 71.2, 3.0]\n",
    "det_grid_confs = {\n",
    "    \"xbound\": [\n",
    "        [-51.2, 51.2, 0.8],  # lower_bound, upper_bound, interval\n",
    "        [-51.2, 51.2, 0.4],\n",
    "        [-51.2, 51.2, 0.2],\n",
    "        [-51.2, 51.2, 0.1],\n",
    "        [-26.2, 26.2, 0.8],\n",
    "        [-26.2, 26.2, 0.4],\n",
    "    ],\n",
    "    \"ybound\": [\n",
    "        [-51.2, 51.2, 0.8],\n",
    "        [-51.2, 51.2, 0.4],\n",
    "        [-51.2, 51.2, 0.2],\n",
    "        [-51.2, 51.2, 0.1],\n",
    "        [-71.2, 71.2, 0.8],\n",
    "        [-71.2, 71.2, 0.4],\n",
    "    ],\n",
    "    \"zbound\": [-10.0, 10.0, 20.0],\n",
    "    \"dbound\": [\n",
    "        [1.0, 60.0, 1.0],\n",
    "        [1.0, 60.0, 1.0],\n",
    "        [1.0, 60.0, 0.5],\n",
    "        [1.0, 60.0, 0.5],\n",
    "        [1.0, 70.0, 1.0],\n",
    "        [1.0, 70.0, 5.0],\n",
    "    ],  # [(lower_bound, upper_bound, interval).]\n",
    "}\n",
    "\n",
    "motion_grid_confs = {\n",
    "    \"xbound\": [\n",
    "        [-50.0, 50.0, 0.5],\n",
    "        [-50.0, 50.0, 0.25],\n",
    "        [-50.0, 50.0, 0.125],\n",
    "        [-50.0, 50.0, 0.075],\n",
    "        [-25.0, 25.0, 0.5],\n",
    "        [-25.0, 25.0, 0.25],\n",
    "    ],\n",
    "    \"ybound\": [\n",
    "        [-50.0, 50.0, 0.5],\n",
    "        [-50.0, 50.0, 0.25],\n",
    "        [-50.0, 50.0, 0.125],\n",
    "        [-50.0, 50.0, 0.075],\n",
    "        [-70.0, 70.0, 0.5],\n",
    "        [-70.0, 70.0, 0.25],\n",
    "    ],\n",
    "    \"zbound\": [-10.0, 10.0, 20.0],\n",
    "    \"dbound\": [\n",
    "        [1.0, 60.0, 1.0],\n",
    "        [1.0, 60.0, 1.0],\n",
    "        [1.0, 60.0, 0.5],\n",
    "        [1.0, 60.0, 0.5],\n",
    "        [1.0, 70.0, 1.0],\n",
    "        [1.0, 70.0, 5.0],\n",
    "    ],\n",
    "}\n",
    "\n",
    "map_grid_confs = {\n",
    "    \"xbound\": [-30.0, 30.0, 0.15],\n",
    "    \"ybound\": [-15.0, 15.0, 0.15],\n",
    "    \"zbound\": [-10.0, 10.0, 20.0],\n",
    "    \"dbound\": [\n",
    "        [1.0, 60.0, 1.0],\n",
    "        [1.0, 60.0, 1.0],\n",
    "        [1.0, 60.0, 0.5],\n",
    "        [1.0, 60.0, 0.5],\n",
    "        [1.0, 70.0, 1.0],\n",
    "        [1.0, 70.0, 5.0],\n",
    "    ],\n",
    "}\n",
    "det_grid_conf = {\n",
    "    \"xbound\": [-51.2, 51.2, 0.8],\n",
    "    \"ybound\": [-51.2, 51.2, 0.8],\n",
    "    \"zbound\": [-10.0, 10.0, 20.0],\n",
    "    \"dbound\": [1.0, 60.0, 1.0],\n",
    "}\n",
    "\n",
    "motion_grid_conf = {\n",
    "    \"xbound\": [-50.0, 50.0, 0.5],\n",
    "    \"ybound\": [-50.0, 50.0, 0.5],\n",
    "    \"zbound\": [-10.0, 10.0, 20.0],\n",
    "    \"dbound\": [1.0, 60.0, 1.0],\n",
    "}\n",
    "\n",
    "map_grid_conf = {\n",
    "    \"xbound\": [-30.0, 30.0, 0.15],\n",
    "    \"ybound\": [-15.0, 15.0, 0.15],\n",
    "    \"zbound\": [-10.0, 10.0, 20.0],\n",
    "    \"dbound\": [1.0, 60.0, 1.0],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# det_grid_conf[\"xbound\"] = det_grid_confs[\"xbound\"][1]\n",
    "# det_grid_conf[\"ybound\"] = det_grid_confs[\"ybound\"][1]\n",
    "# det_grid_conf[\"zbound\"] = det_grid_confs[\"zbound\"]\n",
    "# det_grid_conf[\"dbound\"] = det_grid_confs[\"dbound\"][1]\n",
    "\n",
    "# motion_grid_conf[\"xbound\"] = motion_grid_confs[\"xbound\"][1]\n",
    "# motion_grid_conf[\"ybound\"] = motion_grid_confs[\"ybound\"][1]\n",
    "# motion_grid_conf[\"zbound\"] = motion_grid_confs[\"zbound\"]\n",
    "# motion_grid_conf[\"dbound\"] = motion_grid_confs[\"dbound\"][1]\n",
    "\n",
    "# map_grid_conf[\"xbound\"] = map_grid_confs[\"xbound\"]\n",
    "# map_grid_conf[\"ybound\"] = map_grid_confs[\"ybound\"]\n",
    "# map_grid_conf[\"zbound\"] = map_grid_confs[\"zbound\"]\n",
    "# map_grid_conf[\"dbound\"] = map_grid_confs[\"dbound\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects.mmdet3d_plugin\n"
     ]
    }
   ],
   "source": [
    "# cfg = import_modules_load_config(cfg_file=backbones[0])\n",
    "# cfg = update_cfg(\n",
    "#     cfg,receptive_field=55,n_future=10\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = build_dataset(cfg.data.test)\n",
    "data_loader = build_dataloader(\n",
    "    dataset,\n",
    "    samples_per_gpu=1,\n",
    "    workers_per_gpu=cfg.data.workers_per_gpu,\n",
    "    dist=False,\n",
    "    shuffle=False,\n",
    ")\n",
    "iter_loader = iter(data_loader)\n",
    "sample = next(iter_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects.mmdet3d_plugin\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# device = torch.device(\"cuda:0\")\n",
    "# cfg = Config.fromfile(\n",
    "#     r\"/home/niklas/ETM_BEV/BEVerse/projects/configs/beverse_tiny.py\"\n",
    "# )\n",
    "\n",
    "# if args.cfg_options is not None:\n",
    "#     cfg.merge_from_dict(args.cfg_options)\n",
    "# import modules from string list.\n",
    "if cfg.get(\"custom_imports\", None):\n",
    "    from mmcv.utils import import_modules_from_strings\n",
    "\n",
    "    import_modules_from_strings(**cfg[\"custom_imports\"])\n",
    "\n",
    "# import modules from plguin/xx, registry will be updated\n",
    "if hasattr(cfg, \"plugin\"):\n",
    "    if cfg.plugin:\n",
    "        import importlib\n",
    "\n",
    "        if hasattr(cfg, \"plugin_dir\"):\n",
    "            plugin_dir = cfg.plugin_dir\n",
    "            _module_dir = os.path.dirname(plugin_dir)\n",
    "            _module_dir = _module_dir.split(\"/\")\n",
    "            _module_path = _module_dir[0]\n",
    "\n",
    "            for m in _module_dir[1:]:\n",
    "                _module_path = _module_path + \".\" + m\n",
    "            print(_module_path)\n",
    "            plg_lib = importlib.import_module(_module_path)\n",
    "        else:\n",
    "            # import dir is the dirpath for the config file\n",
    "            _module_dir = os.path.dirname(\n",
    "                r\"/home/niklas/ETM_BEV/BEVerse/projects/configs/beverse_tiny_exp.py\"\n",
    "            )\n",
    "            _module_dir = _module_dir.split(\"/\")\n",
    "            _module_path = _module_dir[0]\n",
    "            for m in _module_dir[1:]:\n",
    "                _module_path = _module_path + \".\" + m\n",
    "            print(_module_path)\n",
    "            plg_lib = importlib.import_module(_module_path)\n",
    "            \n",
    "\n",
    "samples_per_gpu = 1\n",
    "if isinstance(cfg.data.test, dict):\n",
    "    cfg.data.test.test_mode = True\n",
    "    samples_per_gpu = cfg.data.test.pop(\"samples_per_gpu\", 1)\n",
    "    if samples_per_gpu > 1:\n",
    "        # Replace 'ImageToTensor' to 'DefaultFormatBundle'\n",
    "        cfg.data.test.pipeline = replace_ImageToTensor(cfg.data.test.pipeline)\n",
    "elif isinstance(cfg.data.test, list):\n",
    "    for ds_cfg in cfg.data.test:\n",
    "        ds_cfg.test_mode = True\n",
    "    samples_per_gpu = max(\n",
    "        [ds_cfg.pop(\"samples_per_gpu\", 1) for ds_cfg in cfg.data.test]\n",
    "    )\n",
    "    if samples_per_gpu > 1:\n",
    "        for ds_cfg in cfg.data.test:\n",
    "            ds_cfg.pipeline = replace_ImageToTensor(ds_cfg.pipeline)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def measure_time_host(\n",
    "    model: nn.Module,\n",
    "    input_tensor: torch.Tensor,\n",
    "    num_repeats: int = 100,\n",
    "    num_warmups: int = 10,\n",
    "    synchronize: bool = True,\n",
    "    continuous_measure: bool = True,\n",
    ") -> float:\n",
    "\n",
    "    for _ in range(num_warmups):\n",
    "        _ = model.forward(input_tensor)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    elapsed_time_ms = 0\n",
    "\n",
    "    if continuous_measure:\n",
    "        start = timer()\n",
    "        for _ in range(num_repeats):\n",
    "            _ = model.forward(input_tensor)\n",
    "        if synchronize:\n",
    "            torch.cuda.synchronize()\n",
    "        end = timer()\n",
    "        elapsed_time_ms = (end - start) * 1000\n",
    "\n",
    "    else:\n",
    "        for _ in range(num_repeats):\n",
    "            start = timer()\n",
    "            _ = model.forward(input_tensor)\n",
    "            if synchronize:\n",
    "                torch.cuda.synchronize()\n",
    "            end = timer()\n",
    "            elapsed_time_ms += (end - start) * 1000\n",
    "\n",
    "    return elapsed_time_ms / num_repeats\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def measure_time_device(\n",
    "    model: nn.Module,\n",
    "    input_tensor: torch.Tensor,\n",
    "    num_repeats: int = 100,\n",
    "    num_warmups: int = 10,\n",
    "    synchronize: bool = True,\n",
    "    continuous_measure: bool = True,\n",
    ") -> float:\n",
    "\n",
    "    for _ in range(num_warmups):\n",
    "        _ = model.forward(input_tensor)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    elapsed_time_ms = 0\n",
    "\n",
    "    if continuous_measure:\n",
    "        start_event = torch.cuda.Event(enable_timing=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True)\n",
    "        start_event.record()\n",
    "        for _ in range(num_repeats):\n",
    "            _ = model.forward(input_tensor)\n",
    "        end_event.record()\n",
    "        if synchronize:\n",
    "            # This has to be synchronized to compute the elapsed time.\n",
    "            # Otherwise, there will be runtime error.\n",
    "            torch.cuda.synchronize()\n",
    "        elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "\n",
    "    else:\n",
    "        for _ in range(num_repeats):\n",
    "            start_event = torch.cuda.Event(enable_timing=True)\n",
    "            end_event = torch.cuda.Event(enable_timing=True)\n",
    "            start_event.record()\n",
    "            _ = model.forward(input_tensor)\n",
    "            end_event.record()\n",
    "            if synchronize:\n",
    "                # This has to be synchronized to compute the elapsed time.\n",
    "                # Otherwise, there will be runtime error.\n",
    "                torch.cuda.synchronize()\n",
    "            elapsed_time_ms += start_event.elapsed_time(end_event)\n",
    "\n",
    "    return elapsed_time_ms / num_repeats\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_inference(model: nn.Module, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "    return model.forward(input_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_grid_conf = {\n",
    "    \"xbound\": [-62.0, 62.0, 0.8],\n",
    "    \"ybound\": [-36.2, 36.2, 0.8],\n",
    "    \"zbound\": [-10.0, 10.0, 20.0],\n",
    "    \"dbound\": [1.0, 70.0, 1.0],\n",
    "}\n",
    "\n",
    "motion_grid_conf = {\n",
    "    \"xbound\": [-60.0, 60.0, 0.5],\n",
    "    \"ybound\": [-36.0, 36.0, 0.5],\n",
    "    \"zbound\": [-10.0, 10.0, 20.0],\n",
    "    \"dbound\": [1.0, 70.0, 1.0],\n",
    "}\n",
    "\n",
    "map_grid_conf = {\n",
    "    \"xbound\": [-30.0, 30.0, 0.15],\n",
    "    \"ybound\": [-15.0, 15.0, 0.15],\n",
    "    \"zbound\": [-10.0, 10.0, 20.0],\n",
    "    \"dbound\": [1.0, 70.0, 1.0],\n",
    "}\n",
    "\n",
    "----------\n",
    "det_grid_conf = {\n",
    "    \"xbound\": [-51.2, 51.2, 0.8],\n",
    "    \"ybound\": [-51.2, 51.2, 0.8],\n",
    "    \"zbound\": [-10.0, 10.0, 20.0],\n",
    "    \"dbound\": [1.0, 60.0, 1.0],\n",
    "}\n",
    "\n",
    "motion_grid_conf = {\n",
    "    \"xbound\": [-50.0, 50.0, 0.5],\n",
    "    \"ybound\": [-50.0, 50.0, 0.5],\n",
    "    \"zbound\": [-10.0, 10.0, 20.0],\n",
    "    \"dbound\": [1.0, 60.0, 1.0],\n",
    "}\n",
    "\n",
    "map_grid_conf = {\n",
    "    \"xbound\": [-30.0, 30.0, 0.15],\n",
    "    \"ybound\": [-15.0, 15.0, 0.15],\n",
    "    \"zbound\": [-10.0, 10.0, 20.0],\n",
    "    \"dbound\": [1.0, 60.0, 1.0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_bounds, y_bounds,z_bounds=cfg[\"det_grid_conf\"][\"xbound\"],cfg[\"det_grid_conf\"][\"ybound\"],cfg[\"det_grid_conf\"][\"zbound\"]\n",
    "\n",
    "bev_dimension = torch.tensor(\n",
    "        [(row[1] - row[0]) / row[2] for row in [x_bounds, y_bounds, z_bounds]],\n",
    "        dtype=torch.long,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155.0\n",
      "90.5\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "for row in [x_bounds, y_bounds, z_bounds]:\n",
    "    print((row[1] - row[0]) / row[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# det_grid_conf = {\n",
    "#     \"xbound\": [-62.0, 62.0, 0.37],#37\n",
    "#     \"ybound\": [-36.2, 36.2, 0.375],#37,5\n",
    "#     \"zbound\": [-10.0, 10.0, 20.0],\n",
    "#     \"dbound\": [1.0, 70.0, 1.0],\n",
    "# }\n",
    "\n",
    "# motion_grid_conf = {\n",
    "#     \"xbound\": [-60.0, 60.0, 0.25],\n",
    "#     \"ybound\": [-36.0, 36.0, 0.25],\n",
    "#     \"zbound\": [-10.0, 10.0, 20.0],\n",
    "#     \"dbound\": [1.0, 70.0, 1.0],\n",
    "# }\n",
    "\n",
    "# map_grid_conf = {\n",
    "#     \"xbound\": [-30.0, 30.0, 0.15],\n",
    "#     \"ybound\": [-15.0, 15.0, 0.15],\n",
    "#     \"zbound\": [-10.0, 10.0, 20.0],\n",
    "#     \"dbound\": [1.0, 70.0, 1.0],\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects.mmdet3d_plugin\n"
     ]
    }
   ],
   "source": [
    "det_grid_conf = {\n",
    "    \"xbound\": [-62.0, 62.0, 0.37],#37\n",
    "    \"ybound\": [-36.2, 36.2, 0.375],#37,5\n",
    "    \"zbound\": [-10.0, 10.0, 20.0],\n",
    "    \"dbound\": [1.0, 70.0, 1.0],\n",
    "}\n",
    "\n",
    "motion_grid_conf = {\n",
    "    \"xbound\": [-60.0, 60.0, 0.25],\n",
    "    \"ybound\": [-36.0, 36.0, 0.25],\n",
    "    \"zbound\": [-10.0, 10.0, 20.0],\n",
    "    \"dbound\": [1.0, 70.0, 1.0],\n",
    "}\n",
    "\n",
    "map_grid_conf = {\n",
    "    \"xbound\": [-30.0, 30.0, 0.15],\n",
    "    \"ybound\": [-15.0, 15.0, 0.15],\n",
    "    \"zbound\": [-10.0, 10.0, 20.0],\n",
    "    \"dbound\": [1.0, 70.0, 1.0],\n",
    "}\n",
    "\n",
    "point_cloud_range_base = [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]\n",
    "point_cloud_range_extended_fustrum = [-62.0, -62.0, -5.0, 62.0, 62.0, 3.0]\n",
    "\n",
    "\n",
    "cfg = import_modules_load_config(cfg_file=\"/home/niklas/ETM_BEV/BEVerse/projects/configs/beverse_tiny_exp.py\")\n",
    "cfg = update_cfg(\n",
    "    cfg,det_grid_conf=det_grid_conf,grid_conf=det_grid_conf, map_grid_conf=map_grid_conf, motion_grid_conf=motion_grid_conf, point_cloud_range=point_cloud_range_base\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upsample(scale_factor=4.0, mode=bilinear)\n",
      "Upsample(scale_factor=4.0, mode=bilinear)\n",
      "Upsample(scale_factor=4.0, mode=bilinear)\n",
      "CenterHeadv1\n",
      "CenterPointBBoxCoder\n",
      "MapHead\n",
      "BaseMotionHead\n",
      "IterativeFlow\n",
      "Temp3d:  (128, 128)\n",
      "Temp3d:  {'xbound': [-62.0, 62.0, 0.37], 'ybound': [-36.2, 36.2, 0.375], 'zbound': [-10.0, 10.0, 20.0], 'dbound': [1.0, 70.0, 1.0]}\n",
      "pool_sizes [(2, 96, 167)]\n",
      "temp block, in_channels  70 reduction_channels  23 pool_sizes  [(2, 96, 167)] \n",
      "Temproal3DConvModel: block_in_channels 70, block_out_channels 64, pool_sizes [(2, 96, 167)]\n",
      "pool_sizes [(2, 96, 167)]\n",
      "temp block, in_channels  64 reduction_channels  21 pool_sizes  [(2, 96, 167)] \n",
      "Temproal3DConvModel: block_in_channels 64, block_out_channels 64, pool_sizes [(2, 96, 167)]\n"
     ]
    }
   ],
   "source": [
    "dataset = build_dataset(cfg.data.test)\n",
    "data_loader = build_dataloader(\n",
    "    dataset,\n",
    "    samples_per_gpu=1,\n",
    "    workers_per_gpu=cfg.data.workers_per_gpu,\n",
    "    dist=False,\n",
    "    shuffle=False)\n",
    "\n",
    "sample = next(iter(data_loader))\n",
    "\n",
    "model = build_model(cfg.model, test_cfg=cfg.get(\"test_cfg\"))\n",
    "wrap_fp16_model(model)\n",
    "model.cuda()\n",
    "model = MMDataParallel(model, device_ids=[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update\n",
      "pyramid pooling:  torch.Size([1, 70, 3, 96, 167])\n",
      "x_pool:  torch.Size([1, 23, 3, 1, 1])\n",
      "c:  23\n",
      "x_pool2:  torch.Size([3, 23, 96, 167])\n",
      "x_pool3:  torch.Size([1, 23, 3, 96, 167])\n",
      "update\n",
      "pyramid pooling:  torch.Size([1, 64, 3, 96, 167])\n",
      "x_pool:  torch.Size([1, 21, 3, 1, 1])\n",
      "c:  21\n",
      "x_pool2:  torch.Size([3, 21, 96, 167])\n",
      "x_pool3:  torch.Size([1, 21, 3, 96, 167])\n",
      "feats-1: torch.Size([1, 512, 25, 50]), feats-3: torch.Size([1, 128, 100, 200])\n",
      "Up: x1 torch.Size([1, 512, 100, 200]), x2 torch.Size([1, 128, 100, 200])\n",
      "feats-1: torch.Size([1, 512, 12, 21]), feats-3: torch.Size([1, 128, 48, 84])\n",
      "Up: x1 torch.Size([1, 512, 48, 84]), x2 torch.Size([1, 128, 48, 84])\n",
      "feats-1: torch.Size([1, 512, 36, 60]), feats-3: torch.Size([1, 128, 144, 240])\n",
      "Up: x1 torch.Size([1, 512, 144, 240]), x2 torch.Size([1, 128, 144, 240])\n"
     ]
    }
   ],
   "source": [
    "logger.debug(cfg[\"det_grid_conf\"])\n",
    "logger.debug(cfg[\"motion_grid_conf\"])\n",
    "logger.debug(cfg[\"map_grid_conf\"])\n",
    "\n",
    "\n",
    "bev_resolution, bev_start_position, bev_dimension=calculate_birds_eye_view_parameters(cfg[\"det_grid_conf\"][\"xbound\"],cfg[\"det_grid_conf\"][\"ybound\"],cfg[\"det_grid_conf\"][\"zbound\"])\n",
    "logger.debug(f\"bev_resolution: {bev_resolution}\")\n",
    "logger.debug(f\"bev_start_position: {bev_start_position}\")\n",
    "logger.debug(f\"bev_dimension: {bev_dimension}\")\n",
    "\n",
    "\n",
    "motion_distribution_targets = {\n",
    "    # for motion prediction\n",
    "    'motion_segmentation': sample['motion_segmentation'][0],\n",
    "    'motion_instance': sample['motion_instance'][0],\n",
    "    'instance_centerness': sample['instance_centerness'][0],\n",
    "    'instance_offset': sample['instance_offset'][0],\n",
    "    'instance_flow': sample['instance_flow'][0],\n",
    "    'future_egomotion': sample['future_egomotions'][0],\n",
    "}\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    result = model(\n",
    "        return_loss=False,\n",
    "        rescale=True,\n",
    "        img_metas=sample['img_metas'],\n",
    "        img_inputs=sample['img_inputs'],\n",
    "        future_egomotions=sample['future_egomotions'],\n",
    "        motion_targets=motion_distribution_targets,\n",
    "        img_is_valid=sample['img_is_valid'][0],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2716849234.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [12], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    image (T: 3 H: 65 W: 178) smaller than kernel size (kT: 2 kH: 128 kW: 128)\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "image (T: 3 H: 65 W: 178) smaller than kernel size (kT: 2 kH: 128 kW: 128)\n",
    "\n",
    "\n",
    "update\n",
    "pyramid pooling:  torch.Size([1, 70, 3, 128, 128])\n",
    "x_pool:  torch.Size([1, 23, 3, 1, 1])\n",
    "c:  23\n",
    "x_pool2:  torch.Size([3, 23, 128, 128])\n",
    "x_pool3:  torch.Size([1, 23, 3, 128, 128])\n",
    "update\n",
    "pyramid pooling:  torch.Size([1, 64, 3, 128, 128])\n",
    "x_pool:  torch.Size([1, 21, 3, 1, 1])\n",
    "c:  21\n",
    "x_pool2:  torch.Size([3, 21, 128, 128])\n",
    "x_pool3:  torch.Size([1, 21, 3, 128, 128])\n",
    "feats-1: torch.Size([1, 512, 25, 50]), feats-3: torch.Size([1, 128, 100, 200])\n",
    "Up: x1 torch.Size([1, 512, 100, 200]), x2 torch.Size([1, 128, 100, 200])\n",
    "feats-1: torch.Size([1, 512, 16, 16]), feats-3: torch.Size([1, 128, 64, 64])\n",
    "Up: x1 torch.Size([1, 512, 64, 64]), x2 torch.Size([1, 128, 64, 64])\n",
    "feats-1: torch.Size([1, 512, 25, 25]), feats-3: torch.Size([1, 128, 100, 100])\n",
    "Up: x1 torch.Size([1, 512, 100, 100]), x2 torch.Size([1, 128, 100, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"IMAGE\": {\n",
    "    \"ORIGINAL_HEIGHT\": 900,\n",
    "    \"ORIGINAL_WIDTH\": 1600,\n",
    "    \"FINAL_DIM\": (512, 1408),\n",
    "    \"RESIZE_SCALE\": 0.25,\n",
    "    \"TOP_CROP\": 0,\n",
    "}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['IMAGE'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero padding left and right parts of the image.\n",
      "Zero padding bottom part of the image.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'scale_width': 0.25,\n",
       " 'scale_height': 0.25,\n",
       " 'resize_dims': (400, 225),\n",
       " 'crop': (0, 0, 1408, 512)}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_resizing_and_cropping_parameters(config):\n",
    "    original_height, original_width = config[\"IMAGE\"][\"ORIGINAL_HEIGHT\"], config[\"IMAGE\"][\"ORIGINAL_WIDTH\"]\n",
    "    final_height, final_width = config[\"IMAGE\"][\"FINAL_DIM\"]\n",
    "\n",
    "    resize_scale = config[\"IMAGE\"][\"RESIZE_SCALE\"]\n",
    "    resize_dims = (int(original_width * resize_scale), int(original_height * resize_scale))\n",
    "    resized_width, resized_height = resize_dims\n",
    "\n",
    "    crop_h = config[\"IMAGE\"][\"TOP_CROP\"]\n",
    "    crop_w = int(max(0, (resized_width - final_width) / 2))\n",
    "    # Left, top, right, bottom crops.\n",
    "    crop = (crop_w, crop_h, crop_w + final_width, crop_h + final_height)\n",
    "\n",
    "    if resized_width != final_width:\n",
    "        print('Zero padding left and right parts of the image.')\n",
    "    if crop_h + final_height != resized_height:\n",
    "        print('Zero padding bottom part of the image.')\n",
    "\n",
    "    return {'scale_width': resize_scale,\n",
    "            'scale_height': resize_scale,\n",
    "            'resize_dims': resize_dims,\n",
    "            'crop': crop,\n",
    "            }\n",
    "get_resizing_and_cropping_parameters(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t_BEV': 1663055717.0110373,\n",
       " 't_temporal': 1663055717.5791538,\n",
       " 't0': 1663055712.6830127,\n",
       " 't_end': 1663055746.884341}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"time_stats\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x, rots, trans, intrins, post_rots, post_trans = input\n",
    "\n",
    "'CAM_FRONT_RIGHT': {\n",
    "    'data_path': './data/nuscenes/samples/CAM_FRONT_RIGHT/n015-2018-10-08-15-36-50+0800__CAM_FRONT_RIGHT__1538984233520339.jpg', \n",
    "    'type': 'CAM_FRONT_RIGHT', \n",
    "    'sample_data_token': 'd6f89460954c43d39ed7c9ac91ab03d0', \n",
    "    'sensor2ego_translation': [1.5508477543, -0.493404796419, 1.49574800619], \n",
    "    'sensor2ego_rotation': [0.2060347966337182, -0.2026940577919598, 0.6824507824531167, -0.6713610884174485], 'ego2global_translation': [715.6566537856895, 1810.1516804263824, 0.0], \n",
    "    'ego2global_rotation': [0.8004835927391405, 0.00541081062084495, -0.0028680900818508943, -0.5993233809414961], 'timestamp': 1538984233520339, \n",
    "    'sensor2lidar_rotation': array([[ 0.55971752, -0.01057234,  0.82861603],\n",
    "       [-0.82828535,  0.02385392,  0.55979851],\n",
    "       [-0.02568412, -0.99965955,  0.00459454]]), \n",
    "    'sensor2lidar_translation': array([ 0.48135698,  0.51094805, -0.32997566]), \n",
    "    'cam_intrinsic': array([[1.26084744e+03, 0.00000000e+00, 8.07968245e+02],\n",
    "       [0.00000000e+00, 1.26084744e+03, 4.95334427e+02],\n",
    "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future Egomotions:  torch.Size([1, 7, 6])\n",
      "img_is_valid:  torch.Size([1, 7])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "num_warmups = 100\n",
    "num_repeats = 1000\n",
    "# Change to C x 1 x 3 x 1600 x 900\n",
    "# 704×256 Tiny\n",
    "# 1408×512\n",
    "# B, S, N, C, imH, imW = imgs.shape\n",
    "img_inputs =  torch.rand(1,7,6,3,704,256).cuda()\n",
    "# (batch, seq, num_cam)\n",
    "future_egomotions = torch.zeros((batch_size, 7, 6)).type_as(img_inputs).cuda()\n",
    "img_is_valid = torch.ones((batch_size, 7)).type_as(img_inputs) > 0\n",
    "img_is_valid.cuda()\n",
    "print(\"Future Egomotions: \", future_egomotions.shape)\n",
    "print(\"img_is_valid: \", img_is_valid.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #single frame \n",
    "# future_egomotions = future_egomotions[:, :1]\n",
    "# img_is_valid = img_is_valid[:, :1]\n",
    "# print(\"Future Egomotions: \", future_egomotions.shape)\n",
    "# print(\"img_is_valid: \", img_is_valid.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n",
      "torch.Size([3])\n",
      "image meta: [{'box_type_3d': <class 'mmdet3d.core.bbox.structures.lidar_box3d.LiDARInstance3DBoxes'>, 'lidar2ego_rots': tensor([[-5.4280e-04,  9.9893e-01,  4.6229e-02],\n",
      "        [-1.0000e+00, -4.0569e-04, -2.9750e-03],\n",
      "        [-2.9531e-03, -4.6231e-02,  9.9893e-01]]), 'lidar2ego_trans': tensor([0.9858, 0.0000, 1.8402])}]\n"
     ]
    }
   ],
   "source": [
    "from mmdet3d.core.bbox.structures.box_3d_mode import LiDARInstance3DBoxes\n",
    "\n",
    "dummy_lidar2ego_rots = (\n",
    "    torch.tensor(\n",
    "        [\n",
    "            [-5.4280e-04, 9.9893e-01, 4.6229e-02],\n",
    "            [-1.0000e00, -4.0569e-04, -2.9750e-03],\n",
    "            [-2.9531e-03, -4.6231e-02, 9.9893e-01],\n",
    "        ]\n",
    "    )\n",
    "    .type_as(img_inputs)\n",
    "    .cpu()\n",
    ")\n",
    "dummy_lidar2ego_trans = (\n",
    "    torch.tensor([0.9858, 0.0000, 1.8402]).type_as(img_inputs).cpu()\n",
    ")\n",
    "print(dummy_lidar2ego_rots.shape)\n",
    "print(dummy_lidar2ego_trans.shape)\n",
    "img_metas = [\n",
    "    dict(\n",
    "        box_type_3d=LiDARInstance3DBoxes,\n",
    "        lidar2ego_rots=dummy_lidar2ego_rots,\n",
    "        lidar2ego_trans=dummy_lidar2ego_trans,\n",
    "    )\n",
    "]\n",
    "print(\"image meta:\", img_metas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'projects.mmdet3d_plugin.models.detectors.beverse.BEVerse'>\n"
     ]
    }
   ],
   "source": [
    "model = build_model(cfg.model, test_cfg=cfg.get(\"test_cfg\"))\n",
    "print(type(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch?:  torch.Size([1, 7, 6, 3, 704, 256])\n",
      "B 1, S 7, 6, C 3, imH 704, imW 256\n",
      "imgs  torch.Size([42, 3, 704, 256])\n",
      "after backbone:  2\n",
      "shape in list:  [torch.Size([42, 384, 44, 16]), torch.Size([42, 768, 22, 8])]\n",
      "after backbone with_img_neck:  torch.Size([42, 512, 44, 16])\n",
      "after transformation:  torch.Size([1, 7, 6, 512, 44, 16])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"Tensor\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m build_model(cfg\u001b[38;5;241m.\u001b[39mmodel, test_cfg\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_cfg\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      2\u001b[0m wrap_fp16_model(model)\n\u001b[0;32m----> 3\u001b[0m img_feats,time_dict  \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_img_feat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimg_metas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_metas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfuture_egomotion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuture_egomotions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimg_is_valid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_is_valid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(img_feats\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(time_dict)\n",
      "File \u001b[0;32m~/ETM_BEV/BEVerse/projects/mmdet3d_plugin/models/detectors/beverse.py:105\u001b[0m, in \u001b[0;36mBEVerse.extract_img_feat\u001b[0;34m(self, img, img_metas, future_egomotion, aug_transform, img_is_valid, count_time)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mafter transformation: \u001b[39m\u001b[39m\"\u001b[39m, x\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    104\u001b[0m \u001b[39m# lifting with LSS\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer([x] \u001b[39m+\u001b[39;49m img[\u001b[39m1\u001b[39;49m:])\n\u001b[1;32m    107\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39msynchronize()\n\u001b[1;32m    108\u001b[0m t_BEV \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"Tensor\") to list"
     ]
    }
   ],
   "source": [
    "model = build_model(cfg.model, test_cfg=cfg.get(\"test_cfg\")).cuda()\n",
    "wrap_fp16_model(model)\n",
    "img_feats,time_dict  = model.extract_img_feat(\n",
    "            img=img_inputs,\n",
    "            img_metas=img_metas,\n",
    "            future_egomotion=future_egomotions,\n",
    "            img_is_valid=img_is_valid,\n",
    "        )\n",
    "print(img_feats.shape)\n",
    "print(time_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2000,  0.2000, 20.0000]) tensor([-51.1000, -51.1000,   0.0000]) tensor([512, 512,   1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_362/3561629359.py:21: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  bev_dimension = torch.tensor(\n"
     ]
    }
   ],
   "source": [
    "# 'xbound': [-51.2, 51.2, 0.2], 'ybound': [-51.2, 51.2, 0.2], 'zbound': [-10.0, 10.0, 20.0], 'dbound': [1.0, 60.0, 0.5]}\n",
    "\n",
    "\n",
    "\n",
    "x_bounds, y_bounds, z_bounds = [-51.2, 51.2, 0.2], [-51.2, 51.2, 0.2], [-10.0, 10.0, 20.0]\n",
    "\n",
    "bev_resolution, bev_start_position, bev_dimension = calculate_birds_eye_view_parameters(x_bounds, y_bounds, z_bounds)\n",
    "\n",
    "print(bev_resolution, bev_start_position, bev_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_bounds, y_bounds, z_bounds = [-26.2, 26.2, 0.8], [-71.2, 71.2, 0.8], [-10.0, 10.0, 20.0]\n",
    "tensor([ 0.8000,  0.8000, 20.0000]) tensor([-25.8000, -70.8000,   0.0000]) tensor([ 65, 178,   1])\n",
    "\n",
    "\n",
    "x_bounds, y_bounds, z_bounds = [-26.2, 26.2, 0.4], [-71.2, 71.2, 0.4], [-10.0, 10.0, 20.0]\n",
    "tensor([ 0.4000,  0.4000, 20.0000]) tensor([-26., -71.,   0.]) tensor([131, 356,   1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0, 7, 6, 3, 704, 256])\n",
      "torch.Size([1, 7, 6, 3, 704, 256])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"Tensor\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m img_inputs \u001b[38;5;241m=\u001b[39m  torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m7\u001b[39m,\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m704\u001b[39m,\u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(img_inputs[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 6\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mtrans_output\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimg_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      7\u001b[0m test\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"Tensor\") to list"
     ]
    }
   ],
   "source": [
    "img_inputs =  torch.rand(1,7,6,3,704,256)\n",
    "print(img_inputs[1:].shape)\n",
    "trans_output = torch.rand(1, 7, 6, 512, 44, 16)\n",
    "img_inputs =  torch.rand(2,7,6,3,704,256)\n",
    "print(img_inputs[1:].shape)\n",
    "test = [trans_output] + img_inputs[1:]\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency Measurement Using CPU Timer...\n",
      "|Synchronization: True | Continuous Measurement: True | Latency: N/A     ms| \n",
      "Latency Measurement Using CUDA Timer...\n",
      "|Synchronization: True | Continuous Measurement: True | Latency: N/A     ms| \n",
      "|Synchronization: False| Continuous Measurement: True | Latency: N/A     ms| \n",
      "|Synchronization: True | Continuous Measurement: False| Latency: N/A     ms| \n",
      "|Synchronization: False| Continuous Measurement: False| Latency: N/A     ms| \n",
      "Latency Measurement Using PyTorch Benchmark...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dummy_input.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 95\u001b[0m\n\u001b[1;32m     85\u001b[0m num_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     86\u001b[0m timer \u001b[38;5;241m=\u001b[39m benchmark\u001b[38;5;241m.\u001b[39mTimer(\n\u001b[1;32m     87\u001b[0m     stmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_inference(model, input_tensor)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     88\u001b[0m     setup\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom __main__ import run_inference\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m     sub_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.utils.benchmark.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     93\u001b[0m )\n\u001b[0;32m---> 95\u001b[0m profile_result \u001b[38;5;241m=\u001b[39m \u001b[43mtimer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_repeats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# https://pytorch.org/docs/stable/_modules/torch/utils/benchmark/utils/common.html#Measurement\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLatency: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprofile_result\u001b[38;5;241m.\u001b[39mmean \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/torch/utils/benchmark/utils/timer.py:261\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39m\"\"\"Mirrors the semantics of timeit.Timer.timeit().\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \n\u001b[1;32m    256\u001b[0m \u001b[39mExecute the main statement (`stmt`) `number` times.\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[39mhttps://docs.python.org/3/library/timeit.html#timeit.Timer.timeit\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[39mwith\u001b[39;00m common\u001b[39m.\u001b[39mset_torch_threads(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_spec\u001b[39m.\u001b[39mnum_threads):\n\u001b[1;32m    260\u001b[0m     \u001b[39m# Warmup\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_timer\u001b[39m.\u001b[39;49mtimeit(number\u001b[39m=\u001b[39;49m\u001b[39mmax\u001b[39;49m(\u001b[39mint\u001b[39;49m(number \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m \u001b[39m100\u001b[39;49m), \u001b[39m2\u001b[39;49m))\n\u001b[1;32m    263\u001b[0m     \u001b[39mreturn\u001b[39;00m common\u001b[39m.\u001b[39mMeasurement(\n\u001b[1;32m    264\u001b[0m         number_per_run\u001b[39m=\u001b[39mnumber,\n\u001b[1;32m    265\u001b[0m         raw_times\u001b[39m=\u001b[39m[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timer\u001b[39m.\u001b[39mtimeit(number\u001b[39m=\u001b[39mnumber)],\n\u001b[1;32m    266\u001b[0m         task_spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_spec\n\u001b[1;32m    267\u001b[0m     )\n",
      "File \u001b[0;32m/usr/lib/python3.8/timeit.py:177\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    175\u001b[0m gc\u001b[39m.\u001b[39mdisable()\n\u001b[1;32m    176\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     timing \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner(it, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimer)\n\u001b[1;32m    178\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     \u001b[39mif\u001b[39;00m gcold:\n",
      "File \u001b[0;32m<timeit-src>:6\u001b[0m, in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn [15], line 89\u001b[0m, in \u001b[0;36mrun_inference\u001b[0;34m(model, input_tensor)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_inference\u001b[39m(model: nn\u001b[38;5;241m.\u001b[39mModule, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# from model.forward because BEVerse differentiates between different input types - img lidar etc \u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_dummy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ETM_BEV/BEVerse/projects/mmdet3d_plugin/models/detectors/beverse.py:209\u001b[0m, in \u001b[0;36mforward_dummy\u001b[0;34m(self, img_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39m\"\"\"Forward training function.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \n\u001b[1;32m    169\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39m    dict: Losses of different branches.\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    193\u001b[0m img_feats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextract_img_feat(\n\u001b[1;32m    194\u001b[0m     img\u001b[39m=\u001b[39mimg_inputs,\n\u001b[1;32m    195\u001b[0m     img_metas\u001b[39m=\u001b[39mimg_metas,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    198\u001b[0m     img_is_valid\u001b[39m=\u001b[39mimg_is_valid,\n\u001b[1;32m    199\u001b[0m )\n\u001b[1;32m    201\u001b[0m mtl_targets \u001b[39m=\u001b[39m {\n\u001b[1;32m    202\u001b[0m     \u001b[39m# for detection\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mgt_bboxes_3d\u001b[39m\u001b[39m\"\u001b[39m: gt_bboxes_3d,\n\u001b[1;32m    204\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mgt_labels_3d\u001b[39m\u001b[39m\"\u001b[39m: gt_labels_3d,\n\u001b[1;32m    205\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mgt_bboxes_ignore\u001b[39m\u001b[39m\"\u001b[39m: gt_bboxes_ignore,\n\u001b[1;32m    206\u001b[0m     \u001b[39m# for map segmentation\u001b[39;00m\n\u001b[1;32m    207\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39msemantic_seg\u001b[39m\u001b[39m\"\u001b[39m: semantic_indices,\n\u001b[1;32m    208\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39msemantic_map\u001b[39m\u001b[39m\"\u001b[39m: semantic_map,\n\u001b[0;32m--> 209\u001b[0m     \u001b[39m# for motion prediction\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmotion_segmentation\u001b[39m\u001b[39m\"\u001b[39m: motion_segmentation,\n\u001b[1;32m    211\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmotion_instance\u001b[39m\u001b[39m\"\u001b[39m: motion_instance,\n\u001b[1;32m    212\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minstance_centerness\u001b[39m\u001b[39m\"\u001b[39m: instance_centerness,\n\u001b[1;32m    213\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minstance_offset\u001b[39m\u001b[39m\"\u001b[39m: instance_offset,\n\u001b[1;32m    214\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minstance_flow\u001b[39m\u001b[39m\"\u001b[39m: instance_flow,\n\u001b[1;32m    215\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mfuture_egomotion\u001b[39m\u001b[39m\"\u001b[39m: future_egomotions,\n\u001b[1;32m    216\u001b[0m     \u001b[39m# for bev_augmentation\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39maug_transform\u001b[39m\u001b[39m\"\u001b[39m: aug_transform,\n\u001b[1;32m    218\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mimg_is_valid\u001b[39m\u001b[39m\"\u001b[39m: img_is_valid,\n\u001b[1;32m    219\u001b[0m }\n\u001b[1;32m    221\u001b[0m loss_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_pts_train(img_feats, img_metas, mtl_targets)\n\u001b[1;32m    223\u001b[0m \u001b[39mreturn\u001b[39;00m loss_dict\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/torch/serialization.py:594\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    592\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 594\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    595\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    596\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    598\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    599\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    231\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dummy_input.pt'"
     ]
    }
   ],
   "source": [
    "\n",
    "num_warmups = 100\n",
    "num_repeats = 1000\n",
    "# Change to C x 1 x 3 x 1600 x 900\n",
    "# 704×256 Tiny\n",
    "# 1408×512\n",
    "# B, T, N, C, imH, imW = imgs.shape\n",
    "input_shape = (1,3,6,3,1600,900)\n",
    "\n",
    "future_egomotions = torch.zeros((batch_size, 7, 6)).type_as(img_inputs)\n",
    "img_is_valid = torch.ones((batch_size, 7)).type_as(img_inputs) > 0\n",
    "print(\"Future Egomotions: \", future_egomotions.shape)\n",
    "print(\"img_is_valid: \", img_is_valid.shape)\n",
    "\n",
    "\n",
    "# imgs = imgs.view(B * S * N, C, imH, imW)\n",
    "\n",
    "model = build_model(cfg.model, test_cfg=cfg.get(\"test_cfg\"))\n",
    "wrap_fp16_model(model)\n",
    "# load_checkpoint(\n",
    "#     model,\n",
    "#     r\"/home/niklas/ETM_BEV/BEVerse/checkpoints/beverse_tiny.pth\",\n",
    "#     map_location=\"cpu\",\n",
    "# )\n",
    "# model = fuse_module(model)\n",
    "model.cuda(device)\n",
    "model.eval()\n",
    "# model = nn.Conv2d(in_channels=input_shape[1], out_channels=256, kernel_size=(5, 5))\n",
    "\n",
    "# Input tensor\n",
    "input_tensor = torch.rand(input_shape, device=device)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "print(\"Latency Measurement Using CPU Timer...\")\n",
    "for continuous_measure in [True]:\n",
    "    for synchronize in [True]:\n",
    "        try:\n",
    "            latency_ms = measure_time_host(\n",
    "                model=model,\n",
    "                input_tensor=input_tensor,\n",
    "                num_repeats=num_repeats,\n",
    "                num_warmups=num_warmups,\n",
    "                synchronize=synchronize,\n",
    "                continuous_measure=continuous_measure,\n",
    "            )\n",
    "            print(\n",
    "                f\"|\"\n",
    "                f\"Synchronization: {synchronize!s:5}| \"\n",
    "                f\"Continuous Measurement: {continuous_measure!s:5}| \"\n",
    "                f\"Latency: {latency_ms:.5f} ms| \"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"|\"\n",
    "                f\"Synchronization: {synchronize!s:5}| \"\n",
    "                f\"Continuous Measurement: {continuous_measure!s:5}| \"\n",
    "                f\"Latency: N/A     ms| \"\n",
    "            )\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "print(\"Latency Measurement Using CUDA Timer...\")\n",
    "for continuous_measure in [True, False]:\n",
    "    for synchronize in [True, False]:\n",
    "        try:\n",
    "            latency_ms = measure_time_device(\n",
    "                model=model,\n",
    "                input_tensor=input_tensor,\n",
    "                num_repeats=num_repeats,\n",
    "                num_warmups=num_warmups,\n",
    "                synchronize=synchronize,\n",
    "                continuous_measure=continuous_measure,\n",
    "            )\n",
    "            print(\n",
    "                f\"|\"\n",
    "                f\"Synchronization: {synchronize!s:5}| \"\n",
    "                f\"Continuous Measurement: {continuous_measure!s:5}| \"\n",
    "                f\"Latency: {latency_ms:.5f} ms| \"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"|\"\n",
    "                f\"Synchronization: {synchronize!s:5}| \"\n",
    "                f\"Continuous Measurement: {continuous_measure!s:5}| \"\n",
    "                f\"Latency: N/A     ms| \"\n",
    "            )\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "print(\"Latency Measurement Using PyTorch Benchmark...\")\n",
    "num_threads = 1\n",
    "timer = benchmark.Timer(\n",
    "    stmt=\"run_inference(model, input_tensor)\",\n",
    "    setup=\"from __main__ import run_inference\",\n",
    "    globals={\"model\": model, \"input_tensor\": input_tensor},\n",
    "    num_threads=num_threads,\n",
    "    label=\"Latency Measurement\",\n",
    "    sub_label=\"torch.utils.benchmark.\",\n",
    ")\n",
    "\n",
    "profile_result = timer.timeit(num_repeats)\n",
    "# https://pytorch.org/docs/stable/_modules/torch/utils/benchmark/utils/common.html#Measurement\n",
    "print(f\"Latency: {profile_result.mean * 1000:.5f} ms\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49a6cb26e152f15aca94d1d3fa9630fb57fb8fd83a336982cd2ebf9e9635e69c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
