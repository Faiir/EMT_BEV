{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet import __version__ as mmdet_version\n",
    "from mmdet3d import __version__ as mmdet3d_version\n",
    "from mmseg import __version__ as mmseg_version\n",
    "import os\n",
    "from custome_logger import setup_custom_logger\n",
    "logger = setup_custom_logger()\n",
    "logger.debug(\"test\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.benchmark as benchmark\n",
    "from timeit import default_timer as timer\n",
    "from mmcv import Config\n",
    "from mmcv.runner import wrap_fp16_model\n",
    "from mmdet3d.models import build_model\n",
    "from mmdet3d.datasets import build_dataset\n",
    "from mmcv.parallel import MMDataParallel\n",
    "from mmdet.datasets import (build_dataloader, build_dataset,\n",
    "                            replace_ImageToTensor)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "def update_cfg(\n",
    "    cfg,\n",
    "    n_future=3,\n",
    "    receptive_field=3,\n",
    "    resize_lim=(0.38, 0.55),\n",
    "    final_dim=(256, 704),\n",
    "    grid_conf={\n",
    "        \"xbound\": [-51.2, 51.2, 0.8],\n",
    "        \"ybound\": [-51.2, 51.2, 0.8],\n",
    "        \"zbound\": [-10.0, 10.0, 20.0],\n",
    "        \"dbound\": [1.0, 60.0, 1.0],\n",
    "    },\n",
    "    det_grid_conf={\n",
    "        \"xbound\": [-51.2, 51.2, 0.8],\n",
    "        \"ybound\": [-51.2, 51.2, 0.8],\n",
    "        \"zbound\": [-10.0, 10.0, 20.0],\n",
    "        \"dbound\": [1.0, 60.0, 1.0],\n",
    "    },\n",
    "    map_grid_conf={\n",
    "        \"xbound\": [-30.0, 30.0, 0.15],\n",
    "        \"ybound\": [-15.0, 15.0, 0.15],\n",
    "        \"zbound\": [-10.0, 10.0, 20.0],\n",
    "        \"dbound\": [1.0, 60.0, 1.0],\n",
    "    },\n",
    "    motion_grid_conf={\n",
    "        \"xbound\": [-50.0, 50.0, 0.5],\n",
    "        \"ybound\": [-50.0, 50.0, 0.5],\n",
    "        \"zbound\": [-10.0, 10.0, 20.0],\n",
    "        \"dbound\": [1.0, 60.0, 1.0],\n",
    "    },\n",
    "    t_input_shape=(128, 128),\n",
    "    point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],\n",
    "):\n",
    "    \n",
    "    cfg[\"det_grid_conf\"] = det_grid_conf\n",
    "    cfg[\"map_grid_conf\"] = map_grid_conf\n",
    "    cfg[\"motion_grid_conf\"] = motion_grid_conf\n",
    "    cfg[\"grid_conf\"] = det_grid_conf\n",
    "\n",
    "    cfg[\"model\"][\"temporal_model\"][\"input_shape\"] = t_input_shape\n",
    "\n",
    "    cfg[\"data\"][\"val\"][\"pipeline\"][0][\"data_aug_conf\"][\"resize_lim\"] = resize_lim\n",
    "    cfg[\"data\"][\"train\"][\"dataset\"][\"pipeline\"][0][\"data_aug_conf\"][\n",
    "        \"resize_lim\"\n",
    "    ] = resize_lim\n",
    "    cfg[\"data\"][\"val\"][\"pipeline\"][0][\"data_aug_conf\"][\"final_dim\"] = final_dim\n",
    "    cfg[\"data\"][\"train\"][\"dataset\"][\"pipeline\"][0][\"data_aug_conf\"][\n",
    "        \"final_dim\"\n",
    "    ] = final_dim\n",
    "\n",
    "    cfg[\"data\"][\"test\"][\"pipeline\"][0][\"data_aug_conf\"][\"resize_lim\"] = resize_lim\n",
    "    cfg[\"data\"][\"test\"][\"pipeline\"][0][\"data_aug_conf\"][\"final_dim\"] = final_dim\n",
    "\n",
    "    cfg[\"model\"][\"pts_bbox_head\"][\"cfg_motion\"][\n",
    "        \"grid_conf\"\n",
    "    ] = motion_grid_conf  # motion_grid\n",
    "    cfg[\"model\"][\"temporal_model\"][\"grid_conf\"] = grid_conf\n",
    "    cfg[\"model\"][\"transformer\"][\"grid_conf\"] = grid_conf\n",
    "    cfg[\"model\"][\"pts_bbox_head\"][\"grid_conf\"] = grid_conf\n",
    "    cfg[\"data\"][\"train\"][\"dataset\"][\"grid_conf\"] = motion_grid_conf\n",
    "    cfg[\"data\"][\"val\"][\"pipeline\"][3][\"grid_conf\"] = motion_grid_conf\n",
    "    cfg[\"data\"][\"test\"][\"pipeline\"][3][\"grid_conf\"] = motion_grid_conf\n",
    "    cfg[\"data\"][\"val\"][\"grid_conf\"] = grid_conf\n",
    "    cfg[\"data\"][\"test\"][\"grid_conf\"] = grid_conf\n",
    "\n",
    "    cfg[\"model\"][\"pts_bbox_head\"][\"det_grid_conf\"] = det_grid_conf\n",
    "\n",
    "    cfg[\"model\"][\"pts_bbox_head\"][\"map_grid_conf\"] = map_grid_conf\n",
    "    cfg[\"data\"][\"train\"][\"dataset\"][\"map_grid_conf\"] = map_grid_conf\n",
    "    cfg[\"data\"][\"test\"][\"pipeline\"][2][\"map_grid_conf\"] = map_grid_conf\n",
    "    cfg[\"data\"][\"val\"][\"pipeline\"][2][\"map_grid_conf\"] = map_grid_conf\n",
    "    cfg[\"data\"][\"test\"][\"map_grid_conf\"] = map_grid_conf\n",
    "    cfg[\"data\"][\"val\"][\"map_grid_conf\"] = map_grid_conf\n",
    "\n",
    "    cfg[\"model\"][\"pts_bbox_head\"][\"motion_grid_conf\"] = motion_grid_conf\n",
    "\n",
    "    cfg[\"data\"][\"test\"][\"pipeline\"][5][\"point_cloud_range\"] = point_cloud_range\n",
    "    cfg[\"data\"][\"train\"][\"pipeline\"][5][\n",
    "        \"point_cloud_range\"\n",
    "    ] = point_cloud_range  # point_cloud_range=None\n",
    "    cfg[\"data\"][\"train\"][\"pipeline\"][6][\n",
    "        \"point_cloud_range\"\n",
    "    ] = point_cloud_range  #'point_cloud_range =None\n",
    "    cfg[\"data\"][\"val\"][\"pipeline\"][5][\"point_cloud_range\"] = point_cloud_range\n",
    "\n",
    "    cfg[\"model\"][\"pts_bbox_head\"][\"cfg_motion\"][\"receptive_field\"] = receptive_field\n",
    "    cfg[\"data\"][\"train\"][\"dataset\"][\"receptive_field\"] = receptive_field\n",
    "    cfg[\"model\"][\"temporal_model\"][\"receptive_field\"] = receptive_field\n",
    "    cfg[\"data\"][\"test\"][\"receptive_field\"] = receptive_field\n",
    "    cfg[\"data\"][\"val\"][\"receptive_field\"] = receptive_field\n",
    "\n",
    "    cfg[\"data\"][\"val\"][\"future_frames\"] = n_future\n",
    "    cfg[\"model\"][\"pts_bbox_head\"][\"cfg_motion\"][\"n_future\"] = n_future\n",
    "    cfg[\"data\"][\"test\"][\"future_frames\"] = n_future\n",
    "    cfg[\"data\"][\"train\"][\"dataset\"][\"future_frames\"] = n_future\n",
    "    \n",
    "    cfg[\"data\"][\"train\"][\"dataset\"][\"pipeline\"][4][\"map_grid_conf\"] = map_grid_conf\n",
    "    cfg[\"data\"][\"train\"][\"dataset\"][\"pipeline\"][5][\"grid_conf\"] = motion_grid_conf\n",
    "    cfg[\"data\"][\"train\"][\"dataset\"][\"pipeline\"][7][\"point_cloud_range\"] = point_cloud_range\n",
    "\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def import_modules_load_config(cfg_file=\"beverse_tiny.py\", samples_per_gpu=1):\n",
    "    cfg_path = r\"/home/niklas/ETM_BEV/BEVerse/projects/configs\"\n",
    "    cfg_path = os.path.join(cfg_path, cfg_file)\n",
    "\n",
    "    cfg = Config.fromfile(cfg_path)\n",
    "\n",
    "    # if args.cfg_options is not None:\n",
    "    #     cfg.merge_from_dict(args.cfg_options)\n",
    "    # import modules from string list.\n",
    "    if cfg.get(\"custom_imports\", None):\n",
    "        from mmcv.utils import import_modules_from_strings\n",
    "\n",
    "        import_modules_from_strings(**cfg[\"custom_imports\"])\n",
    "\n",
    "    # import modules from plguin/xx, registry will be updated\n",
    "    if hasattr(cfg, \"plugin\"):\n",
    "        if cfg.plugin:\n",
    "            import importlib\n",
    "\n",
    "            if hasattr(cfg, \"plugin_dir\"):\n",
    "                plugin_dir = cfg.plugin_dir\n",
    "                _module_dir = os.path.dirname(plugin_dir)\n",
    "                _module_dir = _module_dir.split(\"/\")\n",
    "                _module_path = _module_dir[0]\n",
    "\n",
    "                for m in _module_dir[1:]:\n",
    "                    _module_path = _module_path + \".\" + m\n",
    "                print(_module_path)\n",
    "                plg_lib = importlib.import_module(_module_path)\n",
    "            else:\n",
    "                # import dir is the dirpath for the config file\n",
    "                _module_dir = cfg_path\n",
    "                _module_dir = _module_dir.split(\"/\")\n",
    "                _module_path = _module_dir[0]\n",
    "                for m in _module_dir[1:]:\n",
    "                    _module_path = _module_path + \".\" + m\n",
    "                print(_module_path)\n",
    "                plg_lib = importlib.import_module(_module_path)\n",
    "\n",
    "    samples_per_gpu = 1\n",
    "    if isinstance(cfg.data.test, dict):\n",
    "        cfg.data.test.test_mode = True\n",
    "        samples_per_gpu = cfg.data.test.pop(\"samples_per_gpu\", 1)\n",
    "        if samples_per_gpu > 1:\n",
    "            # Replace 'ImageToTensor' to 'DefaultFormatBundle'\n",
    "            cfg.data.test.pipeline = replace_ImageToTensor(cfg.data.test.pipeline)\n",
    "    elif isinstance(cfg.data.test, list):\n",
    "        for ds_cfg in cfg.data.test:\n",
    "            ds_cfg.test_mode = True\n",
    "        samples_per_gpu = max(\n",
    "            [ds_cfg.pop(\"samples_per_gpu\", 1) for ds_cfg in cfg.data.test]\n",
    "        )\n",
    "        if samples_per_gpu > 1:\n",
    "            for ds_cfg in cfg.data.test:\n",
    "                ds_cfg.pipeline = replace_ImageToTensor(ds_cfg.pipeline)\n",
    "\n",
    "    return cfg\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def measure_time_host(\n",
    "    model: nn.Module,\n",
    "    input_tensor: torch.Tensor,\n",
    "    num_repeats: int = 100,\n",
    "    num_warmups: int = 10,\n",
    "    synchronize: bool = True,\n",
    "    continuous_measure: bool = True,\n",
    ") -> float:\n",
    "\n",
    "    for _ in range(num_warmups):\n",
    "        _ = model.forward(input_tensor)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    elapsed_time_ms = 0\n",
    "\n",
    "    if continuous_measure:\n",
    "        start = timer()\n",
    "        for _ in range(num_repeats):\n",
    "            _ = model.forward(input_tensor)\n",
    "        if synchronize:\n",
    "            torch.cuda.synchronize()\n",
    "        end = timer()\n",
    "        elapsed_time_ms = (end - start) * 1000\n",
    "\n",
    "    else:\n",
    "        for _ in range(num_repeats):\n",
    "            start = timer()\n",
    "            _ = model.forward(input_tensor)\n",
    "            if synchronize:\n",
    "                torch.cuda.synchronize()\n",
    "            end = timer()\n",
    "            elapsed_time_ms += (end - start) * 1000\n",
    "\n",
    "    return elapsed_time_ms / num_repeats\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def measure_time_device(\n",
    "    model: nn.Module,\n",
    "    input_tensor: torch.Tensor,\n",
    "    num_repeats: int = 100,\n",
    "    num_warmups: int = 10,\n",
    "    synchronize: bool = True,\n",
    "    continuous_measure: bool = True,\n",
    ") -> float:\n",
    "\n",
    "    for _ in range(num_warmups):\n",
    "        _ = model.forward(input_tensor)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    elapsed_time_ms = 0\n",
    "\n",
    "    if continuous_measure:\n",
    "        start_event = torch.cuda.Event(enable_timing=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True)\n",
    "        start_event.record()\n",
    "        for _ in range(num_repeats):\n",
    "            _ = model.forward(input_tensor)\n",
    "        end_event.record()\n",
    "        if synchronize:\n",
    "            # This has to be synchronized to compute the elapsed time.\n",
    "            # Otherwise, there will be runtime error.\n",
    "            torch.cuda.synchronize()\n",
    "        elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "\n",
    "    else:\n",
    "        for _ in range(num_repeats):\n",
    "            start_event = torch.cuda.Event(enable_timing=True)\n",
    "            end_event = torch.cuda.Event(enable_timing=True)\n",
    "            start_event.record()\n",
    "            _ = model.forward(input_tensor)\n",
    "            end_event.record()\n",
    "            if synchronize:\n",
    "                # This has to be synchronized to compute the elapsed time.\n",
    "                # Otherwise, there will be runtime error.\n",
    "                torch.cuda.synchronize()\n",
    "            elapsed_time_ms += start_event.elapsed_time(end_event)\n",
    "\n",
    "    return elapsed_time_ms / num_repeats\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_inference(model: nn.Module, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    # from model.forward because BEVerse differentiates between different input types - img lidar etc \n",
    "    return model.forward_dummy()\n",
    "\n",
    "def calculate_birds_eye_view_parameters(x_bounds, y_bounds, z_bounds):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "        x_bounds: Forward direction in the ego-car.\n",
    "        y_bounds: Sides\n",
    "        z_bounds: Height\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        bev_resolution: Bird's-eye view bev_resolution\n",
    "        bev_start_position Bird's-eye view first element\n",
    "        bev_dimension Bird's-eye view tensor spatial dimension\n",
    "    \"\"\"\n",
    "    bev_resolution = torch.tensor([row[2] for row in [x_bounds, y_bounds, z_bounds]])\n",
    "    bev_start_position = torch.tensor(\n",
    "        [row[0] + row[2] / 2.0 for row in [x_bounds, y_bounds, z_bounds]]\n",
    "    )\n",
    "    bev_dimension = torch.tensor(\n",
    "        [(row[1] - row[0]) / row[2] for row in [x_bounds, y_bounds, z_bounds]],\n",
    "        dtype=torch.long,\n",
    "    )\n",
    "\n",
    "    return bev_resolution, bev_start_position, bev_dimension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "weights = torch.load(\"/home/niklas/ETM_BEV/BEVerse/weights/beverse_tiny.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.OrderedDict"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(weights[\"state_dict\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passed pts_bbox_head.taskfeat_encoders.map.layers.0.0.conv1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.0.0.bn1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.0.0.bn1.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.0.0.bn1.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.0.0.bn1.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.0.0.bn1.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.0.0.conv2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.0.0.bn2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.0.0.bn2.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.0.0.bn2.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.0.0.bn2.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.0.0.bn2.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.0.0.downsample.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.0.0.downsample.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.0.1.conv1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.0.1.bn1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.0.1.bn1.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.0.1.bn1.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.0.1.bn1.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.0.1.bn1.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.0.1.conv2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.0.1.bn2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.0.1.bn2.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.0.1.bn2.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.0.1.bn2.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.0.1.bn2.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.1.0.conv1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.1.0.bn1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.1.0.bn1.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.1.0.bn1.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.1.0.bn1.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.1.0.bn1.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.1.0.conv2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.1.0.bn2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.1.0.bn2.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.1.0.bn2.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.1.0.bn2.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.1.0.bn2.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.1.0.downsample.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.1.0.downsample.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.1.1.conv1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.1.1.bn1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.1.1.bn1.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.1.1.bn1.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.1.1.bn1.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.1.1.bn1.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.1.1.conv2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.1.1.bn2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.1.1.bn2.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.1.1.bn2.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.1.1.bn2.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.1.1.bn2.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.2.0.conv1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.2.0.bn1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.2.0.bn1.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.2.0.bn1.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.2.0.bn1.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.2.0.bn1.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.2.0.conv2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.2.0.bn2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.2.0.bn2.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.2.0.bn2.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.2.0.bn2.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.2.0.bn2.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.2.0.downsample.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.2.0.downsample.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.2.1.conv1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.2.1.bn1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.2.1.bn1.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.2.1.bn1.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.2.1.bn1.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.2.1.bn1.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.2.1.conv2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.2.1.bn2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.2.1.bn2.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.2.1.bn2.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.2.1.bn2.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.map.layers.2.1.bn2.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.map.up1.conv.0.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.map.up1.conv.1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.map.up1.conv.1.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.map.up1.conv.1.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.map.up1.conv.1.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.map.up1.conv.1.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.map.up1.conv.3.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.map.up1.conv.4.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.map.up1.conv.4.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.map.up1.conv.4.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.map.up1.conv.4.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.map.up1.conv.4.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.map.up2.1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.map.up2.2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.map.up2.2.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.map.up2.2.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.map.up2.2.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.map.up2.2.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.map.up2.4.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.map.up2.4.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.0.0.conv1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.0.0.bn1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.0.0.bn1.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.0.0.bn1.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.0.0.bn1.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.0.0.bn1.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.0.0.conv2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.0.0.bn2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.0.0.bn2.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.0.0.bn2.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.0.0.bn2.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.0.0.bn2.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.0.0.downsample.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.0.0.downsample.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.0.1.conv1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.0.1.bn1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.0.1.bn1.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.0.1.bn1.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.0.1.bn1.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.0.1.bn1.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.0.1.conv2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.0.1.bn2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.0.1.bn2.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.0.1.bn2.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.0.1.bn2.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.0.1.bn2.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.1.0.conv1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.1.0.bn1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.1.0.bn1.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.1.0.bn1.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.1.0.bn1.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.1.0.bn1.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.1.0.conv2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.1.0.bn2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.1.0.bn2.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.1.0.bn2.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.1.0.bn2.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.1.0.bn2.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.1.0.downsample.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.1.0.downsample.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.1.1.conv1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.1.1.bn1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.1.1.bn1.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.1.1.bn1.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.1.1.bn1.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.1.1.bn1.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.1.1.conv2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.1.1.bn2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.1.1.bn2.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.1.1.bn2.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.1.1.bn2.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.1.1.bn2.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.2.0.conv1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.2.0.bn1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.2.0.bn1.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.2.0.bn1.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.2.0.bn1.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.2.0.bn1.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.2.0.conv2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.2.0.bn2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.2.0.bn2.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.2.0.bn2.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.2.0.bn2.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.2.0.bn2.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.2.0.downsample.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.2.0.downsample.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.2.1.conv1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.2.1.bn1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.2.1.bn1.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.2.1.bn1.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.2.1.bn1.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.2.1.bn1.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.2.1.conv2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.2.1.bn2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.2.1.bn2.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.2.1.bn2.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.2.1.bn2.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.layers.2.1.bn2.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.up1.conv.0.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.up1.conv.1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.up1.conv.1.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.up1.conv.1.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.up1.conv.1.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.up1.conv.1.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.up1.conv.3.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.up1.conv.4.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.up1.conv.4.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.up1.conv.4.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.up1.conv.4.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.up1.conv.4.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.up2.1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.up2.2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.up2.2.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.up2.2.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.up2.2.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.up2.2.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.up2.4.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.3dod.up2.4.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.0.0.conv1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.0.0.bn1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.0.0.bn1.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.0.0.bn1.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.0.0.bn1.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.0.0.bn1.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.0.0.conv2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.0.0.bn2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.0.0.bn2.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.0.0.bn2.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.0.0.bn2.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.0.0.bn2.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.0.0.downsample.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.0.0.downsample.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.0.1.conv1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.0.1.bn1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.0.1.bn1.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.0.1.bn1.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.0.1.bn1.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.0.1.bn1.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.0.1.conv2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.0.1.bn2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.0.1.bn2.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.0.1.bn2.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.0.1.bn2.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.0.1.bn2.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.1.0.conv1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.1.0.bn1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.1.0.bn1.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.1.0.bn1.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.1.0.bn1.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.1.0.bn1.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.1.0.conv2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.1.0.bn2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.1.0.bn2.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.1.0.bn2.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.1.0.bn2.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.1.0.bn2.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.1.0.downsample.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.1.0.downsample.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.1.1.conv1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.1.1.bn1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.1.1.bn1.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.1.1.bn1.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.1.1.bn1.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.1.1.bn1.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.1.1.conv2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.1.1.bn2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.1.1.bn2.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.1.1.bn2.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.1.1.bn2.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.1.1.bn2.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.2.0.conv1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.2.0.bn1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.2.0.bn1.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.2.0.bn1.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.2.0.bn1.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.2.0.bn1.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.2.0.conv2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.2.0.bn2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.2.0.bn2.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.2.0.bn2.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.2.0.bn2.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.2.0.bn2.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.2.0.downsample.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.2.0.downsample.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.2.1.conv1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.2.1.bn1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.2.1.bn1.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.2.1.bn1.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.2.1.bn1.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.2.1.bn1.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.2.1.conv2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.2.1.bn2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.2.1.bn2.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.2.1.bn2.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.2.1.bn2.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.layers.2.1.bn2.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.up1.conv.0.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.up1.conv.1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.up1.conv.1.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.up1.conv.1.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.up1.conv.1.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.up1.conv.1.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.up1.conv.3.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.up1.conv.4.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.up1.conv.4.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.up1.conv.4.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.up1.conv.4.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.up1.conv.4.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.up2.1.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.up2.2.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.up2.2.bias\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.up2.2.running_mean\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.up2.2.running_var\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.up2.2.num_batches_tracked\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.up2.4.weight\n",
      "passed pts_bbox_head.taskfeat_encoders.motion.up2.4.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.shared_conv.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.shared_conv.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.shared_conv.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.shared_conv.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.shared_conv.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.shared_conv.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.reg.0.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.reg.0.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.reg.0.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.reg.0.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.reg.0.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.reg.0.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.reg.1.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.reg.1.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.height.0.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.height.0.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.height.0.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.height.0.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.height.0.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.height.0.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.height.1.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.height.1.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.dim.0.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.dim.0.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.dim.0.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.dim.0.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.dim.0.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.dim.0.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.dim.1.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.dim.1.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.rot.0.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.rot.0.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.rot.0.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.rot.0.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.rot.0.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.rot.0.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.rot.1.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.rot.1.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.vel.0.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.vel.0.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.vel.0.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.vel.0.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.vel.0.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.vel.0.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.vel.1.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.vel.1.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.heatmap.0.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.heatmap.0.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.heatmap.0.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.heatmap.0.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.heatmap.0.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.heatmap.0.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.heatmap.1.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.0.heatmap.1.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.reg.0.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.reg.0.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.reg.0.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.reg.0.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.reg.0.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.reg.0.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.reg.1.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.reg.1.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.height.0.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.height.0.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.height.0.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.height.0.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.height.0.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.height.0.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.height.1.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.height.1.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.dim.0.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.dim.0.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.dim.0.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.dim.0.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.dim.0.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.dim.0.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.dim.1.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.dim.1.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.rot.0.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.rot.0.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.rot.0.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.rot.0.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.rot.0.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.rot.0.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.rot.1.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.rot.1.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.vel.0.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.vel.0.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.vel.0.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.vel.0.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.vel.0.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.vel.0.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.vel.1.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.vel.1.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.heatmap.0.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.heatmap.0.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.heatmap.0.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.heatmap.0.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.heatmap.0.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.heatmap.0.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.heatmap.1.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.1.heatmap.1.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.reg.0.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.reg.0.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.reg.0.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.reg.0.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.reg.0.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.reg.0.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.reg.1.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.reg.1.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.height.0.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.height.0.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.height.0.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.height.0.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.height.0.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.height.0.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.height.1.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.height.1.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.dim.0.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.dim.0.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.dim.0.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.dim.0.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.dim.0.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.dim.0.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.dim.1.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.dim.1.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.rot.0.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.rot.0.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.rot.0.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.rot.0.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.rot.0.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.rot.0.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.rot.1.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.rot.1.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.vel.0.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.vel.0.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.vel.0.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.vel.0.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.vel.0.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.vel.0.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.vel.1.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.vel.1.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.heatmap.0.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.heatmap.0.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.heatmap.0.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.heatmap.0.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.heatmap.0.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.heatmap.0.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.heatmap.1.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.2.heatmap.1.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.reg.0.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.reg.0.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.reg.0.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.reg.0.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.reg.0.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.reg.0.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.reg.1.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.reg.1.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.height.0.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.height.0.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.height.0.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.height.0.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.height.0.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.height.0.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.height.1.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.height.1.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.dim.0.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.dim.0.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.dim.0.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.dim.0.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.dim.0.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.dim.0.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.dim.1.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.dim.1.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.rot.0.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.rot.0.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.rot.0.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.rot.0.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.rot.0.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.rot.0.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.rot.1.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.rot.1.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.vel.0.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.vel.0.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.vel.0.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.vel.0.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.vel.0.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.vel.0.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.vel.1.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.vel.1.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.heatmap.0.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.heatmap.0.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.heatmap.0.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.heatmap.0.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.heatmap.0.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.heatmap.0.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.heatmap.1.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.3.heatmap.1.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.reg.0.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.reg.0.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.reg.0.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.reg.0.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.reg.0.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.reg.0.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.reg.1.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.reg.1.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.height.0.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.height.0.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.height.0.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.height.0.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.height.0.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.height.0.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.height.1.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.height.1.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.dim.0.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.dim.0.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.dim.0.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.dim.0.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.dim.0.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.dim.0.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.dim.1.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.dim.1.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.rot.0.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.rot.0.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.rot.0.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.rot.0.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.rot.0.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.rot.0.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.rot.1.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.rot.1.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.vel.0.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.vel.0.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.vel.0.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.vel.0.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.vel.0.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.vel.0.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.vel.1.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.vel.1.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.heatmap.0.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.heatmap.0.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.heatmap.0.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.heatmap.0.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.heatmap.0.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.heatmap.0.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.heatmap.1.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.4.heatmap.1.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.reg.0.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.reg.0.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.reg.0.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.reg.0.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.reg.0.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.reg.0.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.reg.1.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.reg.1.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.height.0.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.height.0.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.height.0.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.height.0.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.height.0.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.height.0.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.height.1.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.height.1.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.dim.0.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.dim.0.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.dim.0.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.dim.0.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.dim.0.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.dim.0.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.dim.1.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.dim.1.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.rot.0.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.rot.0.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.rot.0.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.rot.0.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.rot.0.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.rot.0.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.rot.1.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.rot.1.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.vel.0.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.vel.0.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.vel.0.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.vel.0.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.vel.0.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.vel.0.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.vel.1.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.vel.1.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.heatmap.0.conv.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.heatmap.0.bn.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.heatmap.0.bn.bias\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.heatmap.0.bn.running_mean\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.heatmap.0.bn.running_var\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.heatmap.0.bn.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.heatmap.1.weight\n",
      "passed pts_bbox_head.task_decoders.3dod.task_heads.5.heatmap.1.bias\n",
      "passed pts_bbox_head.task_decoders.map.task_heads.semantic_seg.0.weight\n",
      "passed pts_bbox_head.task_decoders.map.task_heads.semantic_seg.1.weight\n",
      "passed pts_bbox_head.task_decoders.map.task_heads.semantic_seg.1.bias\n",
      "passed pts_bbox_head.task_decoders.map.task_heads.semantic_seg.1.running_mean\n",
      "passed pts_bbox_head.task_decoders.map.task_heads.semantic_seg.1.running_var\n",
      "passed pts_bbox_head.task_decoders.map.task_heads.semantic_seg.1.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.map.task_heads.semantic_seg.3.weight\n",
      "passed pts_bbox_head.task_decoders.map.task_heads.semantic_seg.3.bias\n",
      "passed pts_bbox_head.task_decoders.motion.task_heads.segmentation.0.weight\n",
      "passed pts_bbox_head.task_decoders.motion.task_heads.segmentation.1.weight\n",
      "passed pts_bbox_head.task_decoders.motion.task_heads.segmentation.1.bias\n",
      "passed pts_bbox_head.task_decoders.motion.task_heads.segmentation.1.running_mean\n",
      "passed pts_bbox_head.task_decoders.motion.task_heads.segmentation.1.running_var\n",
      "passed pts_bbox_head.task_decoders.motion.task_heads.segmentation.1.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.motion.task_heads.segmentation.3.weight\n",
      "passed pts_bbox_head.task_decoders.motion.task_heads.segmentation.3.bias\n",
      "passed pts_bbox_head.task_decoders.motion.task_heads.instance_center.0.weight\n",
      "passed pts_bbox_head.task_decoders.motion.task_heads.instance_center.1.weight\n",
      "passed pts_bbox_head.task_decoders.motion.task_heads.instance_center.1.bias\n",
      "passed pts_bbox_head.task_decoders.motion.task_heads.instance_center.1.running_mean\n",
      "passed pts_bbox_head.task_decoders.motion.task_heads.instance_center.1.running_var\n",
      "passed pts_bbox_head.task_decoders.motion.task_heads.instance_center.1.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.motion.task_heads.instance_center.3.weight\n",
      "passed pts_bbox_head.task_decoders.motion.task_heads.instance_center.3.bias\n",
      "passed pts_bbox_head.task_decoders.motion.task_heads.instance_offset.0.weight\n",
      "passed pts_bbox_head.task_decoders.motion.task_heads.instance_offset.1.weight\n",
      "passed pts_bbox_head.task_decoders.motion.task_heads.instance_offset.1.bias\n",
      "passed pts_bbox_head.task_decoders.motion.task_heads.instance_offset.1.running_mean\n",
      "passed pts_bbox_head.task_decoders.motion.task_heads.instance_offset.1.running_var\n",
      "passed pts_bbox_head.task_decoders.motion.task_heads.instance_offset.1.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.motion.task_heads.instance_offset.3.weight\n",
      "passed pts_bbox_head.task_decoders.motion.task_heads.instance_offset.3.bias\n",
      "passed pts_bbox_head.task_decoders.motion.task_heads.instance_flow.0.weight\n",
      "passed pts_bbox_head.task_decoders.motion.task_heads.instance_flow.1.weight\n",
      "passed pts_bbox_head.task_decoders.motion.task_heads.instance_flow.1.bias\n",
      "passed pts_bbox_head.task_decoders.motion.task_heads.instance_flow.1.running_mean\n",
      "passed pts_bbox_head.task_decoders.motion.task_heads.instance_flow.1.running_var\n",
      "passed pts_bbox_head.task_decoders.motion.task_heads.instance_flow.1.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.motion.task_heads.instance_flow.3.weight\n",
      "passed pts_bbox_head.task_decoders.motion.task_heads.instance_flow.3.bias\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.0.layers.conv_down_project.weight\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.0.layers.abn_down_project.0.weight\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.0.layers.abn_down_project.0.bias\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.0.layers.abn_down_project.0.running_mean\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.0.layers.abn_down_project.0.running_var\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.0.layers.abn_down_project.0.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.0.layers.conv.weight\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.0.layers.abn.0.weight\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.0.layers.abn.0.bias\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.0.layers.abn.0.running_mean\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.0.layers.abn.0.running_var\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.0.layers.abn.0.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.0.layers.conv_up_project.weight\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.0.layers.abn_up_project.0.weight\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.0.layers.abn_up_project.0.bias\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.0.layers.abn_up_project.0.running_mean\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.0.layers.abn_up_project.0.running_var\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.0.layers.abn_up_project.0.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.0.projection.conv_skip_proj.weight\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.0.projection.bn_skip_proj.weight\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.0.projection.bn_skip_proj.bias\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.0.projection.bn_skip_proj.running_mean\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.0.projection.bn_skip_proj.running_var\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.0.projection.bn_skip_proj.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.1.layers.conv_down_project.weight\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.1.layers.abn_down_project.0.weight\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.1.layers.abn_down_project.0.bias\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.1.layers.abn_down_project.0.running_mean\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.1.layers.abn_down_project.0.running_var\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.1.layers.abn_down_project.0.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.1.layers.conv.weight\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.1.layers.abn.0.weight\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.1.layers.abn.0.bias\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.1.layers.abn.0.running_mean\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.1.layers.abn.0.running_var\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.1.layers.abn.0.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.1.layers.conv_up_project.weight\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.1.layers.abn_up_project.0.weight\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.1.layers.abn_up_project.0.bias\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.1.layers.abn_up_project.0.running_mean\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.1.layers.abn_up_project.0.running_var\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.1.layers.abn_up_project.0.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.1.projection.conv_skip_proj.weight\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.1.projection.bn_skip_proj.weight\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.1.projection.bn_skip_proj.bias\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.1.projection.bn_skip_proj.running_mean\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.1.projection.bn_skip_proj.running_var\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.encoder.model.1.projection.bn_skip_proj.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.last_conv.0.weight\n",
      "passed pts_bbox_head.task_decoders.motion.present_distribution.last_conv.0.bias\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.0.layers.conv_down_project.weight\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.0.layers.abn_down_project.0.weight\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.0.layers.abn_down_project.0.bias\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.0.layers.abn_down_project.0.running_mean\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.0.layers.abn_down_project.0.running_var\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.0.layers.abn_down_project.0.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.0.layers.conv.weight\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.0.layers.abn.0.weight\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.0.layers.abn.0.bias\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.0.layers.abn.0.running_mean\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.0.layers.abn.0.running_var\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.0.layers.abn.0.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.0.layers.conv_up_project.weight\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.0.layers.abn_up_project.0.weight\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.0.layers.abn_up_project.0.bias\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.0.layers.abn_up_project.0.running_mean\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.0.layers.abn_up_project.0.running_var\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.0.layers.abn_up_project.0.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.0.projection.conv_skip_proj.weight\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.0.projection.bn_skip_proj.weight\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.0.projection.bn_skip_proj.bias\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.0.projection.bn_skip_proj.running_mean\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.0.projection.bn_skip_proj.running_var\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.0.projection.bn_skip_proj.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.1.layers.conv_down_project.weight\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.1.layers.abn_down_project.0.weight\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.1.layers.abn_down_project.0.bias\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.1.layers.abn_down_project.0.running_mean\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.1.layers.abn_down_project.0.running_var\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.1.layers.abn_down_project.0.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.1.layers.conv.weight\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.1.layers.abn.0.weight\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.1.layers.abn.0.bias\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.1.layers.abn.0.running_mean\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.1.layers.abn.0.running_var\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.1.layers.abn.0.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.1.layers.conv_up_project.weight\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.1.layers.abn_up_project.0.weight\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.1.layers.abn_up_project.0.bias\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.1.layers.abn_up_project.0.running_mean\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.1.layers.abn_up_project.0.running_var\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.1.layers.abn_up_project.0.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.1.projection.conv_skip_proj.weight\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.1.projection.bn_skip_proj.weight\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.1.projection.bn_skip_proj.bias\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.1.projection.bn_skip_proj.running_mean\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.1.projection.bn_skip_proj.running_var\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.encoder.model.1.projection.bn_skip_proj.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.last_conv.0.weight\n",
      "passed pts_bbox_head.task_decoders.motion.future_distribution.last_conv.0.bias\n",
      "passed pts_bbox_head.task_decoders.motion.future_prediction.offset_conv.conv.weight\n",
      "passed pts_bbox_head.task_decoders.motion.future_prediction.offset_conv.norm.weight\n",
      "passed pts_bbox_head.task_decoders.motion.future_prediction.offset_conv.norm.bias\n",
      "passed pts_bbox_head.task_decoders.motion.future_prediction.offset_conv.norm.running_mean\n",
      "passed pts_bbox_head.task_decoders.motion.future_prediction.offset_conv.norm.running_var\n",
      "passed pts_bbox_head.task_decoders.motion.future_prediction.offset_conv.norm.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.motion.future_prediction.offset_pred.weight\n",
      "passed pts_bbox_head.task_decoders.motion.future_prediction.offset_pred.bias\n",
      "passed pts_bbox_head.task_decoders.motion.future_prediction.gru_cells.0.conv_update.weight\n",
      "passed pts_bbox_head.task_decoders.motion.future_prediction.gru_cells.0.conv_update.bias\n",
      "passed pts_bbox_head.task_decoders.motion.future_prediction.gru_cells.0.conv_reset.weight\n",
      "passed pts_bbox_head.task_decoders.motion.future_prediction.gru_cells.0.conv_reset.bias\n",
      "passed pts_bbox_head.task_decoders.motion.future_prediction.gru_cells.0.conv_state_tilde.conv.weight\n",
      "passed pts_bbox_head.task_decoders.motion.future_prediction.gru_cells.0.conv_state_tilde.norm.weight\n",
      "passed pts_bbox_head.task_decoders.motion.future_prediction.gru_cells.0.conv_state_tilde.norm.bias\n",
      "passed pts_bbox_head.task_decoders.motion.future_prediction.gru_cells.0.conv_state_tilde.norm.running_mean\n",
      "passed pts_bbox_head.task_decoders.motion.future_prediction.gru_cells.0.conv_state_tilde.norm.running_var\n",
      "passed pts_bbox_head.task_decoders.motion.future_prediction.gru_cells.0.conv_state_tilde.norm.num_batches_tracked\n",
      "passed pts_bbox_head.task_decoders.motion.future_prediction.spatial_conv.conv.weight\n",
      "passed pts_bbox_head.task_decoders.motion.future_prediction.spatial_conv.norm.weight\n",
      "passed pts_bbox_head.task_decoders.motion.future_prediction.spatial_conv.norm.bias\n",
      "passed pts_bbox_head.task_decoders.motion.future_prediction.spatial_conv.norm.running_mean\n",
      "passed pts_bbox_head.task_decoders.motion.future_prediction.spatial_conv.norm.running_var\n",
      "passed pts_bbox_head.task_decoders.motion.future_prediction.spatial_conv.norm.num_batches_tracked\n",
      "took img_backbone.patch_embed.projection.weight\n",
      "took img_backbone.patch_embed.projection.bias\n",
      "took img_backbone.patch_embed.norm.weight\n",
      "took img_backbone.patch_embed.norm.bias\n",
      "took img_backbone.stages.0.blocks.0.norm1.weight\n",
      "took img_backbone.stages.0.blocks.0.norm1.bias\n",
      "took img_backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table\n",
      "took img_backbone.stages.0.blocks.0.attn.w_msa.relative_position_index\n",
      "took img_backbone.stages.0.blocks.0.attn.w_msa.qkv.weight\n",
      "took img_backbone.stages.0.blocks.0.attn.w_msa.qkv.bias\n",
      "took img_backbone.stages.0.blocks.0.attn.w_msa.proj.weight\n",
      "took img_backbone.stages.0.blocks.0.attn.w_msa.proj.bias\n",
      "took img_backbone.stages.0.blocks.0.norm2.weight\n",
      "took img_backbone.stages.0.blocks.0.norm2.bias\n",
      "took img_backbone.stages.0.blocks.0.ffn.layers.0.0.weight\n",
      "took img_backbone.stages.0.blocks.0.ffn.layers.0.0.bias\n",
      "took img_backbone.stages.0.blocks.0.ffn.layers.1.weight\n",
      "took img_backbone.stages.0.blocks.0.ffn.layers.1.bias\n",
      "took img_backbone.stages.0.blocks.1.norm1.weight\n",
      "took img_backbone.stages.0.blocks.1.norm1.bias\n",
      "took img_backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table\n",
      "took img_backbone.stages.0.blocks.1.attn.w_msa.relative_position_index\n",
      "took img_backbone.stages.0.blocks.1.attn.w_msa.qkv.weight\n",
      "took img_backbone.stages.0.blocks.1.attn.w_msa.qkv.bias\n",
      "took img_backbone.stages.0.blocks.1.attn.w_msa.proj.weight\n",
      "took img_backbone.stages.0.blocks.1.attn.w_msa.proj.bias\n",
      "took img_backbone.stages.0.blocks.1.norm2.weight\n",
      "took img_backbone.stages.0.blocks.1.norm2.bias\n",
      "took img_backbone.stages.0.blocks.1.ffn.layers.0.0.weight\n",
      "took img_backbone.stages.0.blocks.1.ffn.layers.0.0.bias\n",
      "took img_backbone.stages.0.blocks.1.ffn.layers.1.weight\n",
      "took img_backbone.stages.0.blocks.1.ffn.layers.1.bias\n",
      "took img_backbone.stages.0.downsample.norm.weight\n",
      "took img_backbone.stages.0.downsample.norm.bias\n",
      "took img_backbone.stages.0.downsample.reduction.weight\n",
      "took img_backbone.stages.1.blocks.0.norm1.weight\n",
      "took img_backbone.stages.1.blocks.0.norm1.bias\n",
      "took img_backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table\n",
      "took img_backbone.stages.1.blocks.0.attn.w_msa.relative_position_index\n",
      "took img_backbone.stages.1.blocks.0.attn.w_msa.qkv.weight\n",
      "took img_backbone.stages.1.blocks.0.attn.w_msa.qkv.bias\n",
      "took img_backbone.stages.1.blocks.0.attn.w_msa.proj.weight\n",
      "took img_backbone.stages.1.blocks.0.attn.w_msa.proj.bias\n",
      "took img_backbone.stages.1.blocks.0.norm2.weight\n",
      "took img_backbone.stages.1.blocks.0.norm2.bias\n",
      "took img_backbone.stages.1.blocks.0.ffn.layers.0.0.weight\n",
      "took img_backbone.stages.1.blocks.0.ffn.layers.0.0.bias\n",
      "took img_backbone.stages.1.blocks.0.ffn.layers.1.weight\n",
      "took img_backbone.stages.1.blocks.0.ffn.layers.1.bias\n",
      "took img_backbone.stages.1.blocks.1.norm1.weight\n",
      "took img_backbone.stages.1.blocks.1.norm1.bias\n",
      "took img_backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table\n",
      "took img_backbone.stages.1.blocks.1.attn.w_msa.relative_position_index\n",
      "took img_backbone.stages.1.blocks.1.attn.w_msa.qkv.weight\n",
      "took img_backbone.stages.1.blocks.1.attn.w_msa.qkv.bias\n",
      "took img_backbone.stages.1.blocks.1.attn.w_msa.proj.weight\n",
      "took img_backbone.stages.1.blocks.1.attn.w_msa.proj.bias\n",
      "took img_backbone.stages.1.blocks.1.norm2.weight\n",
      "took img_backbone.stages.1.blocks.1.norm2.bias\n",
      "took img_backbone.stages.1.blocks.1.ffn.layers.0.0.weight\n",
      "took img_backbone.stages.1.blocks.1.ffn.layers.0.0.bias\n",
      "took img_backbone.stages.1.blocks.1.ffn.layers.1.weight\n",
      "took img_backbone.stages.1.blocks.1.ffn.layers.1.bias\n",
      "took img_backbone.stages.1.downsample.norm.weight\n",
      "took img_backbone.stages.1.downsample.norm.bias\n",
      "took img_backbone.stages.1.downsample.reduction.weight\n",
      "took img_backbone.stages.2.blocks.0.norm1.weight\n",
      "took img_backbone.stages.2.blocks.0.norm1.bias\n",
      "took img_backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table\n",
      "took img_backbone.stages.2.blocks.0.attn.w_msa.relative_position_index\n",
      "took img_backbone.stages.2.blocks.0.attn.w_msa.qkv.weight\n",
      "took img_backbone.stages.2.blocks.0.attn.w_msa.qkv.bias\n",
      "took img_backbone.stages.2.blocks.0.attn.w_msa.proj.weight\n",
      "took img_backbone.stages.2.blocks.0.attn.w_msa.proj.bias\n",
      "took img_backbone.stages.2.blocks.0.norm2.weight\n",
      "took img_backbone.stages.2.blocks.0.norm2.bias\n",
      "took img_backbone.stages.2.blocks.0.ffn.layers.0.0.weight\n",
      "took img_backbone.stages.2.blocks.0.ffn.layers.0.0.bias\n",
      "took img_backbone.stages.2.blocks.0.ffn.layers.1.weight\n",
      "took img_backbone.stages.2.blocks.0.ffn.layers.1.bias\n",
      "took img_backbone.stages.2.blocks.1.norm1.weight\n",
      "took img_backbone.stages.2.blocks.1.norm1.bias\n",
      "took img_backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table\n",
      "took img_backbone.stages.2.blocks.1.attn.w_msa.relative_position_index\n",
      "took img_backbone.stages.2.blocks.1.attn.w_msa.qkv.weight\n",
      "took img_backbone.stages.2.blocks.1.attn.w_msa.qkv.bias\n",
      "took img_backbone.stages.2.blocks.1.attn.w_msa.proj.weight\n",
      "took img_backbone.stages.2.blocks.1.attn.w_msa.proj.bias\n",
      "took img_backbone.stages.2.blocks.1.norm2.weight\n",
      "took img_backbone.stages.2.blocks.1.norm2.bias\n",
      "took img_backbone.stages.2.blocks.1.ffn.layers.0.0.weight\n",
      "took img_backbone.stages.2.blocks.1.ffn.layers.0.0.bias\n",
      "took img_backbone.stages.2.blocks.1.ffn.layers.1.weight\n",
      "took img_backbone.stages.2.blocks.1.ffn.layers.1.bias\n",
      "took img_backbone.stages.2.blocks.2.norm1.weight\n",
      "took img_backbone.stages.2.blocks.2.norm1.bias\n",
      "took img_backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table\n",
      "took img_backbone.stages.2.blocks.2.attn.w_msa.relative_position_index\n",
      "took img_backbone.stages.2.blocks.2.attn.w_msa.qkv.weight\n",
      "took img_backbone.stages.2.blocks.2.attn.w_msa.qkv.bias\n",
      "took img_backbone.stages.2.blocks.2.attn.w_msa.proj.weight\n",
      "took img_backbone.stages.2.blocks.2.attn.w_msa.proj.bias\n",
      "took img_backbone.stages.2.blocks.2.norm2.weight\n",
      "took img_backbone.stages.2.blocks.2.norm2.bias\n",
      "took img_backbone.stages.2.blocks.2.ffn.layers.0.0.weight\n",
      "took img_backbone.stages.2.blocks.2.ffn.layers.0.0.bias\n",
      "took img_backbone.stages.2.blocks.2.ffn.layers.1.weight\n",
      "took img_backbone.stages.2.blocks.2.ffn.layers.1.bias\n",
      "took img_backbone.stages.2.blocks.3.norm1.weight\n",
      "took img_backbone.stages.2.blocks.3.norm1.bias\n",
      "took img_backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table\n",
      "took img_backbone.stages.2.blocks.3.attn.w_msa.relative_position_index\n",
      "took img_backbone.stages.2.blocks.3.attn.w_msa.qkv.weight\n",
      "took img_backbone.stages.2.blocks.3.attn.w_msa.qkv.bias\n",
      "took img_backbone.stages.2.blocks.3.attn.w_msa.proj.weight\n",
      "took img_backbone.stages.2.blocks.3.attn.w_msa.proj.bias\n",
      "took img_backbone.stages.2.blocks.3.norm2.weight\n",
      "took img_backbone.stages.2.blocks.3.norm2.bias\n",
      "took img_backbone.stages.2.blocks.3.ffn.layers.0.0.weight\n",
      "took img_backbone.stages.2.blocks.3.ffn.layers.0.0.bias\n",
      "took img_backbone.stages.2.blocks.3.ffn.layers.1.weight\n",
      "took img_backbone.stages.2.blocks.3.ffn.layers.1.bias\n",
      "took img_backbone.stages.2.blocks.4.norm1.weight\n",
      "took img_backbone.stages.2.blocks.4.norm1.bias\n",
      "took img_backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table\n",
      "took img_backbone.stages.2.blocks.4.attn.w_msa.relative_position_index\n",
      "took img_backbone.stages.2.blocks.4.attn.w_msa.qkv.weight\n",
      "took img_backbone.stages.2.blocks.4.attn.w_msa.qkv.bias\n",
      "took img_backbone.stages.2.blocks.4.attn.w_msa.proj.weight\n",
      "took img_backbone.stages.2.blocks.4.attn.w_msa.proj.bias\n",
      "took img_backbone.stages.2.blocks.4.norm2.weight\n",
      "took img_backbone.stages.2.blocks.4.norm2.bias\n",
      "took img_backbone.stages.2.blocks.4.ffn.layers.0.0.weight\n",
      "took img_backbone.stages.2.blocks.4.ffn.layers.0.0.bias\n",
      "took img_backbone.stages.2.blocks.4.ffn.layers.1.weight\n",
      "took img_backbone.stages.2.blocks.4.ffn.layers.1.bias\n",
      "took img_backbone.stages.2.blocks.5.norm1.weight\n",
      "took img_backbone.stages.2.blocks.5.norm1.bias\n",
      "took img_backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table\n",
      "took img_backbone.stages.2.blocks.5.attn.w_msa.relative_position_index\n",
      "took img_backbone.stages.2.blocks.5.attn.w_msa.qkv.weight\n",
      "took img_backbone.stages.2.blocks.5.attn.w_msa.qkv.bias\n",
      "took img_backbone.stages.2.blocks.5.attn.w_msa.proj.weight\n",
      "took img_backbone.stages.2.blocks.5.attn.w_msa.proj.bias\n",
      "took img_backbone.stages.2.blocks.5.norm2.weight\n",
      "took img_backbone.stages.2.blocks.5.norm2.bias\n",
      "took img_backbone.stages.2.blocks.5.ffn.layers.0.0.weight\n",
      "took img_backbone.stages.2.blocks.5.ffn.layers.0.0.bias\n",
      "took img_backbone.stages.2.blocks.5.ffn.layers.1.weight\n",
      "took img_backbone.stages.2.blocks.5.ffn.layers.1.bias\n",
      "took img_backbone.stages.2.downsample.norm.weight\n",
      "took img_backbone.stages.2.downsample.norm.bias\n",
      "took img_backbone.stages.2.downsample.reduction.weight\n",
      "took img_backbone.stages.3.blocks.0.norm1.weight\n",
      "took img_backbone.stages.3.blocks.0.norm1.bias\n",
      "took img_backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table\n",
      "took img_backbone.stages.3.blocks.0.attn.w_msa.relative_position_index\n",
      "took img_backbone.stages.3.blocks.0.attn.w_msa.qkv.weight\n",
      "took img_backbone.stages.3.blocks.0.attn.w_msa.qkv.bias\n",
      "took img_backbone.stages.3.blocks.0.attn.w_msa.proj.weight\n",
      "took img_backbone.stages.3.blocks.0.attn.w_msa.proj.bias\n",
      "took img_backbone.stages.3.blocks.0.norm2.weight\n",
      "took img_backbone.stages.3.blocks.0.norm2.bias\n",
      "took img_backbone.stages.3.blocks.0.ffn.layers.0.0.weight\n",
      "took img_backbone.stages.3.blocks.0.ffn.layers.0.0.bias\n",
      "took img_backbone.stages.3.blocks.0.ffn.layers.1.weight\n",
      "took img_backbone.stages.3.blocks.0.ffn.layers.1.bias\n",
      "took img_backbone.stages.3.blocks.1.norm1.weight\n",
      "took img_backbone.stages.3.blocks.1.norm1.bias\n",
      "took img_backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table\n",
      "took img_backbone.stages.3.blocks.1.attn.w_msa.relative_position_index\n",
      "took img_backbone.stages.3.blocks.1.attn.w_msa.qkv.weight\n",
      "took img_backbone.stages.3.blocks.1.attn.w_msa.qkv.bias\n",
      "took img_backbone.stages.3.blocks.1.attn.w_msa.proj.weight\n",
      "took img_backbone.stages.3.blocks.1.attn.w_msa.proj.bias\n",
      "took img_backbone.stages.3.blocks.1.norm2.weight\n",
      "took img_backbone.stages.3.blocks.1.norm2.bias\n",
      "took img_backbone.stages.3.blocks.1.ffn.layers.0.0.weight\n",
      "took img_backbone.stages.3.blocks.1.ffn.layers.0.0.bias\n",
      "took img_backbone.stages.3.blocks.1.ffn.layers.1.weight\n",
      "took img_backbone.stages.3.blocks.1.ffn.layers.1.bias\n",
      "took img_backbone.norm2.weight\n",
      "took img_backbone.norm2.bias\n",
      "took img_backbone.norm3.weight\n",
      "took img_backbone.norm3.bias\n",
      "took img_neck.up.conv.0.weight\n",
      "took img_neck.up.conv.1.weight\n",
      "took img_neck.up.conv.1.bias\n",
      "took img_neck.up.conv.1.running_mean\n",
      "took img_neck.up.conv.1.running_var\n",
      "took img_neck.up.conv.1.num_batches_tracked\n",
      "took img_neck.up.conv.3.weight\n",
      "took img_neck.up.conv.4.weight\n",
      "took img_neck.up.conv.4.bias\n",
      "took img_neck.up.conv.4.running_mean\n",
      "took img_neck.up.conv.4.running_var\n",
      "took img_neck.up.conv.4.num_batches_tracked\n",
      "took transformer.frustum\n",
      "took transformer.depthnet.weight\n",
      "took transformer.depthnet.bias\n",
      "took temporal_model.model.0.convolution_paths.0.0.conv.weight\n",
      "took temporal_model.model.0.convolution_paths.0.0.norm.weight\n",
      "took temporal_model.model.0.convolution_paths.0.0.norm.bias\n",
      "took temporal_model.model.0.convolution_paths.0.0.norm.running_mean\n",
      "took temporal_model.model.0.convolution_paths.0.0.norm.running_var\n",
      "took temporal_model.model.0.convolution_paths.0.0.norm.num_batches_tracked\n",
      "took temporal_model.model.0.convolution_paths.0.1.conv.weight\n",
      "took temporal_model.model.0.convolution_paths.0.1.norm.weight\n",
      "took temporal_model.model.0.convolution_paths.0.1.norm.bias\n",
      "took temporal_model.model.0.convolution_paths.0.1.norm.running_mean\n",
      "took temporal_model.model.0.convolution_paths.0.1.norm.running_var\n",
      "took temporal_model.model.0.convolution_paths.0.1.norm.num_batches_tracked\n",
      "took temporal_model.model.0.convolution_paths.1.0.conv.weight\n",
      "took temporal_model.model.0.convolution_paths.1.0.norm.weight\n",
      "took temporal_model.model.0.convolution_paths.1.0.norm.bias\n",
      "took temporal_model.model.0.convolution_paths.1.0.norm.running_mean\n",
      "took temporal_model.model.0.convolution_paths.1.0.norm.running_var\n",
      "took temporal_model.model.0.convolution_paths.1.0.norm.num_batches_tracked\n",
      "took temporal_model.model.0.convolution_paths.1.1.conv.weight\n",
      "took temporal_model.model.0.convolution_paths.1.1.norm.weight\n",
      "took temporal_model.model.0.convolution_paths.1.1.norm.bias\n",
      "took temporal_model.model.0.convolution_paths.1.1.norm.running_mean\n",
      "took temporal_model.model.0.convolution_paths.1.1.norm.running_var\n",
      "took temporal_model.model.0.convolution_paths.1.1.norm.num_batches_tracked\n",
      "took temporal_model.model.0.convolution_paths.2.conv.weight\n",
      "took temporal_model.model.0.convolution_paths.2.norm.weight\n",
      "took temporal_model.model.0.convolution_paths.2.norm.bias\n",
      "took temporal_model.model.0.convolution_paths.2.norm.running_mean\n",
      "took temporal_model.model.0.convolution_paths.2.norm.running_var\n",
      "took temporal_model.model.0.convolution_paths.2.norm.num_batches_tracked\n",
      "took temporal_model.model.0.pyramid_pooling.features.0.conv_bn_relu.conv.weight\n",
      "took temporal_model.model.0.pyramid_pooling.features.0.conv_bn_relu.norm.weight\n",
      "took temporal_model.model.0.pyramid_pooling.features.0.conv_bn_relu.norm.bias\n",
      "took temporal_model.model.0.pyramid_pooling.features.0.conv_bn_relu.norm.running_mean\n",
      "took temporal_model.model.0.pyramid_pooling.features.0.conv_bn_relu.norm.running_var\n",
      "took temporal_model.model.0.pyramid_pooling.features.0.conv_bn_relu.norm.num_batches_tracked\n",
      "took temporal_model.model.0.aggregation.0.conv.weight\n",
      "took temporal_model.model.0.aggregation.0.norm.weight\n",
      "took temporal_model.model.0.aggregation.0.norm.bias\n",
      "took temporal_model.model.0.aggregation.0.norm.running_mean\n",
      "took temporal_model.model.0.aggregation.0.norm.running_var\n",
      "took temporal_model.model.0.aggregation.0.norm.num_batches_tracked\n",
      "took temporal_model.model.0.projection.0.weight\n",
      "took temporal_model.model.0.projection.1.weight\n",
      "took temporal_model.model.0.projection.1.bias\n",
      "took temporal_model.model.0.projection.1.running_mean\n",
      "took temporal_model.model.0.projection.1.running_var\n",
      "took temporal_model.model.0.projection.1.num_batches_tracked\n",
      "took temporal_model.model.1.convolution_paths.0.0.conv.weight\n",
      "took temporal_model.model.1.convolution_paths.0.0.norm.weight\n",
      "took temporal_model.model.1.convolution_paths.0.0.norm.bias\n",
      "took temporal_model.model.1.convolution_paths.0.0.norm.running_mean\n",
      "took temporal_model.model.1.convolution_paths.0.0.norm.running_var\n",
      "took temporal_model.model.1.convolution_paths.0.0.norm.num_batches_tracked\n",
      "took temporal_model.model.1.convolution_paths.0.1.conv.weight\n",
      "took temporal_model.model.1.convolution_paths.0.1.norm.weight\n",
      "took temporal_model.model.1.convolution_paths.0.1.norm.bias\n",
      "took temporal_model.model.1.convolution_paths.0.1.norm.running_mean\n",
      "took temporal_model.model.1.convolution_paths.0.1.norm.running_var\n",
      "took temporal_model.model.1.convolution_paths.0.1.norm.num_batches_tracked\n",
      "took temporal_model.model.1.convolution_paths.1.0.conv.weight\n",
      "took temporal_model.model.1.convolution_paths.1.0.norm.weight\n",
      "took temporal_model.model.1.convolution_paths.1.0.norm.bias\n",
      "took temporal_model.model.1.convolution_paths.1.0.norm.running_mean\n",
      "took temporal_model.model.1.convolution_paths.1.0.norm.running_var\n",
      "took temporal_model.model.1.convolution_paths.1.0.norm.num_batches_tracked\n",
      "took temporal_model.model.1.convolution_paths.1.1.conv.weight\n",
      "took temporal_model.model.1.convolution_paths.1.1.norm.weight\n",
      "took temporal_model.model.1.convolution_paths.1.1.norm.bias\n",
      "took temporal_model.model.1.convolution_paths.1.1.norm.running_mean\n",
      "took temporal_model.model.1.convolution_paths.1.1.norm.running_var\n",
      "took temporal_model.model.1.convolution_paths.1.1.norm.num_batches_tracked\n",
      "took temporal_model.model.1.convolution_paths.2.conv.weight\n",
      "took temporal_model.model.1.convolution_paths.2.norm.weight\n",
      "took temporal_model.model.1.convolution_paths.2.norm.bias\n",
      "took temporal_model.model.1.convolution_paths.2.norm.running_mean\n",
      "took temporal_model.model.1.convolution_paths.2.norm.running_var\n",
      "took temporal_model.model.1.convolution_paths.2.norm.num_batches_tracked\n",
      "took temporal_model.model.1.pyramid_pooling.features.0.conv_bn_relu.conv.weight\n",
      "took temporal_model.model.1.pyramid_pooling.features.0.conv_bn_relu.norm.weight\n",
      "took temporal_model.model.1.pyramid_pooling.features.0.conv_bn_relu.norm.bias\n",
      "took temporal_model.model.1.pyramid_pooling.features.0.conv_bn_relu.norm.running_mean\n",
      "took temporal_model.model.1.pyramid_pooling.features.0.conv_bn_relu.norm.running_var\n",
      "took temporal_model.model.1.pyramid_pooling.features.0.conv_bn_relu.norm.num_batches_tracked\n",
      "took temporal_model.model.1.aggregation.0.conv.weight\n",
      "took temporal_model.model.1.aggregation.0.norm.weight\n",
      "took temporal_model.model.1.aggregation.0.norm.bias\n",
      "took temporal_model.model.1.aggregation.0.norm.running_mean\n",
      "took temporal_model.model.1.aggregation.0.norm.running_var\n",
      "took temporal_model.model.1.aggregation.0.norm.num_batches_tracked\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "relevant_weights = [\"transformer\",\"img_neck\", \"temporal_model\", \"img_backbone\"]\n",
    "\n",
    "new_weights = {}\n",
    "for key in weights[\"state_dict\"]:\n",
    "    if key.startswith(tuple(relevant_weights)):\n",
    "        print(\"took\", key)\n",
    "        new_weights[key] = weights[\"state_dict\"][key].clone()\n",
    "    else:\n",
    "        print(\"passed\",key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['img_backbone.patch_embed.projection.weight', 'img_backbone.patch_embed.projection.bias', 'img_backbone.patch_embed.norm.weight', 'img_backbone.patch_embed.norm.bias', 'img_backbone.stages.0.blocks.0.norm1.weight', 'img_backbone.stages.0.blocks.0.norm1.bias', 'img_backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table', 'img_backbone.stages.0.blocks.0.attn.w_msa.relative_position_index', 'img_backbone.stages.0.blocks.0.attn.w_msa.qkv.weight', 'img_backbone.stages.0.blocks.0.attn.w_msa.qkv.bias', 'img_backbone.stages.0.blocks.0.attn.w_msa.proj.weight', 'img_backbone.stages.0.blocks.0.attn.w_msa.proj.bias', 'img_backbone.stages.0.blocks.0.norm2.weight', 'img_backbone.stages.0.blocks.0.norm2.bias', 'img_backbone.stages.0.blocks.0.ffn.layers.0.0.weight', 'img_backbone.stages.0.blocks.0.ffn.layers.0.0.bias', 'img_backbone.stages.0.blocks.0.ffn.layers.1.weight', 'img_backbone.stages.0.blocks.0.ffn.layers.1.bias', 'img_backbone.stages.0.blocks.1.norm1.weight', 'img_backbone.stages.0.blocks.1.norm1.bias', 'img_backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table', 'img_backbone.stages.0.blocks.1.attn.w_msa.relative_position_index', 'img_backbone.stages.0.blocks.1.attn.w_msa.qkv.weight', 'img_backbone.stages.0.blocks.1.attn.w_msa.qkv.bias', 'img_backbone.stages.0.blocks.1.attn.w_msa.proj.weight', 'img_backbone.stages.0.blocks.1.attn.w_msa.proj.bias', 'img_backbone.stages.0.blocks.1.norm2.weight', 'img_backbone.stages.0.blocks.1.norm2.bias', 'img_backbone.stages.0.blocks.1.ffn.layers.0.0.weight', 'img_backbone.stages.0.blocks.1.ffn.layers.0.0.bias', 'img_backbone.stages.0.blocks.1.ffn.layers.1.weight', 'img_backbone.stages.0.blocks.1.ffn.layers.1.bias', 'img_backbone.stages.0.downsample.norm.weight', 'img_backbone.stages.0.downsample.norm.bias', 'img_backbone.stages.0.downsample.reduction.weight', 'img_backbone.stages.1.blocks.0.norm1.weight', 'img_backbone.stages.1.blocks.0.norm1.bias', 'img_backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table', 'img_backbone.stages.1.blocks.0.attn.w_msa.relative_position_index', 'img_backbone.stages.1.blocks.0.attn.w_msa.qkv.weight', 'img_backbone.stages.1.blocks.0.attn.w_msa.qkv.bias', 'img_backbone.stages.1.blocks.0.attn.w_msa.proj.weight', 'img_backbone.stages.1.blocks.0.attn.w_msa.proj.bias', 'img_backbone.stages.1.blocks.0.norm2.weight', 'img_backbone.stages.1.blocks.0.norm2.bias', 'img_backbone.stages.1.blocks.0.ffn.layers.0.0.weight', 'img_backbone.stages.1.blocks.0.ffn.layers.0.0.bias', 'img_backbone.stages.1.blocks.0.ffn.layers.1.weight', 'img_backbone.stages.1.blocks.0.ffn.layers.1.bias', 'img_backbone.stages.1.blocks.1.norm1.weight', 'img_backbone.stages.1.blocks.1.norm1.bias', 'img_backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table', 'img_backbone.stages.1.blocks.1.attn.w_msa.relative_position_index', 'img_backbone.stages.1.blocks.1.attn.w_msa.qkv.weight', 'img_backbone.stages.1.blocks.1.attn.w_msa.qkv.bias', 'img_backbone.stages.1.blocks.1.attn.w_msa.proj.weight', 'img_backbone.stages.1.blocks.1.attn.w_msa.proj.bias', 'img_backbone.stages.1.blocks.1.norm2.weight', 'img_backbone.stages.1.blocks.1.norm2.bias', 'img_backbone.stages.1.blocks.1.ffn.layers.0.0.weight', 'img_backbone.stages.1.blocks.1.ffn.layers.0.0.bias', 'img_backbone.stages.1.blocks.1.ffn.layers.1.weight', 'img_backbone.stages.1.blocks.1.ffn.layers.1.bias', 'img_backbone.stages.1.downsample.norm.weight', 'img_backbone.stages.1.downsample.norm.bias', 'img_backbone.stages.1.downsample.reduction.weight', 'img_backbone.stages.2.blocks.0.norm1.weight', 'img_backbone.stages.2.blocks.0.norm1.bias', 'img_backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table', 'img_backbone.stages.2.blocks.0.attn.w_msa.relative_position_index', 'img_backbone.stages.2.blocks.0.attn.w_msa.qkv.weight', 'img_backbone.stages.2.blocks.0.attn.w_msa.qkv.bias', 'img_backbone.stages.2.blocks.0.attn.w_msa.proj.weight', 'img_backbone.stages.2.blocks.0.attn.w_msa.proj.bias', 'img_backbone.stages.2.blocks.0.norm2.weight', 'img_backbone.stages.2.blocks.0.norm2.bias', 'img_backbone.stages.2.blocks.0.ffn.layers.0.0.weight', 'img_backbone.stages.2.blocks.0.ffn.layers.0.0.bias', 'img_backbone.stages.2.blocks.0.ffn.layers.1.weight', 'img_backbone.stages.2.blocks.0.ffn.layers.1.bias', 'img_backbone.stages.2.blocks.1.norm1.weight', 'img_backbone.stages.2.blocks.1.norm1.bias', 'img_backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table', 'img_backbone.stages.2.blocks.1.attn.w_msa.relative_position_index', 'img_backbone.stages.2.blocks.1.attn.w_msa.qkv.weight', 'img_backbone.stages.2.blocks.1.attn.w_msa.qkv.bias', 'img_backbone.stages.2.blocks.1.attn.w_msa.proj.weight', 'img_backbone.stages.2.blocks.1.attn.w_msa.proj.bias', 'img_backbone.stages.2.blocks.1.norm2.weight', 'img_backbone.stages.2.blocks.1.norm2.bias', 'img_backbone.stages.2.blocks.1.ffn.layers.0.0.weight', 'img_backbone.stages.2.blocks.1.ffn.layers.0.0.bias', 'img_backbone.stages.2.blocks.1.ffn.layers.1.weight', 'img_backbone.stages.2.blocks.1.ffn.layers.1.bias', 'img_backbone.stages.2.blocks.2.norm1.weight', 'img_backbone.stages.2.blocks.2.norm1.bias', 'img_backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table', 'img_backbone.stages.2.blocks.2.attn.w_msa.relative_position_index', 'img_backbone.stages.2.blocks.2.attn.w_msa.qkv.weight', 'img_backbone.stages.2.blocks.2.attn.w_msa.qkv.bias', 'img_backbone.stages.2.blocks.2.attn.w_msa.proj.weight', 'img_backbone.stages.2.blocks.2.attn.w_msa.proj.bias', 'img_backbone.stages.2.blocks.2.norm2.weight', 'img_backbone.stages.2.blocks.2.norm2.bias', 'img_backbone.stages.2.blocks.2.ffn.layers.0.0.weight', 'img_backbone.stages.2.blocks.2.ffn.layers.0.0.bias', 'img_backbone.stages.2.blocks.2.ffn.layers.1.weight', 'img_backbone.stages.2.blocks.2.ffn.layers.1.bias', 'img_backbone.stages.2.blocks.3.norm1.weight', 'img_backbone.stages.2.blocks.3.norm1.bias', 'img_backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table', 'img_backbone.stages.2.blocks.3.attn.w_msa.relative_position_index', 'img_backbone.stages.2.blocks.3.attn.w_msa.qkv.weight', 'img_backbone.stages.2.blocks.3.attn.w_msa.qkv.bias', 'img_backbone.stages.2.blocks.3.attn.w_msa.proj.weight', 'img_backbone.stages.2.blocks.3.attn.w_msa.proj.bias', 'img_backbone.stages.2.blocks.3.norm2.weight', 'img_backbone.stages.2.blocks.3.norm2.bias', 'img_backbone.stages.2.blocks.3.ffn.layers.0.0.weight', 'img_backbone.stages.2.blocks.3.ffn.layers.0.0.bias', 'img_backbone.stages.2.blocks.3.ffn.layers.1.weight', 'img_backbone.stages.2.blocks.3.ffn.layers.1.bias', 'img_backbone.stages.2.blocks.4.norm1.weight', 'img_backbone.stages.2.blocks.4.norm1.bias', 'img_backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table', 'img_backbone.stages.2.blocks.4.attn.w_msa.relative_position_index', 'img_backbone.stages.2.blocks.4.attn.w_msa.qkv.weight', 'img_backbone.stages.2.blocks.4.attn.w_msa.qkv.bias', 'img_backbone.stages.2.blocks.4.attn.w_msa.proj.weight', 'img_backbone.stages.2.blocks.4.attn.w_msa.proj.bias', 'img_backbone.stages.2.blocks.4.norm2.weight', 'img_backbone.stages.2.blocks.4.norm2.bias', 'img_backbone.stages.2.blocks.4.ffn.layers.0.0.weight', 'img_backbone.stages.2.blocks.4.ffn.layers.0.0.bias', 'img_backbone.stages.2.blocks.4.ffn.layers.1.weight', 'img_backbone.stages.2.blocks.4.ffn.layers.1.bias', 'img_backbone.stages.2.blocks.5.norm1.weight', 'img_backbone.stages.2.blocks.5.norm1.bias', 'img_backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table', 'img_backbone.stages.2.blocks.5.attn.w_msa.relative_position_index', 'img_backbone.stages.2.blocks.5.attn.w_msa.qkv.weight', 'img_backbone.stages.2.blocks.5.attn.w_msa.qkv.bias', 'img_backbone.stages.2.blocks.5.attn.w_msa.proj.weight', 'img_backbone.stages.2.blocks.5.attn.w_msa.proj.bias', 'img_backbone.stages.2.blocks.5.norm2.weight', 'img_backbone.stages.2.blocks.5.norm2.bias', 'img_backbone.stages.2.blocks.5.ffn.layers.0.0.weight', 'img_backbone.stages.2.blocks.5.ffn.layers.0.0.bias', 'img_backbone.stages.2.blocks.5.ffn.layers.1.weight', 'img_backbone.stages.2.blocks.5.ffn.layers.1.bias', 'img_backbone.stages.2.downsample.norm.weight', 'img_backbone.stages.2.downsample.norm.bias', 'img_backbone.stages.2.downsample.reduction.weight', 'img_backbone.stages.3.blocks.0.norm1.weight', 'img_backbone.stages.3.blocks.0.norm1.bias', 'img_backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table', 'img_backbone.stages.3.blocks.0.attn.w_msa.relative_position_index', 'img_backbone.stages.3.blocks.0.attn.w_msa.qkv.weight', 'img_backbone.stages.3.blocks.0.attn.w_msa.qkv.bias', 'img_backbone.stages.3.blocks.0.attn.w_msa.proj.weight', 'img_backbone.stages.3.blocks.0.attn.w_msa.proj.bias', 'img_backbone.stages.3.blocks.0.norm2.weight', 'img_backbone.stages.3.blocks.0.norm2.bias', 'img_backbone.stages.3.blocks.0.ffn.layers.0.0.weight', 'img_backbone.stages.3.blocks.0.ffn.layers.0.0.bias', 'img_backbone.stages.3.blocks.0.ffn.layers.1.weight', 'img_backbone.stages.3.blocks.0.ffn.layers.1.bias', 'img_backbone.stages.3.blocks.1.norm1.weight', 'img_backbone.stages.3.blocks.1.norm1.bias', 'img_backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table', 'img_backbone.stages.3.blocks.1.attn.w_msa.relative_position_index', 'img_backbone.stages.3.blocks.1.attn.w_msa.qkv.weight', 'img_backbone.stages.3.blocks.1.attn.w_msa.qkv.bias', 'img_backbone.stages.3.blocks.1.attn.w_msa.proj.weight', 'img_backbone.stages.3.blocks.1.attn.w_msa.proj.bias', 'img_backbone.stages.3.blocks.1.norm2.weight', 'img_backbone.stages.3.blocks.1.norm2.bias', 'img_backbone.stages.3.blocks.1.ffn.layers.0.0.weight', 'img_backbone.stages.3.blocks.1.ffn.layers.0.0.bias', 'img_backbone.stages.3.blocks.1.ffn.layers.1.weight', 'img_backbone.stages.3.blocks.1.ffn.layers.1.bias', 'img_backbone.norm2.weight', 'img_backbone.norm2.bias', 'img_backbone.norm3.weight', 'img_backbone.norm3.bias', 'img_neck.up.conv.0.weight', 'img_neck.up.conv.1.weight', 'img_neck.up.conv.1.bias', 'img_neck.up.conv.1.running_mean', 'img_neck.up.conv.1.running_var', 'img_neck.up.conv.1.num_batches_tracked', 'img_neck.up.conv.3.weight', 'img_neck.up.conv.4.weight', 'img_neck.up.conv.4.bias', 'img_neck.up.conv.4.running_mean', 'img_neck.up.conv.4.running_var', 'img_neck.up.conv.4.num_batches_tracked', 'transformer.frustum', 'transformer.depthnet.weight', 'transformer.depthnet.bias', 'temporal_model.model.0.convolution_paths.0.0.conv.weight', 'temporal_model.model.0.convolution_paths.0.0.norm.weight', 'temporal_model.model.0.convolution_paths.0.0.norm.bias', 'temporal_model.model.0.convolution_paths.0.0.norm.running_mean', 'temporal_model.model.0.convolution_paths.0.0.norm.running_var', 'temporal_model.model.0.convolution_paths.0.0.norm.num_batches_tracked', 'temporal_model.model.0.convolution_paths.0.1.conv.weight', 'temporal_model.model.0.convolution_paths.0.1.norm.weight', 'temporal_model.model.0.convolution_paths.0.1.norm.bias', 'temporal_model.model.0.convolution_paths.0.1.norm.running_mean', 'temporal_model.model.0.convolution_paths.0.1.norm.running_var', 'temporal_model.model.0.convolution_paths.0.1.norm.num_batches_tracked', 'temporal_model.model.0.convolution_paths.1.0.conv.weight', 'temporal_model.model.0.convolution_paths.1.0.norm.weight', 'temporal_model.model.0.convolution_paths.1.0.norm.bias', 'temporal_model.model.0.convolution_paths.1.0.norm.running_mean', 'temporal_model.model.0.convolution_paths.1.0.norm.running_var', 'temporal_model.model.0.convolution_paths.1.0.norm.num_batches_tracked', 'temporal_model.model.0.convolution_paths.1.1.conv.weight', 'temporal_model.model.0.convolution_paths.1.1.norm.weight', 'temporal_model.model.0.convolution_paths.1.1.norm.bias', 'temporal_model.model.0.convolution_paths.1.1.norm.running_mean', 'temporal_model.model.0.convolution_paths.1.1.norm.running_var', 'temporal_model.model.0.convolution_paths.1.1.norm.num_batches_tracked', 'temporal_model.model.0.convolution_paths.2.conv.weight', 'temporal_model.model.0.convolution_paths.2.norm.weight', 'temporal_model.model.0.convolution_paths.2.norm.bias', 'temporal_model.model.0.convolution_paths.2.norm.running_mean', 'temporal_model.model.0.convolution_paths.2.norm.running_var', 'temporal_model.model.0.convolution_paths.2.norm.num_batches_tracked', 'temporal_model.model.0.pyramid_pooling.features.0.conv_bn_relu.conv.weight', 'temporal_model.model.0.pyramid_pooling.features.0.conv_bn_relu.norm.weight', 'temporal_model.model.0.pyramid_pooling.features.0.conv_bn_relu.norm.bias', 'temporal_model.model.0.pyramid_pooling.features.0.conv_bn_relu.norm.running_mean', 'temporal_model.model.0.pyramid_pooling.features.0.conv_bn_relu.norm.running_var', 'temporal_model.model.0.pyramid_pooling.features.0.conv_bn_relu.norm.num_batches_tracked', 'temporal_model.model.0.aggregation.0.conv.weight', 'temporal_model.model.0.aggregation.0.norm.weight', 'temporal_model.model.0.aggregation.0.norm.bias', 'temporal_model.model.0.aggregation.0.norm.running_mean', 'temporal_model.model.0.aggregation.0.norm.running_var', 'temporal_model.model.0.aggregation.0.norm.num_batches_tracked', 'temporal_model.model.0.projection.0.weight', 'temporal_model.model.0.projection.1.weight', 'temporal_model.model.0.projection.1.bias', 'temporal_model.model.0.projection.1.running_mean', 'temporal_model.model.0.projection.1.running_var', 'temporal_model.model.0.projection.1.num_batches_tracked', 'temporal_model.model.1.convolution_paths.0.0.conv.weight', 'temporal_model.model.1.convolution_paths.0.0.norm.weight', 'temporal_model.model.1.convolution_paths.0.0.norm.bias', 'temporal_model.model.1.convolution_paths.0.0.norm.running_mean', 'temporal_model.model.1.convolution_paths.0.0.norm.running_var', 'temporal_model.model.1.convolution_paths.0.0.norm.num_batches_tracked', 'temporal_model.model.1.convolution_paths.0.1.conv.weight', 'temporal_model.model.1.convolution_paths.0.1.norm.weight', 'temporal_model.model.1.convolution_paths.0.1.norm.bias', 'temporal_model.model.1.convolution_paths.0.1.norm.running_mean', 'temporal_model.model.1.convolution_paths.0.1.norm.running_var', 'temporal_model.model.1.convolution_paths.0.1.norm.num_batches_tracked', 'temporal_model.model.1.convolution_paths.1.0.conv.weight', 'temporal_model.model.1.convolution_paths.1.0.norm.weight', 'temporal_model.model.1.convolution_paths.1.0.norm.bias', 'temporal_model.model.1.convolution_paths.1.0.norm.running_mean', 'temporal_model.model.1.convolution_paths.1.0.norm.running_var', 'temporal_model.model.1.convolution_paths.1.0.norm.num_batches_tracked', 'temporal_model.model.1.convolution_paths.1.1.conv.weight', 'temporal_model.model.1.convolution_paths.1.1.norm.weight', 'temporal_model.model.1.convolution_paths.1.1.norm.bias', 'temporal_model.model.1.convolution_paths.1.1.norm.running_mean', 'temporal_model.model.1.convolution_paths.1.1.norm.running_var', 'temporal_model.model.1.convolution_paths.1.1.norm.num_batches_tracked', 'temporal_model.model.1.convolution_paths.2.conv.weight', 'temporal_model.model.1.convolution_paths.2.norm.weight', 'temporal_model.model.1.convolution_paths.2.norm.bias', 'temporal_model.model.1.convolution_paths.2.norm.running_mean', 'temporal_model.model.1.convolution_paths.2.norm.running_var', 'temporal_model.model.1.convolution_paths.2.norm.num_batches_tracked', 'temporal_model.model.1.pyramid_pooling.features.0.conv_bn_relu.conv.weight', 'temporal_model.model.1.pyramid_pooling.features.0.conv_bn_relu.norm.weight', 'temporal_model.model.1.pyramid_pooling.features.0.conv_bn_relu.norm.bias', 'temporal_model.model.1.pyramid_pooling.features.0.conv_bn_relu.norm.running_mean', 'temporal_model.model.1.pyramid_pooling.features.0.conv_bn_relu.norm.running_var', 'temporal_model.model.1.pyramid_pooling.features.0.conv_bn_relu.norm.num_batches_tracked', 'temporal_model.model.1.aggregation.0.conv.weight', 'temporal_model.model.1.aggregation.0.norm.weight', 'temporal_model.model.1.aggregation.0.norm.bias', 'temporal_model.model.1.aggregation.0.norm.running_mean', 'temporal_model.model.1.aggregation.0.norm.running_var', 'temporal_model.model.1.aggregation.0.norm.num_batches_tracked'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_weights.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_tiny = OrderedDict(new_weights)\n",
    "torch.save(weights, \"weights/clean_weights_tiny.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "\n",
    "cfg = Config.fromfile(\n",
    "    r\"/home/niklas/ETM_BEV/BEVerse/projects/configs/beverse_tiny_org.py\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_lims = [\n",
    "    (0.3, 0.45),  # fiery\n",
    "    (0.38, 0.55),  # desTINY\n",
    "    (0.82, 0.99),  # small\n",
    "    (1, 1),  # BEVDEt\n",
    "]\n",
    "\n",
    "final_dims = [(224, 480), (256, 704), (512, 1408), (900, 1600)]\n",
    "\n",
    "backbones = [\n",
    "    \"beverse_tiny.py\",\n",
    "    \"beverse_tiny.py\",\n",
    "    \"beverse_small.py\",\n",
    "    \"beverse_small.py\",\n",
    "]\n",
    "\n",
    "# future frames -> tiny settings\n",
    "future_frames_list = [4, 4, 4, 4, 5, 7, 10]\n",
    "receptive_field_list = [\n",
    "    3,\n",
    "    5,\n",
    "    8,\n",
    "    13,\n",
    "    4,\n",
    "    6,\n",
    "    9,\n",
    "]\n",
    "\n",
    "# grid_size = (\n",
    "#     point_cloud_range[3:] -  # type: ignore\n",
    "#     point_cloud_range[:3]) / voxel_size  # type: ignore\n",
    "\n",
    "point_cloud_range_base = [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]\n",
    "point_cloud_range_extended_fustrum = [-71.2, -71.2, -5.0, 71.2, 71.2, 3.0]\n",
    "det_grid_confs = {\n",
    "    \"xbound\": [\n",
    "        [-51.2, 51.2, 0.8],  # lower_bound, upper_bound, interval\n",
    "        [-51.2, 51.2, 0.4],\n",
    "        [-51.2, 51.2, 0.2],\n",
    "        [-51.2, 51.2, 0.1],\n",
    "        [-26.2, 26.2, 0.8],\n",
    "        [-26.2, 26.2, 0.4],\n",
    "    ],\n",
    "    \"ybound\": [\n",
    "        [-51.2, 51.2, 0.8],\n",
    "        [-51.2, 51.2, 0.4],\n",
    "        [-51.2, 51.2, 0.2],\n",
    "        [-51.2, 51.2, 0.1],\n",
    "        [-71.2, 71.2, 0.8],\n",
    "        [-71.2, 71.2, 0.4],\n",
    "    ],\n",
    "    \"zbound\": [-10.0, 10.0, 20.0],\n",
    "    \"dbound\": [\n",
    "        [1.0, 60.0, 1.0],\n",
    "        [1.0, 60.0, 1.0],\n",
    "        [1.0, 60.0, 0.5],\n",
    "        [1.0, 60.0, 0.5],\n",
    "        [1.0, 70.0, 1.0],\n",
    "        [1.0, 70.0, 5.0],\n",
    "    ],  # [(lower_bound, upper_bound, interval).]\n",
    "}\n",
    "\n",
    "motion_grid_confs = {\n",
    "    \"xbound\": [\n",
    "        [-50.0, 50.0, 0.5],\n",
    "        [-50.0, 50.0, 0.25],\n",
    "        [-50.0, 50.0, 0.125],\n",
    "        [-50.0, 50.0, 0.075],\n",
    "        [-25.0, 25.0, 0.5],\n",
    "        [-25.0, 25.0, 0.25],\n",
    "    ],\n",
    "    \"ybound\": [\n",
    "        [-50.0, 50.0, 0.5],\n",
    "        [-50.0, 50.0, 0.25],\n",
    "        [-50.0, 50.0, 0.125],\n",
    "        [-50.0, 50.0, 0.075],\n",
    "        [-70.0, 70.0, 0.5],\n",
    "        [-70.0, 70.0, 0.25],\n",
    "    ],\n",
    "    \"zbound\": [-10.0, 10.0, 20.0],\n",
    "    \"dbound\": [\n",
    "        [1.0, 60.0, 1.0],\n",
    "        [1.0, 60.0, 1.0],\n",
    "        [1.0, 60.0, 0.5],\n",
    "        [1.0, 60.0, 0.5],\n",
    "        [1.0, 70.0, 1.0],\n",
    "        [1.0, 70.0, 5.0],\n",
    "    ],\n",
    "}\n",
    "\n",
    "map_grid_confs = {\n",
    "    \"xbound\": [-30.0, 30.0, 0.15],\n",
    "    \"ybound\": [-15.0, 15.0, 0.15],\n",
    "    \"zbound\": [-10.0, 10.0, 20.0],\n",
    "    \"dbound\": [\n",
    "        [1.0, 60.0, 1.0],\n",
    "        [1.0, 60.0, 1.0],\n",
    "        [1.0, 60.0, 0.5],\n",
    "        [1.0, 60.0, 0.5],\n",
    "        [1.0, 70.0, 1.0],\n",
    "        [1.0, 70.0, 5.0],\n",
    "    ],\n",
    "}\n",
    "det_grid_conf = {\n",
    "    \"xbound\": [-51.2, 51.2, 0.8],\n",
    "    \"ybound\": [-51.2, 51.2, 0.8],\n",
    "    \"zbound\": [-10.0, 10.0, 20.0],\n",
    "    \"dbound\": [1.0, 60.0, 1.0],\n",
    "}\n",
    "\n",
    "motion_grid_conf = {\n",
    "    \"xbound\": [-50.0, 50.0, 0.5],\n",
    "    \"ybound\": [-50.0, 50.0, 0.5],\n",
    "    \"zbound\": [-10.0, 10.0, 20.0],\n",
    "    \"dbound\": [1.0, 60.0, 1.0],\n",
    "}\n",
    "\n",
    "map_grid_conf = {\n",
    "    \"xbound\": [-30.0, 30.0, 0.15],\n",
    "    \"ybound\": [-15.0, 15.0, 0.15],\n",
    "    \"zbound\": [-10.0, 10.0, 20.0],\n",
    "    \"dbound\": [1.0, 60.0, 1.0],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# det_grid_conf[\"xbound\"] = det_grid_confs[\"xbound\"][1]\n",
    "# det_grid_conf[\"ybound\"] = det_grid_confs[\"ybound\"][1]\n",
    "# det_grid_conf[\"zbound\"] = det_grid_confs[\"zbound\"]\n",
    "# det_grid_conf[\"dbound\"] = det_grid_confs[\"dbound\"][1]\n",
    "\n",
    "# motion_grid_conf[\"xbound\"] = motion_grid_confs[\"xbound\"][1]\n",
    "# motion_grid_conf[\"ybound\"] = motion_grid_confs[\"ybound\"][1]\n",
    "# motion_grid_conf[\"zbound\"] = motion_grid_confs[\"zbound\"]\n",
    "# motion_grid_conf[\"dbound\"] = motion_grid_confs[\"dbound\"][1]\n",
    "\n",
    "# map_grid_conf[\"xbound\"] = map_grid_confs[\"xbound\"]\n",
    "# map_grid_conf[\"ybound\"] = map_grid_confs[\"ybound\"]\n",
    "# map_grid_conf[\"zbound\"] = map_grid_confs[\"zbound\"]\n",
    "# map_grid_conf[\"dbound\"] = map_grid_confs[\"dbound\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = build_dataset(cfg.data.test)\n",
    "data_loader = build_dataloader(\n",
    "    dataset,\n",
    "    samples_per_gpu=2,\n",
    "    workers_per_gpu=cfg.data.workers_per_gpu,\n",
    "    dist=False,\n",
    "    shuffle=False,\n",
    ")\n",
    "iter_loader = iter(data_loader)\n",
    "sample = next(iter_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cfg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# device = torch.device(\"cuda:0\")\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# cfg = Config.fromfile(\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m#     r\"/home/niklas/ETM_BEV/BEVerse/projects/configs/beverse_tiny.py\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39m#     cfg.merge_from_dict(args.cfg_options)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m# import modules from string list.\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[39mif\u001b[39;00m cfg\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mcustom_imports\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     10\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mmmcv\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m import_modules_from_strings\n\u001b[1;32m     12\u001b[0m     import_modules_from_strings(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcfg[\u001b[39m\"\u001b[39m\u001b[39mcustom_imports\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cfg' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# device = torch.device(\"cuda:0\")\n",
    "# cfg = Config.fromfile(\n",
    "#     r\"/home/niklas/ETM_BEV/BEVerse/projects/configs/beverse_tiny.py\"\n",
    "# )\n",
    "\n",
    "# if args.cfg_options is not None:\n",
    "#     cfg.merge_from_dict(args.cfg_options)\n",
    "# import modules from string list.\n",
    "if cfg.get(\"custom_imports\", None):\n",
    "    from mmcv.utils import import_modules_from_strings\n",
    "\n",
    "    import_modules_from_strings(**cfg[\"custom_imports\"])\n",
    "\n",
    "# import modules from plguin/xx, registry will be updated\n",
    "if hasattr(cfg, \"plugin\"):\n",
    "    if cfg.plugin:\n",
    "        import importlib\n",
    "\n",
    "        if hasattr(cfg, \"plugin_dir\"):\n",
    "            plugin_dir = cfg.plugin_dir\n",
    "            _module_dir = os.path.dirname(plugin_dir)\n",
    "            _module_dir = _module_dir.split(\"/\")\n",
    "            _module_path = _module_dir[0]\n",
    "\n",
    "            for m in _module_dir[1:]:\n",
    "                _module_path = _module_path + \".\" + m\n",
    "            print(_module_path)\n",
    "            plg_lib = importlib.import_module(_module_path)\n",
    "        else:\n",
    "            # import dir is the dirpath for the config file\n",
    "            _module_dir = os.path.dirname(\n",
    "                r\"/home/niklas/ETM_BEV/BEVerse/projects/configs/beverse_tiny_exp.py\"\n",
    "            )\n",
    "            _module_dir = _module_dir.split(\"/\")\n",
    "            _module_path = _module_dir[0]\n",
    "            for m in _module_dir[1:]:\n",
    "                _module_path = _module_path + \".\" + m\n",
    "            print(_module_path)\n",
    "            plg_lib = importlib.import_module(_module_path)\n",
    "            \n",
    "\n",
    "samples_per_gpu = 1\n",
    "if isinstance(cfg.data.test, dict):\n",
    "    cfg.data.test.test_mode = True\n",
    "    samples_per_gpu = cfg.data.test.pop(\"samples_per_gpu\", 1)\n",
    "    if samples_per_gpu > 1:\n",
    "        # Replace 'ImageToTensor' to 'DefaultFormatBundle'\n",
    "        cfg.data.test.pipeline = replace_ImageToTensor(cfg.data.test.pipeline)\n",
    "elif isinstance(cfg.data.test, list):\n",
    "    for ds_cfg in cfg.data.test:\n",
    "        ds_cfg.test_mode = True\n",
    "    samples_per_gpu = max(\n",
    "        [ds_cfg.pop(\"samples_per_gpu\", 1) for ds_cfg in cfg.data.test]\n",
    "    )\n",
    "    if samples_per_gpu > 1:\n",
    "        for ds_cfg in cfg.data.test:\n",
    "            ds_cfg.pipeline = replace_ImageToTensor(ds_cfg.pipeline)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def measure_time_host(\n",
    "    model: nn.Module,\n",
    "    input_tensor: torch.Tensor,\n",
    "    num_repeats: int = 100,\n",
    "    num_warmups: int = 10,\n",
    "    synchronize: bool = True,\n",
    "    continuous_measure: bool = True,\n",
    ") -> float:\n",
    "\n",
    "    for _ in range(num_warmups):\n",
    "        _ = model.forward(input_tensor)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    elapsed_time_ms = 0\n",
    "\n",
    "    if continuous_measure:\n",
    "        start = timer()\n",
    "        for _ in range(num_repeats):\n",
    "            _ = model.forward(input_tensor)\n",
    "        if synchronize:\n",
    "            torch.cuda.synchronize()\n",
    "        end = timer()\n",
    "        elapsed_time_ms = (end - start) * 1000\n",
    "\n",
    "    else:\n",
    "        for _ in range(num_repeats):\n",
    "            start = timer()\n",
    "            _ = model.forward(input_tensor)\n",
    "            if synchronize:\n",
    "                torch.cuda.synchronize()\n",
    "            end = timer()\n",
    "            elapsed_time_ms += (end - start) * 1000\n",
    "\n",
    "    return elapsed_time_ms / num_repeats\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def measure_time_device(\n",
    "    model: nn.Module,\n",
    "    input_tensor: torch.Tensor,\n",
    "    num_repeats: int = 100,\n",
    "    num_warmups: int = 10,\n",
    "    synchronize: bool = True,\n",
    "    continuous_measure: bool = True,\n",
    ") -> float:\n",
    "\n",
    "    for _ in range(num_warmups):\n",
    "        _ = model.forward(input_tensor)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    elapsed_time_ms = 0\n",
    "\n",
    "    if continuous_measure:\n",
    "        start_event = torch.cuda.Event(enable_timing=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True)\n",
    "        start_event.record()\n",
    "        for _ in range(num_repeats):\n",
    "            _ = model.forward(input_tensor)\n",
    "        end_event.record()\n",
    "        if synchronize:\n",
    "            # This has to be synchronized to compute the elapsed time.\n",
    "            # Otherwise, there will be runtime error.\n",
    "            torch.cuda.synchronize()\n",
    "        elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "\n",
    "    else:\n",
    "        for _ in range(num_repeats):\n",
    "            start_event = torch.cuda.Event(enable_timing=True)\n",
    "            end_event = torch.cuda.Event(enable_timing=True)\n",
    "            start_event.record()\n",
    "            _ = model.forward(input_tensor)\n",
    "            end_event.record()\n",
    "            if synchronize:\n",
    "                # This has to be synchronized to compute the elapsed time.\n",
    "                # Otherwise, there will be runtime error.\n",
    "                torch.cuda.synchronize()\n",
    "            elapsed_time_ms += start_event.elapsed_time(end_event)\n",
    "\n",
    "    return elapsed_time_ms / num_repeats\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_inference(model: nn.Module, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "    return model.forward(input_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_grid_conf = {\n",
    "    \"xbound\": [-62.0, 62.0, 0.8],\n",
    "    \"ybound\": [-36.2, 36.2, 0.8],\n",
    "    \"zbound\": [-10.0, 10.0, 20.0],\n",
    "    \"dbound\": [1.0, 70.0, 1.0],\n",
    "}\n",
    "\n",
    "motion_grid_conf = {\n",
    "    \"xbound\": [-60.0, 60.0, 0.5],\n",
    "    \"ybound\": [-36.0, 36.0, 0.5],\n",
    "    \"zbound\": [-10.0, 10.0, 20.0],\n",
    "    \"dbound\": [1.0, 70.0, 1.0],\n",
    "}\n",
    "\n",
    "map_grid_conf = {\n",
    "    \"xbound\": [-30.0, 30.0, 0.15],\n",
    "    \"ybound\": [-15.0, 15.0, 0.15],\n",
    "    \"zbound\": [-10.0, 10.0, 20.0],\n",
    "    \"dbound\": [1.0, 70.0, 1.0],\n",
    "}\n",
    "\n",
    "----------\n",
    "det_grid_conf = {\n",
    "    \"xbound\": [-51.2, 51.2, 0.8],\n",
    "    \"ybound\": [-51.2, 51.2, 0.8],\n",
    "    \"zbound\": [-10.0, 10.0, 20.0],\n",
    "    \"dbound\": [1.0, 60.0, 1.0],\n",
    "}\n",
    "\n",
    "motion_grid_conf = {\n",
    "    \"xbound\": [-50.0, 50.0, 0.5],\n",
    "    \"ybound\": [-50.0, 50.0, 0.5],\n",
    "    \"zbound\": [-10.0, 10.0, 20.0],\n",
    "    \"dbound\": [1.0, 60.0, 1.0],\n",
    "}\n",
    "\n",
    "map_grid_conf = {\n",
    "    \"xbound\": [-30.0, 30.0, 0.15],\n",
    "    \"ybound\": [-15.0, 15.0, 0.15],\n",
    "    \"zbound\": [-10.0, 10.0, 20.0],\n",
    "    \"dbound\": [1.0, 60.0, 1.0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([240, 144,   1])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x_bounds, y_bounds,z_bounds=cfg[\"det_grid_conf\"][\"xbound\"],cfg[\"det_grid_conf\"][\"ybound\"],cfg[\"det_grid_conf\"][\"zbound\"]\n",
    "\n",
    "bev_dimension = torch.tensor(\n",
    "        [(row[1] - row[0]) / row[2] for row in [x_bounds, y_bounds, z_bounds]],\n",
    "        dtype=torch.long,\n",
    "    )\n",
    "bev_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240.7766990291262\n",
      "144.8\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "for row in [x_bounds, y_bounds, z_bounds]:\n",
    "    print((row[1] - row[0]) / row[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# det_grid_conf = {\n",
    "#     \"xbound\": [-62.0, 62.0, 0.37],#37\n",
    "#     \"ybound\": [-36.2, 36.2, 0.375],#37,5\n",
    "#     \"zbound\": [-10.0, 10.0, 20.0],\n",
    "#     \"dbound\": [1.0, 70.0, 1.0],\n",
    "# }\n",
    "\n",
    "# motion_grid_conf = {\n",
    "#     \"xbound\": [-60.0, 60.0, 0.25],\n",
    "#     \"ybound\": [-36.0, 36.0, 0.25],\n",
    "#     \"zbound\": [-10.0, 10.0, 20.0],\n",
    "#     \"dbound\": [1.0, 70.0, 1.0],\n",
    "# }\n",
    "\n",
    "# map_grid_conf = {\n",
    "#     \"xbound\": [-30.0, 30.0, 0.15],\n",
    "#     \"ybound\": [-15.0, 15.0, 0.15],\n",
    "#     \"zbound\": [-10.0, 10.0, 20.0],\n",
    "#     \"dbound\": [1.0, 70.0, 1.0],\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_grid_conf = {\n",
    "    \"xbound\": [-51.2, 51.2, 0.2],\n",
    "    \"ybound\": [-51.2, 51.2, 0.2],\n",
    "    \"zbound\": [-10.0, 10.0, 20.0],\n",
    "    \"dbound\": [1.0, 60.0, .50],\n",
    "}\n",
    "\n",
    "motion_grid_conf = {\n",
    "    \"xbound\": [-50.0, 50.0, 0.125],\n",
    "    \"ybound\": [-50.0, 50.0, 0.125],\n",
    "    \"zbound\": [-10.0, 10.0, 20.0],\n",
    "    \"dbound\": [1.0, 60.0, .50],\n",
    "}\n",
    "\n",
    "map_grid_conf = {\n",
    "    \"xbound\": [-30.0, 30.0, 0.15],\n",
    "    \"ybound\": [-15.0, 15.0, 0.15],\n",
    "    \"zbound\": [-10.0, 10.0, 20.0],\n",
    "    \"dbound\": [1.0, 60.0, .50],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-25.6, 25.6, 0.1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i/2.0 for i in  [-51.2, 51.2, 0.2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects.mmdet3d_plugin\n"
     ]
    }
   ],
   "source": [
    "\n",
    "det_grid_conf = {\n",
    "    \"xbound\": [-50.0, 50.0, 0.5],\n",
    "    \"ybound\": [-50.0, 50.0, 0.5],\n",
    "    \"zbound\": [-10.0, 10.0, 20.0],\n",
    "    \"dbound\": [1.0, 60.0, 1.0],\n",
    "}\n",
    "\n",
    "motion_grid_conf = {\n",
    "    \"xbound\": [-50.0, 50.0, 0.5],\n",
    "    \"ybound\": [-50.0, 50.0, 0.5],\n",
    "    \"zbound\": [-10.0, 10.0, 20.0],\n",
    "    \"dbound\": [1.0, 60.0, .50],\n",
    "}\n",
    "\n",
    "map_grid_conf = {\n",
    "    \"xbound\": [-50.0, 50.0, 0.5],\n",
    "    \"ybound\": [-50.0, 50.0, 0.5],\n",
    "    \"zbound\": [-10.0, 10.0, 20.0],\n",
    "    \"dbound\": [1.0, 60.0, 1.0],\n",
    "}\n",
    "\n",
    "point_cloud_range_base = [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]\n",
    "point_cloud_range_extended_fustrum = [-62.0, -62.0, -5.0, 62.0, 62.0, 3.0]\n",
    "#motion_detr_tiny.py beverse_tiny_org.py\n",
    "\n",
    "cfg = import_modules_load_config(cfg_file=\"motion_detr_tiny.py\")\n",
    "\n",
    "cfg = update_cfg(\n",
    "    cfg,det_grid_conf=det_grid_conf,grid_conf=det_grid_conf, map_grid_conf=map_grid_conf, motion_grid_conf=motion_grid_conf, point_cloud_range=point_cloud_range_extended_fustrum, t_input_shape=(90, 155)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = build_dataset(cfg.data.test)\n",
    "data_loader = build_dataloader(\n",
    "    dataset,\n",
    "    samples_per_gpu=2,\n",
    "    workers_per_gpu=cfg.data.workers_per_gpu,\n",
    "    dist=False,\n",
    "    shuffle=False)\n",
    "\n",
    "sample = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMSFreeCoder\n",
      "MapHead\n"
     ]
    }
   ],
   "source": [
    "load_external_weights=False\n",
    "\n",
    "model = build_model(cfg.model, test_cfg=cfg.get(\"test_cfg\"))\n",
    "if load_external_weights:\n",
    "    \n",
    "    cfg.checkpoint_config.meta = dict(\n",
    "        mmdet_version=mmdet_version,\n",
    "        mmseg_version=mmseg_version,\n",
    "        mmdet3d_version=mmdet3d_version,\n",
    "        config=cfg.pretty_text,\n",
    "        CLASSES=dataset.CLASSES,\n",
    "        PALETTE=dataset.PALETTE  # for segmentors\n",
    "        if hasattr(dataset, 'PALETTE') else None)\n",
    "    \n",
    "    weights_tiny = torch.load(\n",
    "        \"/home/niklas/ETM_BEV/BEVerse/weights/clean_weights_tiny.pth\")\n",
    "    \n",
    "    search_weights = tuple(weights_tiny.keys())\n",
    "    state_dict_detr = model.state_dict()\n",
    "    for k in state_dict_detr.keys():\n",
    "        if k in search_weights:\n",
    "            try:\n",
    "                state_dict_detr[k] = weights_tiny[k].clone()\n",
    "            except Exception as e:\n",
    "                print(f\"Failure for {k}, exception {e}\")\n",
    "\n",
    "\n",
    "wrap_fp16_model(model)\n",
    "model.cuda()\n",
    "model = MMDataParallel(model, device_ids=[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [128, 64, 3, 3], expected input[2, 256, 200, 200] to have 64 channels, but got 256 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 25\u001b[0m\n\u001b[1;32m     13\u001b[0m motion_distribution_targets \u001b[39m=\u001b[39m {\n\u001b[1;32m     14\u001b[0m     \u001b[39m# for motion prediction\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmotion_segmentation\u001b[39m\u001b[39m'\u001b[39m: sample[\u001b[39m'\u001b[39m\u001b[39mmotion_segmentation\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mfuture_egomotion\u001b[39m\u001b[39m'\u001b[39m: sample[\u001b[39m'\u001b[39m\u001b[39mfuture_egomotions\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m],\n\u001b[1;32m     21\u001b[0m }\n\u001b[1;32m     24\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 25\u001b[0m     result \u001b[39m=\u001b[39m model(\n\u001b[1;32m     26\u001b[0m         return_loss\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     27\u001b[0m         rescale\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     28\u001b[0m         img_metas\u001b[39m=\u001b[39;49msample[\u001b[39m'\u001b[39;49m\u001b[39mimg_metas\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     29\u001b[0m         img_inputs\u001b[39m=\u001b[39;49msample[\u001b[39m'\u001b[39;49m\u001b[39mimg_inputs\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     30\u001b[0m         future_egomotions\u001b[39m=\u001b[39;49msample[\u001b[39m'\u001b[39;49m\u001b[39mfuture_egomotions\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     31\u001b[0m         motion_targets\u001b[39m=\u001b[39;49mmotion_distribution_targets,\n\u001b[1;32m     32\u001b[0m         img_is_valid\u001b[39m=\u001b[39;49msample[\u001b[39m'\u001b[39;49m\u001b[39mimg_is_valid\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m0\u001b[39;49m],\n\u001b[1;32m     33\u001b[0m     )\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/mmcv/parallel/data_parallel.py:42\u001b[0m, in \u001b[0;36mMMDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39minputs[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs[\u001b[39m0\u001b[39m])\n\u001b[1;32m     41\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:166\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m     kwargs \u001b[39m=\u001b[39m ({},)\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule(\u001b[39m*\u001b[39;49minputs[\u001b[39m0\u001b[39;49m], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m    167\u001b[0m replicas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplicate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids[:\u001b[39mlen\u001b[39m(inputs)])\n\u001b[1;32m    168\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparallel_apply(replicas, inputs, kwargs)\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py:128\u001b[0m, in \u001b[0;36mauto_fp16.<locals>.auto_fp16_wrapper.<locals>.new_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mif\u001b[39;00m (TORCH_VERSION \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mparrots\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         digit_version(TORCH_VERSION) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m digit_version(\u001b[39m'\u001b[39m\u001b[39m1.6.0\u001b[39m\u001b[39m'\u001b[39m)):\n\u001b[1;32m    127\u001b[0m     \u001b[39mwith\u001b[39;00m autocast(enabled\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 128\u001b[0m         output \u001b[39m=\u001b[39m old_func(\u001b[39m*\u001b[39;49mnew_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mnew_kwargs)\n\u001b[1;32m    129\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     output \u001b[39m=\u001b[39m old_func(\u001b[39m*\u001b[39mnew_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnew_kwargs)\n",
      "File \u001b[0;32m~/ETM_BEV/BEVerse/mmdet3d/models/detectors/base.py:61\u001b[0m, in \u001b[0;36mBase3DDetector.forward\u001b[0;34m(self, return_loss, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_train(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     60\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_test(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/ETM_BEV/BEVerse/projects/mmdet3d_plugin/models/detectors/beverse_motion_detr_ext.py:366\u001b[0m, in \u001b[0;36mBEVerse_Motion_DETR.forward_test\u001b[0;34m(self, points, img_metas, img_inputs, future_egomotions, **kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(img_inputs[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m], \u001b[39mlist\u001b[39m):\n\u001b[1;32m    365\u001b[0m     img_inputs \u001b[39m=\u001b[39m [img_inputs] \u001b[39mif\u001b[39;00m img_inputs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m img_inputs\n\u001b[0;32m--> 366\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msimple_test(\n\u001b[1;32m    367\u001b[0m         img_metas[\u001b[39m0\u001b[39;49m], img_inputs[\u001b[39m0\u001b[39;49m], future_egomotions[\u001b[39m0\u001b[39;49m], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    368\u001b[0m     )\n\u001b[1;32m    369\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    370\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maug_test(\n\u001b[1;32m    371\u001b[0m         img_metas[\u001b[39m0\u001b[39m], img_inputs[\u001b[39m0\u001b[39m], future_egomotions[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    372\u001b[0m     )\n",
      "File \u001b[0;32m~/ETM_BEV/BEVerse/projects/mmdet3d_plugin/models/detectors/beverse_motion_detr_ext.py:406\u001b[0m, in \u001b[0;36mBEVerse_Motion_DETR.simple_test\u001b[0;34m(self, img_metas, img, future_egomotions, rescale, motion_targets, img_is_valid)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdebug(\n\u001b[1;32m    401\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mBEVerse Extract_img_feat_total \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    402\u001b[0m     \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(t_Extract_img_feat_total)\n\u001b[1;32m    403\u001b[0m )  \u001b[39m# str(t_Extract_img_feat_total)        )\u001b[39;00m\n\u001b[1;32m    405\u001b[0m start \u001b[39m=\u001b[39m timer()\n\u001b[0;32m--> 406\u001b[0m predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msimple_test_pts(\n\u001b[1;32m    407\u001b[0m     img_feats, img_metas, rescale\u001b[39m=\u001b[39;49mrescale, motion_targets\u001b[39m=\u001b[39;49mmotion_targets\n\u001b[1;32m    408\u001b[0m )\n\u001b[1;32m    410\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39msynchronize()\n\u001b[1;32m    411\u001b[0m end \u001b[39m=\u001b[39m timer()\n",
      "File \u001b[0;32m~/ETM_BEV/BEVerse/projects/mmdet3d_plugin/models/detectors/beverse_motion_detr_ext.py:430\u001b[0m, in \u001b[0;36mBEVerse_Motion_DETR.simple_test_pts\u001b[0;34m(self, x, img_metas, rescale, motion_targets)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msimple_test_pts\u001b[39m(\u001b[39mself\u001b[39m, x, img_metas, rescale\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, motion_targets\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    429\u001b[0m     \u001b[39m\"\"\"Test function of point cloud branch.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m     outs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpts_bbox_head(x, targets\u001b[39m=\u001b[39;49mmotion_targets)\n\u001b[1;32m    432\u001b[0m     predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpts_bbox_head\u001b[39m.\u001b[39minference(\n\u001b[1;32m    433\u001b[0m         outs,\n\u001b[1;32m    434\u001b[0m         img_metas,\n\u001b[1;32m    435\u001b[0m         rescale\u001b[39m=\u001b[39mrescale,\n\u001b[1;32m    436\u001b[0m     )\n\u001b[1;32m    438\u001b[0m     \u001b[39m# convert bbox predictions\u001b[39;00m\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ETM_BEV/BEVerse/projects/mmdet3d_plugin/models/dense_heads/mtl_motion_detr.py:360\u001b[0m, in \u001b[0;36mMultiTaskHead_Motion_DETR.forward\u001b[0;34m(self, bev_feats, targets)\u001b[0m\n\u001b[1;32m    352\u001b[0m map_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtask_decoders[\u001b[39m\"\u001b[39m\u001b[39mmap\u001b[39m\u001b[39m\"\u001b[39m]([task_feat])\n\u001b[1;32m    357\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdebug(\n\u001b[1;32m    358\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMTL-HEAD forward Tasks2: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(task_feat\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 360\u001b[0m task_feat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtaskfeat_encoders[\u001b[39m\"\u001b[39;49m\u001b[39m3dod\u001b[39;49m\u001b[39m\"\u001b[39;49m]([task_feat])\n\u001b[1;32m    361\u001b[0m b,c,h,w \u001b[39m=\u001b[39m task_feat\u001b[39m.\u001b[39mshape\n\u001b[1;32m    362\u001b[0m task_mask \u001b[39m=\u001b[39m mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(\n\u001b[1;32m    363\u001b[0m     (b, h, w), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mbool, device\u001b[39m=\u001b[39mtask_feat\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ETM_BEV/BEVerse/projects/mmdet3d_plugin/models/dense_heads/bev_encoder.py:229\u001b[0m, in \u001b[0;36mBevEncode.forward\u001b[0;34m(self, bev_feat_list)\u001b[0m\n\u001b[1;32m    227\u001b[0m x_tmp \u001b[39m=\u001b[39m bev_feat_list[\u001b[39m0\u001b[39m]\n\u001b[1;32m    228\u001b[0m \u001b[39mfor\u001b[39;00m lid, layer \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers):\n\u001b[0;32m--> 229\u001b[0m     x_tmp \u001b[39m=\u001b[39m layer(x_tmp)\n\u001b[1;32m    230\u001b[0m     \u001b[39m# x_tmp = checkpoint.checkpoint(layer,x_tmp)\u001b[39;00m\n\u001b[1;32m    231\u001b[0m     \u001b[39mif\u001b[39;00m lid \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackbone_output_ids:\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/mmdet/models/backbones/resnet.py:89\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     87\u001b[0m     out \u001b[39m=\u001b[39m cp\u001b[39m.\u001b[39mcheckpoint(_inner_forward, x)\n\u001b[1;32m     88\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 89\u001b[0m     out \u001b[39m=\u001b[39m _inner_forward(x)\n\u001b[1;32m     91\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n\u001b[1;32m     93\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/mmdet/models/backbones/resnet.py:72\u001b[0m, in \u001b[0;36mBasicBlock.forward.<locals>._inner_forward\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_inner_forward\u001b[39m(x):\n\u001b[1;32m     70\u001b[0m     identity \u001b[39m=\u001b[39m x\n\u001b[0;32m---> 72\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[1;32m     73\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(out)\n\u001b[1;32m     74\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/torch/nn/modules/conv.py:446\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 446\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/torch/nn/modules/conv.py:442\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    440\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    441\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 442\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    443\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [128, 64, 3, 3], expected input[2, 256, 200, 200] to have 64 channels, but got 256 channels instead"
     ]
    }
   ],
   "source": [
    "logger.debug(cfg[\"det_grid_conf\"])\n",
    "logger.debug(cfg[\"motion_grid_conf\"])\n",
    "logger.debug(cfg[\"map_grid_conf\"])\n",
    "\n",
    "\n",
    "bev_resolution, bev_start_position, bev_dimension = calculate_birds_eye_view_parameters(\n",
    "    cfg[\"det_grid_conf\"][\"xbound\"], cfg[\"det_grid_conf\"][\"ybound\"], cfg[\"det_grid_conf\"][\"zbound\"])\n",
    "logger.debug(f\"bev_resolution: {bev_resolution}\")\n",
    "logger.debug(f\"bev_start_position: {bev_start_position}\")\n",
    "logger.debug(f\"bev_dimension: {bev_dimension}\")\n",
    "\n",
    "\n",
    "motion_distribution_targets = {\n",
    "    # for motion prediction\n",
    "    'motion_segmentation': sample['motion_segmentation'][0],\n",
    "    'motion_instance': sample['motion_instance'][0],\n",
    "    'instance_centerness': sample['instance_centerness'][0],\n",
    "    'instance_offset': sample['instance_offset'][0],\n",
    "    'instance_flow': sample['instance_flow'][0],\n",
    "    'future_egomotion': sample['future_egomotions'][0],\n",
    "}\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    result = model(\n",
    "        return_loss=False,\n",
    "        rescale=True,\n",
    "        img_metas=sample['img_metas'],\n",
    "        img_inputs=sample['img_inputs'],\n",
    "        future_egomotions=sample['future_egomotions'],\n",
    "        motion_targets=motion_distribution_targets,\n",
    "        img_is_valid=sample['img_is_valid'][0],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dict_keys(['img_metas', 'img_inputs', 'semantic_indices', 'semantic_map', 'future_egomotions', 'gt_bboxes_3d', 'gt_labels_3d', 'motion_segmentation', 'motion_instance', 'instance_centerness', 'instance_offset', 'instance_flow', 'has_invalid_frame', 'img_is_valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 200, 200])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sample[\"instance_flow\"][0][0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 1, 200, 200])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sample[\"instance_centerness\"][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 2, 200, 200])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sample[\"instance_offset\"][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 2, 200, 200])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sample[\"instance_flow\"][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 200, 200])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sample[\"motion_segmentation\"][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 6])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sample[\"future_egomotions\"][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200, 200])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"semantic_indices\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 200, 200])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sample[\"semantic_map\"][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([39])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sample[\"gt_labels_3d\"][0].data[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 'bev',\n",
    "* 'bottom_center',\n",
    "* 'bottom_height',\n",
    "* 'box_dim',\n",
    "* 'cat',\n",
    "* 'center',\n",
    "* 'clone',\n",
    "* 'convert_to',\n",
    "* 'corners',\n",
    "* 'device',\n",
    "* 'dims',\n",
    "* 'enlarged_box',\n",
    "* 'flip',\n",
    "* 'gravity_center',\n",
    "* 'height',\n",
    "* 'height_overlaps',\n",
    "* 'in_range_3d',\n",
    "* 'in_range_bev',\n",
    "* 'limit_yaw',\n",
    "* 'nearest_bev',\n",
    "* 'new_box',\n",
    "* 'nonempty',\n",
    "* 'overlaps',\n",
    "* 'points_in_boxes',\n",
    "* 'rotate',\n",
    "* 'scale',\n",
    "* 'tensor',\n",
    "* 'to',\n",
    "* 'top_height',\n",
    "* 'translate',\n",
    "* 'volume',\n",
    "* 'with_yaw',\n",
    "* 'yaw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([39, 4])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sample[\"gt_bboxes_3d\"][0].data[0][0].nearest_bev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([96, 3, 4, 4])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_tiny['img_backbone.patch_embed.projection.weight'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([96, 3, 4, 4])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict_detr['img_backbone.patch_embed.projection.weight'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMSFreeCoder\n",
      "MapHead\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# checkpoint = load_checkpoint(\n",
    "#     model, r\"/home/niklas/ETM_BEV/BEVerse/weights/clean_weights_tiny.pth\", map_location='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = build_model(cfg.model, test_cfg=cfg.get(\"test_cfg\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [30], line 24\u001b[0m\n\u001b[1;32m     12\u001b[0m motion_distribution_targets \u001b[39m=\u001b[39m {\n\u001b[1;32m     13\u001b[0m     \u001b[39m# for motion prediction\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmotion_segmentation\u001b[39m\u001b[39m'\u001b[39m: sample[\u001b[39m'\u001b[39m\u001b[39mmotion_segmentation\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mfuture_egomotion\u001b[39m\u001b[39m'\u001b[39m: sample[\u001b[39m'\u001b[39m\u001b[39mfuture_egomotions\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m],\n\u001b[1;32m     20\u001b[0m }\n\u001b[1;32m     23\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 24\u001b[0m     result \u001b[39m=\u001b[39m model(\n\u001b[1;32m     25\u001b[0m         return_loss\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     26\u001b[0m         rescale\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     27\u001b[0m         img_metas\u001b[39m=\u001b[39;49msample[\u001b[39m'\u001b[39;49m\u001b[39mimg_metas\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     28\u001b[0m         img_inputs\u001b[39m=\u001b[39;49msample[\u001b[39m'\u001b[39;49m\u001b[39mimg_inputs\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     29\u001b[0m         future_egomotions\u001b[39m=\u001b[39;49msample[\u001b[39m'\u001b[39;49m\u001b[39mfuture_egomotions\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     30\u001b[0m         motion_targets\u001b[39m=\u001b[39;49mmotion_distribution_targets,\n\u001b[1;32m     31\u001b[0m         img_is_valid\u001b[39m=\u001b[39;49msample[\u001b[39m'\u001b[39;49m\u001b[39mimg_is_valid\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m0\u001b[39;49m],\n\u001b[1;32m     32\u001b[0m     )\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/mmcv/parallel/data_parallel.py:42\u001b[0m, in \u001b[0;36mMMDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39minputs[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs[\u001b[39m0\u001b[39m])\n\u001b[1;32m     41\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:166\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m     kwargs \u001b[39m=\u001b[39m ({},)\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule(\u001b[39m*\u001b[39;49minputs[\u001b[39m0\u001b[39;49m], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m    167\u001b[0m replicas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplicate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids[:\u001b[39mlen\u001b[39m(inputs)])\n\u001b[1;32m    168\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparallel_apply(replicas, inputs, kwargs)\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py:128\u001b[0m, in \u001b[0;36mauto_fp16.<locals>.auto_fp16_wrapper.<locals>.new_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mif\u001b[39;00m (TORCH_VERSION \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mparrots\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         digit_version(TORCH_VERSION) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m digit_version(\u001b[39m'\u001b[39m\u001b[39m1.6.0\u001b[39m\u001b[39m'\u001b[39m)):\n\u001b[1;32m    127\u001b[0m     \u001b[39mwith\u001b[39;00m autocast(enabled\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 128\u001b[0m         output \u001b[39m=\u001b[39m old_func(\u001b[39m*\u001b[39;49mnew_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mnew_kwargs)\n\u001b[1;32m    129\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     output \u001b[39m=\u001b[39m old_func(\u001b[39m*\u001b[39mnew_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnew_kwargs)\n",
      "File \u001b[0;32m~/ETM_BEV/BEVerse/mmdet3d/models/detectors/base.py:61\u001b[0m, in \u001b[0;36mBase3DDetector.forward\u001b[0;34m(self, return_loss, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_train(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     60\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_test(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/ETM_BEV/BEVerse/projects/mmdet3d_plugin/models/detectors/beverse_motion_detr_ext.py:366\u001b[0m, in \u001b[0;36mBEVerse_Motion_DETR.forward_test\u001b[0;34m(self, points, img_metas, img_inputs, future_egomotions, **kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(img_inputs[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m], \u001b[39mlist\u001b[39m):\n\u001b[1;32m    365\u001b[0m     img_inputs \u001b[39m=\u001b[39m [img_inputs] \u001b[39mif\u001b[39;00m img_inputs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m img_inputs\n\u001b[0;32m--> 366\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msimple_test(\n\u001b[1;32m    367\u001b[0m         img_metas[\u001b[39m0\u001b[39;49m], img_inputs[\u001b[39m0\u001b[39;49m], future_egomotions[\u001b[39m0\u001b[39;49m], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    368\u001b[0m     )\n\u001b[1;32m    369\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    370\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maug_test(\n\u001b[1;32m    371\u001b[0m         img_metas[\u001b[39m0\u001b[39m], img_inputs[\u001b[39m0\u001b[39m], future_egomotions[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    372\u001b[0m     )\n",
      "File \u001b[0;32m~/ETM_BEV/BEVerse/projects/mmdet3d_plugin/models/detectors/beverse_motion_detr_ext.py:397\u001b[0m, in \u001b[0;36mBEVerse_Motion_DETR.simple_test\u001b[0;34m(self, img_metas, img, future_egomotions, rescale, motion_targets, img_is_valid)\u001b[0m\n\u001b[1;32m    395\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39msynchronize()\n\u001b[1;32m    396\u001b[0m end \u001b[39m=\u001b[39m timer()\n\u001b[0;32m--> 397\u001b[0m time_stats[\u001b[39m\"\u001b[39;49m\u001b[39mExtract_img_feat_total\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m=\u001b[39m (end \u001b[39m-\u001b[39m start) \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[1;32m    399\u001b[0m t_Extract_img_feat_total \u001b[39m=\u001b[39m (end \u001b[39m-\u001b[39m start) \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[1;32m    400\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdebug(\n\u001b[1;32m    401\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mBEVerse Extract_img_feat_total \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    402\u001b[0m     \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(t_Extract_img_feat_total)\n\u001b[1;32m    403\u001b[0m )  \u001b[39m# str(t_Extract_img_feat_total)        )\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 3"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_get_backward_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_version',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'device_ids',\n",
       " 'dim',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'gather',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'load_state_dict',\n",
       " 'module',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'output_device',\n",
       " 'parallel_apply',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_parameter',\n",
       " 'replicate',\n",
       " 'requires_grad_',\n",
       " 'scatter',\n",
       " 'set_extra_state',\n",
       " 'share_memory',\n",
       " 'src_device_obj',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'train_step',\n",
       " 'training',\n",
       " 'type',\n",
       " 'val_step',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_dump_init_info',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_get_backward_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_is_init',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_parse_losses',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_version',\n",
       " 'add_module',\n",
       " 'aforward_test',\n",
       " 'apply',\n",
       " 'async_simple_test',\n",
       " 'aug_test',\n",
       " 'aug_test_pts',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'combine_bev_output',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'data_aug_conf',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'extract_feat',\n",
       " 'extract_feats',\n",
       " 'extract_img_feat',\n",
       " 'extract_img_feat_tta',\n",
       " 'extract_pts_feat',\n",
       " 'flip_bev_output',\n",
       " 'flip_feature',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'forward_dummy',\n",
       " 'forward_img_train',\n",
       " 'forward_pts_train',\n",
       " 'forward_test',\n",
       " 'forward_train',\n",
       " 'fp16_enabled',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'img_backbone',\n",
       " 'img_neck',\n",
       " 'init_cfg',\n",
       " 'init_weights',\n",
       " 'is_init',\n",
       " 'load_state_dict',\n",
       " 'logger',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'onnx_export',\n",
       " 'parameters',\n",
       " 'pts_bbox_head',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'set_extra_state',\n",
       " 'share_memory',\n",
       " 'show_result',\n",
       " 'show_results',\n",
       " 'simple_test',\n",
       " 'simple_test_img',\n",
       " 'simple_test_pts',\n",
       " 'simple_test_rpn',\n",
       " 'state_dict',\n",
       " 'temporal_model',\n",
       " 'test_cfg',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'train_cfg',\n",
       " 'train_step',\n",
       " 'training',\n",
       " 'transformer',\n",
       " 'type',\n",
       " 'val_step',\n",
       " 'voxelize',\n",
       " 'with_bbox',\n",
       " 'with_fusion',\n",
       " 'with_img_backbone',\n",
       " 'with_img_bbox',\n",
       " 'with_img_neck',\n",
       " 'with_img_roi_head',\n",
       " 'with_img_rpn',\n",
       " 'with_img_shared_head',\n",
       " 'with_mask',\n",
       " 'with_middle_encoder',\n",
       " 'with_neck',\n",
       " 'with_pts_backbone',\n",
       " 'with_pts_bbox',\n",
       " 'with_pts_neck',\n",
       " 'with_shared_head',\n",
       " 'with_voxel_encoder',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model.module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pred_semantic_indices', 'motion_predictions', 'motion_segmentation', 'motion_instance', 'bbox_results', 'time_stats'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['boxes_3d', 'scores_3d', 'labels_3d'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"bbox_results\"][0][\"pts_bbox\"].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scores_3d - torch.Size([498])\n",
    "labels_3d - torch.Size([498])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([498])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['bbox_results'][0]['pts_bbox'][\"labels_3d\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 'bev', torch.Size([498, 5])\n",
    " 'bottom_center', torch.Size([498, 3])\n",
    " 'bottom_height', torch.Size([498])\n",
    " 'box_dim', 9 \n",
    " 'cat',\n",
    " 'center', orch.Size([498, 3])\n",
    " 'clone',\n",
    " 'convert_to',\n",
    " 'corners',\n",
    " 'device',\n",
    " 'dims',\n",
    " 'enlarged_box',\n",
    " 'flip',\n",
    " 'gravity_center', torch.Size([498, 3])\n",
    " 'height', torch.Size([498])\n",
    " 'height_overlaps',\n",
    " 'in_range_3d',\n",
    " 'in_range_bev',\n",
    " 'limit_yaw',\n",
    " 'nearest_bev',\n",
    " 'new_box',\n",
    " 'nonempty',\n",
    " 'overlaps',\n",
    " 'points_in_boxes',\n",
    " 'rotate',\n",
    " 'scale',\n",
    " 'tensor',\n",
    " 'to',\n",
    " 'top_height',\n",
    " 'translate',\n",
    " 'volume', torch.Size([498])\n",
    " 'with_yaw',\n",
    " 'yaw']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image (T: 3 H: 65 W: 178) smaller than kernel size (kT: 2 kH: 128 kW: 128)\n",
    "\n",
    "\n",
    "update\n",
    "pyramid pooling:  torch.Size([1, 70, 3, 128, 128])\n",
    "x_pool:  torch.Size([1, 23, 3, 1, 1])\n",
    "c:  23\n",
    "x_pool2:  torch.Size([3, 23, 128, 128])\n",
    "x_pool3:  torch.Size([1, 23, 3, 128, 128])\n",
    "update\n",
    "pyramid pooling:  torch.Size([1, 64, 3, 128, 128])\n",
    "x_pool:  torch.Size([1, 21, 3, 1, 1])\n",
    "c:  21\n",
    "x_pool2:  torch.Size([3, 21, 128, 128])\n",
    "x_pool3:  torch.Size([1, 21, 3, 128, 128])\n",
    "feats-1: torch.Size([1, 512, 25, 50]), feats-3: torch.Size([1, 128, 100, 200])\n",
    "Up: x1 torch.Size([1, 512, 100, 200]), x2 torch.Size([1, 128, 100, 200])\n",
    "feats-1: torch.Size([1, 512, 16, 16]), feats-3: torch.Size([1, 128, 64, 64])\n",
    "Up: x1 torch.Size([1, 512, 64, 64]), x2 torch.Size([1, 128, 64, 64])\n",
    "feats-1: torch.Size([1, 512, 25, 25]), feats-3: torch.Size([1, 128, 100, 100])\n",
    "Up: x1 torch.Size([1, 512, 100, 100]), x2 torch.Size([1, 128, 100, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"IMAGE\": {\n",
    "    \"ORIGINAL_HEIGHT\": 900,\n",
    "    \"ORIGINAL_WIDTH\": 1600,\n",
    "    \"FINAL_DIM\": (512, 1408),\n",
    "    \"RESIZE_SCALE\": 0.25,\n",
    "    \"TOP_CROP\": 0,\n",
    "}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['IMAGE'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero padding left and right parts of the image.\n",
      "Zero padding bottom part of the image.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'scale_width': 0.25,\n",
       " 'scale_height': 0.25,\n",
       " 'resize_dims': (400, 225),\n",
       " 'crop': (0, 0, 1408, 512)}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_resizing_and_cropping_parameters(config):\n",
    "    original_height, original_width = config[\"IMAGE\"][\"ORIGINAL_HEIGHT\"], config[\"IMAGE\"][\"ORIGINAL_WIDTH\"]\n",
    "    final_height, final_width = config[\"IMAGE\"][\"FINAL_DIM\"]\n",
    "\n",
    "    resize_scale = config[\"IMAGE\"][\"RESIZE_SCALE\"]\n",
    "    resize_dims = (int(original_width * resize_scale), int(original_height * resize_scale))\n",
    "    resized_width, resized_height = resize_dims\n",
    "\n",
    "    crop_h = config[\"IMAGE\"][\"TOP_CROP\"]\n",
    "    crop_w = int(max(0, (resized_width - final_width) / 2))\n",
    "    # Left, top, right, bottom crops.\n",
    "    crop = (crop_w, crop_h, crop_w + final_width, crop_h + final_height)\n",
    "\n",
    "    if resized_width != final_width:\n",
    "        print('Zero padding left and right parts of the image.')\n",
    "    if crop_h + final_height != resized_height:\n",
    "        print('Zero padding bottom part of the image.')\n",
    "\n",
    "    return {'scale_width': resize_scale,\n",
    "            'scale_height': resize_scale,\n",
    "            'resize_dims': resize_dims,\n",
    "            'crop': crop,\n",
    "            }\n",
    "get_resizing_and_cropping_parameters(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t_BEV': 1663055717.0110373,\n",
       " 't_temporal': 1663055717.5791538,\n",
       " 't0': 1663055712.6830127,\n",
       " 't_end': 1663055746.884341}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"time_stats\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x, rots, trans, intrins, post_rots, post_trans = input\n",
    "\n",
    "'CAM_FRONT_RIGHT': {\n",
    "    'data_path': './data/nuscenes/samples/CAM_FRONT_RIGHT/n015-2018-10-08-15-36-50+0800__CAM_FRONT_RIGHT__1538984233520339.jpg', \n",
    "    'type': 'CAM_FRONT_RIGHT', \n",
    "    'sample_data_token': 'd6f89460954c43d39ed7c9ac91ab03d0', \n",
    "    'sensor2ego_translation': [1.5508477543, -0.493404796419, 1.49574800619], \n",
    "    'sensor2ego_rotation': [0.2060347966337182, -0.2026940577919598, 0.6824507824531167, -0.6713610884174485], 'ego2global_translation': [715.6566537856895, 1810.1516804263824, 0.0], \n",
    "    'ego2global_rotation': [0.8004835927391405, 0.00541081062084495, -0.0028680900818508943, -0.5993233809414961], 'timestamp': 1538984233520339, \n",
    "    'sensor2lidar_rotation': array([[ 0.55971752, -0.01057234,  0.82861603],\n",
    "       [-0.82828535,  0.02385392,  0.55979851],\n",
    "       [-0.02568412, -0.99965955,  0.00459454]]), \n",
    "    'sensor2lidar_translation': array([ 0.48135698,  0.51094805, -0.32997566]), \n",
    "    'cam_intrinsic': array([[1.26084744e+03, 0.00000000e+00, 8.07968245e+02],\n",
    "       [0.00000000e+00, 1.26084744e+03, 4.95334427e+02],\n",
    "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future Egomotions:  torch.Size([1, 7, 6])\n",
      "img_is_valid:  torch.Size([1, 7])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "num_warmups = 100\n",
    "num_repeats = 1000\n",
    "# Change to C x 1 x 3 x 1600 x 900\n",
    "# 704256 Tiny\n",
    "# 1408512\n",
    "# B, S, N, C, imH, imW = imgs.shape\n",
    "img_inputs =  torch.rand(1,7,6,3,704,256).cuda()\n",
    "# (batch, seq, num_cam)\n",
    "future_egomotions = torch.zeros((batch_size, 7, 6)).type_as(img_inputs).cuda()\n",
    "img_is_valid = torch.ones((batch_size, 7)).type_as(img_inputs) > 0\n",
    "img_is_valid.cuda()\n",
    "print(\"Future Egomotions: \", future_egomotions.shape)\n",
    "print(\"img_is_valid: \", img_is_valid.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #single frame \n",
    "# future_egomotions = future_egomotions[:, :1]\n",
    "# img_is_valid = img_is_valid[:, :1]\n",
    "# print(\"Future Egomotions: \", future_egomotions.shape)\n",
    "# print(\"img_is_valid: \", img_is_valid.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n",
      "torch.Size([3])\n",
      "image meta: [{'box_type_3d': <class 'mmdet3d.core.bbox.structures.lidar_box3d.LiDARInstance3DBoxes'>, 'lidar2ego_rots': tensor([[-5.4280e-04,  9.9893e-01,  4.6229e-02],\n",
      "        [-1.0000e+00, -4.0569e-04, -2.9750e-03],\n",
      "        [-2.9531e-03, -4.6231e-02,  9.9893e-01]]), 'lidar2ego_trans': tensor([0.9858, 0.0000, 1.8402])}]\n"
     ]
    }
   ],
   "source": [
    "from mmdet3d.core.bbox.structures.box_3d_mode import LiDARInstance3DBoxes\n",
    "\n",
    "dummy_lidar2ego_rots = (\n",
    "    torch.tensor(\n",
    "        [\n",
    "            [-5.4280e-04, 9.9893e-01, 4.6229e-02],\n",
    "            [-1.0000e00, -4.0569e-04, -2.9750e-03],\n",
    "            [-2.9531e-03, -4.6231e-02, 9.9893e-01],\n",
    "        ]\n",
    "    )\n",
    "    .type_as(img_inputs)\n",
    "    .cpu()\n",
    ")\n",
    "dummy_lidar2ego_trans = (\n",
    "    torch.tensor([0.9858, 0.0000, 1.8402]).type_as(img_inputs).cpu()\n",
    ")\n",
    "print(dummy_lidar2ego_rots.shape)\n",
    "print(dummy_lidar2ego_trans.shape)\n",
    "img_metas = [\n",
    "    dict(\n",
    "        box_type_3d=LiDARInstance3DBoxes,\n",
    "        lidar2ego_rots=dummy_lidar2ego_rots,\n",
    "        lidar2ego_trans=dummy_lidar2ego_trans,\n",
    "    )\n",
    "]\n",
    "print(\"image meta:\", img_metas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'projects.mmdet3d_plugin.models.detectors.beverse.BEVerse'>\n"
     ]
    }
   ],
   "source": [
    "model = build_model(cfg.model, test_cfg=cfg.get(\"test_cfg\"))\n",
    "print(type(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch?:  torch.Size([1, 7, 6, 3, 704, 256])\n",
      "B 1, S 7, 6, C 3, imH 704, imW 256\n",
      "imgs  torch.Size([42, 3, 704, 256])\n",
      "after backbone:  2\n",
      "shape in list:  [torch.Size([42, 384, 44, 16]), torch.Size([42, 768, 22, 8])]\n",
      "after backbone with_img_neck:  torch.Size([42, 512, 44, 16])\n",
      "after transformation:  torch.Size([1, 7, 6, 512, 44, 16])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"Tensor\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m build_model(cfg\u001b[38;5;241m.\u001b[39mmodel, test_cfg\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_cfg\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      2\u001b[0m wrap_fp16_model(model)\n\u001b[0;32m----> 3\u001b[0m img_feats,time_dict  \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_img_feat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimg_metas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_metas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfuture_egomotion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuture_egomotions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimg_is_valid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_is_valid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(img_feats\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(time_dict)\n",
      "File \u001b[0;32m~/ETM_BEV/BEVerse/projects/mmdet3d_plugin/models/detectors/beverse.py:105\u001b[0m, in \u001b[0;36mBEVerse.extract_img_feat\u001b[0;34m(self, img, img_metas, future_egomotion, aug_transform, img_is_valid, count_time)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mafter transformation: \u001b[39m\u001b[39m\"\u001b[39m, x\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    104\u001b[0m \u001b[39m# lifting with LSS\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer([x] \u001b[39m+\u001b[39;49m img[\u001b[39m1\u001b[39;49m:])\n\u001b[1;32m    107\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39msynchronize()\n\u001b[1;32m    108\u001b[0m t_BEV \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"Tensor\") to list"
     ]
    }
   ],
   "source": [
    "model = build_model(cfg.model, test_cfg=cfg.get(\"test_cfg\")).cuda()\n",
    "wrap_fp16_model(model)\n",
    "img_feats,time_dict  = model.extract_img_feat(\n",
    "            img=img_inputs,\n",
    "            img_metas=img_metas,\n",
    "            future_egomotion=future_egomotions,\n",
    "            img_is_valid=img_is_valid,\n",
    "        )\n",
    "print(img_feats.shape)\n",
    "print(time_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2000,  0.2000, 20.0000]) tensor([-51.1000, -51.1000,   0.0000]) tensor([512, 512,   1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_362/3561629359.py:21: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  bev_dimension = torch.tensor(\n"
     ]
    }
   ],
   "source": [
    "# 'xbound': [-51.2, 51.2, 0.2], 'ybound': [-51.2, 51.2, 0.2], 'zbound': [-10.0, 10.0, 20.0], 'dbound': [1.0, 60.0, 0.5]}\n",
    "\n",
    "\n",
    "\n",
    "x_bounds, y_bounds, z_bounds = [-51.2, 51.2, 0.2], [-51.2, 51.2, 0.2], [-10.0, 10.0, 20.0]\n",
    "\n",
    "bev_resolution, bev_start_position, bev_dimension = calculate_birds_eye_view_parameters(x_bounds, y_bounds, z_bounds)\n",
    "\n",
    "print(bev_resolution, bev_start_position, bev_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "x_bounds, y_bounds, z_bounds = [-26.2, 26.2, 0.8], [-71.2, 71.2, 0.8], [-10.0, 10.0, 20.0]\n",
    "tensor([ 0.8000,  0.8000, 20.0000]) tensor([-25.8000, -70.8000,   0.0000]) tensor([ 65, 178,   1])\n",
    "\n",
    "\n",
    "x_bounds, y_bounds, z_bounds = [-26.2, 26.2, 0.4], [-71.2, 71.2, 0.4], [-10.0, 10.0, 20.0]\n",
    "tensor([ 0.4000,  0.4000, 20.0000]) tensor([-26., -71.,   0.]) tensor([131, 356,   1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0, 7, 6, 3, 704, 256])\n",
      "torch.Size([1, 7, 6, 3, 704, 256])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"Tensor\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m img_inputs \u001b[38;5;241m=\u001b[39m  torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m7\u001b[39m,\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m704\u001b[39m,\u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(img_inputs[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 6\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mtrans_output\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimg_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      7\u001b[0m test\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"Tensor\") to list"
     ]
    }
   ],
   "source": [
    "img_inputs =  torch.rand(1,7,6,3,704,256)\n",
    "print(img_inputs[1:].shape)\n",
    "trans_output = torch.rand(1, 7, 6, 512, 44, 16)\n",
    "img_inputs =  torch.rand(2,7,6,3,704,256)\n",
    "print(img_inputs[1:].shape)\n",
    "test = [trans_output] + img_inputs[1:]\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency Measurement Using CPU Timer...\n",
      "|Synchronization: True | Continuous Measurement: True | Latency: N/A     ms| \n",
      "Latency Measurement Using CUDA Timer...\n",
      "|Synchronization: True | Continuous Measurement: True | Latency: N/A     ms| \n",
      "|Synchronization: False| Continuous Measurement: True | Latency: N/A     ms| \n",
      "|Synchronization: True | Continuous Measurement: False| Latency: N/A     ms| \n",
      "|Synchronization: False| Continuous Measurement: False| Latency: N/A     ms| \n",
      "Latency Measurement Using PyTorch Benchmark...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dummy_input.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 95\u001b[0m\n\u001b[1;32m     85\u001b[0m num_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     86\u001b[0m timer \u001b[38;5;241m=\u001b[39m benchmark\u001b[38;5;241m.\u001b[39mTimer(\n\u001b[1;32m     87\u001b[0m     stmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_inference(model, input_tensor)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     88\u001b[0m     setup\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom __main__ import run_inference\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m     sub_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.utils.benchmark.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     93\u001b[0m )\n\u001b[0;32m---> 95\u001b[0m profile_result \u001b[38;5;241m=\u001b[39m \u001b[43mtimer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_repeats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# https://pytorch.org/docs/stable/_modules/torch/utils/benchmark/utils/common.html#Measurement\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLatency: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprofile_result\u001b[38;5;241m.\u001b[39mmean \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/torch/utils/benchmark/utils/timer.py:261\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39m\"\"\"Mirrors the semantics of timeit.Timer.timeit().\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \n\u001b[1;32m    256\u001b[0m \u001b[39mExecute the main statement (`stmt`) `number` times.\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[39mhttps://docs.python.org/3/library/timeit.html#timeit.Timer.timeit\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[39mwith\u001b[39;00m common\u001b[39m.\u001b[39mset_torch_threads(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_spec\u001b[39m.\u001b[39mnum_threads):\n\u001b[1;32m    260\u001b[0m     \u001b[39m# Warmup\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_timer\u001b[39m.\u001b[39;49mtimeit(number\u001b[39m=\u001b[39;49m\u001b[39mmax\u001b[39;49m(\u001b[39mint\u001b[39;49m(number \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m \u001b[39m100\u001b[39;49m), \u001b[39m2\u001b[39;49m))\n\u001b[1;32m    263\u001b[0m     \u001b[39mreturn\u001b[39;00m common\u001b[39m.\u001b[39mMeasurement(\n\u001b[1;32m    264\u001b[0m         number_per_run\u001b[39m=\u001b[39mnumber,\n\u001b[1;32m    265\u001b[0m         raw_times\u001b[39m=\u001b[39m[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timer\u001b[39m.\u001b[39mtimeit(number\u001b[39m=\u001b[39mnumber)],\n\u001b[1;32m    266\u001b[0m         task_spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_spec\n\u001b[1;32m    267\u001b[0m     )\n",
      "File \u001b[0;32m/usr/lib/python3.8/timeit.py:177\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    175\u001b[0m gc\u001b[39m.\u001b[39mdisable()\n\u001b[1;32m    176\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     timing \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner(it, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimer)\n\u001b[1;32m    178\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     \u001b[39mif\u001b[39;00m gcold:\n",
      "File \u001b[0;32m<timeit-src>:6\u001b[0m, in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn [15], line 89\u001b[0m, in \u001b[0;36mrun_inference\u001b[0;34m(model, input_tensor)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_inference\u001b[39m(model: nn\u001b[38;5;241m.\u001b[39mModule, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# from model.forward because BEVerse differentiates between different input types - img lidar etc \u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_dummy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ETM_BEV/BEVerse/projects/mmdet3d_plugin/models/detectors/beverse.py:209\u001b[0m, in \u001b[0;36mforward_dummy\u001b[0;34m(self, img_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39m\"\"\"Forward training function.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \n\u001b[1;32m    169\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39m    dict: Losses of different branches.\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    193\u001b[0m img_feats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextract_img_feat(\n\u001b[1;32m    194\u001b[0m     img\u001b[39m=\u001b[39mimg_inputs,\n\u001b[1;32m    195\u001b[0m     img_metas\u001b[39m=\u001b[39mimg_metas,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    198\u001b[0m     img_is_valid\u001b[39m=\u001b[39mimg_is_valid,\n\u001b[1;32m    199\u001b[0m )\n\u001b[1;32m    201\u001b[0m mtl_targets \u001b[39m=\u001b[39m {\n\u001b[1;32m    202\u001b[0m     \u001b[39m# for detection\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mgt_bboxes_3d\u001b[39m\u001b[39m\"\u001b[39m: gt_bboxes_3d,\n\u001b[1;32m    204\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mgt_labels_3d\u001b[39m\u001b[39m\"\u001b[39m: gt_labels_3d,\n\u001b[1;32m    205\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mgt_bboxes_ignore\u001b[39m\u001b[39m\"\u001b[39m: gt_bboxes_ignore,\n\u001b[1;32m    206\u001b[0m     \u001b[39m# for map segmentation\u001b[39;00m\n\u001b[1;32m    207\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39msemantic_seg\u001b[39m\u001b[39m\"\u001b[39m: semantic_indices,\n\u001b[1;32m    208\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39msemantic_map\u001b[39m\u001b[39m\"\u001b[39m: semantic_map,\n\u001b[0;32m--> 209\u001b[0m     \u001b[39m# for motion prediction\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmotion_segmentation\u001b[39m\u001b[39m\"\u001b[39m: motion_segmentation,\n\u001b[1;32m    211\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmotion_instance\u001b[39m\u001b[39m\"\u001b[39m: motion_instance,\n\u001b[1;32m    212\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minstance_centerness\u001b[39m\u001b[39m\"\u001b[39m: instance_centerness,\n\u001b[1;32m    213\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minstance_offset\u001b[39m\u001b[39m\"\u001b[39m: instance_offset,\n\u001b[1;32m    214\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minstance_flow\u001b[39m\u001b[39m\"\u001b[39m: instance_flow,\n\u001b[1;32m    215\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mfuture_egomotion\u001b[39m\u001b[39m\"\u001b[39m: future_egomotions,\n\u001b[1;32m    216\u001b[0m     \u001b[39m# for bev_augmentation\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39maug_transform\u001b[39m\u001b[39m\"\u001b[39m: aug_transform,\n\u001b[1;32m    218\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mimg_is_valid\u001b[39m\u001b[39m\"\u001b[39m: img_is_valid,\n\u001b[1;32m    219\u001b[0m }\n\u001b[1;32m    221\u001b[0m loss_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_pts_train(img_feats, img_metas, mtl_targets)\n\u001b[1;32m    223\u001b[0m \u001b[39mreturn\u001b[39;00m loss_dict\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/torch/serialization.py:594\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    592\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 594\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    595\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    596\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    598\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    599\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    231\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dummy_input.pt'"
     ]
    }
   ],
   "source": [
    "\n",
    "num_warmups = 100\n",
    "num_repeats = 1000\n",
    "# Change to C x 1 x 3 x 1600 x 900\n",
    "# 704256 Tiny\n",
    "# 1408512\n",
    "# B, T, N, C, imH, imW = imgs.shape\n",
    "input_shape = (1,3,6,3,1600,900)\n",
    "\n",
    "future_egomotions = torch.zeros((batch_size, 7, 6)).type_as(img_inputs)\n",
    "img_is_valid = torch.ones((batch_size, 7)).type_as(img_inputs) > 0\n",
    "print(\"Future Egomotions: \", future_egomotions.shape)\n",
    "print(\"img_is_valid: \", img_is_valid.shape)\n",
    "\n",
    "\n",
    "# imgs = imgs.view(B * S * N, C, imH, imW)\n",
    "\n",
    "model = build_model(cfg.model, test_cfg=cfg.get(\"test_cfg\"))\n",
    "wrap_fp16_model(model)\n",
    "# load_checkpoint(\n",
    "#     model,\n",
    "#     r\"/home/niklas/ETM_BEV/BEVerse/checkpoints/beverse_tiny.pth\",\n",
    "#     map_location=\"cpu\",\n",
    "# )\n",
    "# model = fuse_module(model)\n",
    "model.cuda(device)\n",
    "model.eval()\n",
    "# model = nn.Conv2d(in_channels=input_shape[1], out_channels=256, kernel_size=(5, 5))\n",
    "\n",
    "# Input tensor\n",
    "input_tensor = torch.rand(input_shape, device=device)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "print(\"Latency Measurement Using CPU Timer...\")\n",
    "for continuous_measure in [True]:\n",
    "    for synchronize in [True]:\n",
    "        try:\n",
    "            latency_ms = measure_time_host(\n",
    "                model=model,\n",
    "                input_tensor=input_tensor,\n",
    "                num_repeats=num_repeats,\n",
    "                num_warmups=num_warmups,\n",
    "                synchronize=synchronize,\n",
    "                continuous_measure=continuous_measure,\n",
    "            )\n",
    "            print(\n",
    "                f\"|\"\n",
    "                f\"Synchronization: {synchronize!s:5}| \"\n",
    "                f\"Continuous Measurement: {continuous_measure!s:5}| \"\n",
    "                f\"Latency: {latency_ms:.5f} ms| \"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"|\"\n",
    "                f\"Synchronization: {synchronize!s:5}| \"\n",
    "                f\"Continuous Measurement: {continuous_measure!s:5}| \"\n",
    "                f\"Latency: N/A     ms| \"\n",
    "            )\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "print(\"Latency Measurement Using CUDA Timer...\")\n",
    "for continuous_measure in [True, False]:\n",
    "    for synchronize in [True, False]:\n",
    "        try:\n",
    "            latency_ms = measure_time_device(\n",
    "                model=model,\n",
    "                input_tensor=input_tensor,\n",
    "                num_repeats=num_repeats,\n",
    "                num_warmups=num_warmups,\n",
    "                synchronize=synchronize,\n",
    "                continuous_measure=continuous_measure,\n",
    "            )\n",
    "            print(\n",
    "                f\"|\"\n",
    "                f\"Synchronization: {synchronize!s:5}| \"\n",
    "                f\"Continuous Measurement: {continuous_measure!s:5}| \"\n",
    "                f\"Latency: {latency_ms:.5f} ms| \"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"|\"\n",
    "                f\"Synchronization: {synchronize!s:5}| \"\n",
    "                f\"Continuous Measurement: {continuous_measure!s:5}| \"\n",
    "                f\"Latency: N/A     ms| \"\n",
    "            )\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "print(\"Latency Measurement Using PyTorch Benchmark...\")\n",
    "num_threads = 1\n",
    "timer = benchmark.Timer(\n",
    "    stmt=\"run_inference(model, input_tensor)\",\n",
    "    setup=\"from __main__ import run_inference\",\n",
    "    globals={\"model\": model, \"input_tensor\": input_tensor},\n",
    "    num_threads=num_threads,\n",
    "    label=\"Latency Measurement\",\n",
    "    sub_label=\"torch.utils.benchmark.\",\n",
    ")\n",
    "\n",
    "profile_result = timer.timeit(num_repeats)\n",
    "# https://pytorch.org/docs/stable/_modules/torch/utils/benchmark/utils/common.html#Measurement\n",
    "print(f\"Latency: {profile_result.mean * 1000:.5f} ms\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49a6cb26e152f15aca94d1d3fa9630fb57fb8fd83a336982cd2ebf9e9635e69c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
