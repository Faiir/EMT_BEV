MMDataParallel(
  (module): Petr3D(
    (pts_bbox_head): PETRHead(
      (loss_cls): FocalLoss()
      (loss_bbox): L1Loss()
      (input_proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (cls_branches): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): ReLU(inplace=True)
          (3): Linear(in_features=256, out_features=256, bias=True)
          (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (5): ReLU(inplace=True)
          (6): Linear(in_features=256, out_features=10, bias=True)
        )
        (1): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): ReLU(inplace=True)
          (3): Linear(in_features=256, out_features=256, bias=True)
          (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (5): ReLU(inplace=True)
          (6): Linear(in_features=256, out_features=10, bias=True)
        )
        (2): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): ReLU(inplace=True)
          (3): Linear(in_features=256, out_features=256, bias=True)
          (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (5): ReLU(inplace=True)
          (6): Linear(in_features=256, out_features=10, bias=True)
        )
        (3): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): ReLU(inplace=True)
          (3): Linear(in_features=256, out_features=256, bias=True)
          (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (5): ReLU(inplace=True)
          (6): Linear(in_features=256, out_features=10, bias=True)
        )
        (4): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): ReLU(inplace=True)
          (3): Linear(in_features=256, out_features=256, bias=True)
          (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (5): ReLU(inplace=True)
          (6): Linear(in_features=256, out_features=10, bias=True)
        )
        (5): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): ReLU(inplace=True)
          (3): Linear(in_features=256, out_features=256, bias=True)
          (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (5): ReLU(inplace=True)
          (6): Linear(in_features=256, out_features=10, bias=True)
        )
      )
      (reg_branches): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): ReLU()
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU()
          (4): Linear(in_features=256, out_features=10, bias=True)
        )
        (1): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): ReLU()
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU()
          (4): Linear(in_features=256, out_features=10, bias=True)
        )
        (2): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): ReLU()
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU()
          (4): Linear(in_features=256, out_features=10, bias=True)
        )
        (3): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): ReLU()
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU()
          (4): Linear(in_features=256, out_features=10, bias=True)
        )
        (4): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): ReLU()
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU()
          (4): Linear(in_features=256, out_features=10, bias=True)
        )
        (5): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): ReLU()
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU()
          (4): Linear(in_features=256, out_features=10, bias=True)
        )
      )
      (adapt_pos3d): Sequential(
        (0): Conv2d(384, 1024, kernel_size=(1, 1), stride=(1, 1))
        (1): ReLU()
        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (position_encoder): Sequential(
        (0): Conv2d(192, 1024, kernel_size=(1, 1), stride=(1, 1))
        (1): ReLU()
        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (reference_points): Embedding(900, 3)
      (query_embedding): Sequential(
        (0): Linear(in_features=384, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
      )
      (loss_iou): GIoULoss()
      (positional_encoding): SinePositionalEncoding3D(num_feats=128, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
      (transformer): PETRTransformer(
        (decoder): PETRTransformerDecoder(
          (layers): ModuleList(
            (0): PETRTransformerDecoderLayer(
              (attentions): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.1, inplace=False)
                )
                (1): PETRMultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.1, inplace=False)
                )
              )
              (ffns): ModuleList(
                (0): FFN(
                  (activate): ReLU(inplace=True)
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=256, out_features=2048, bias=True)
                      (1): ReLU(inplace=True)
                      (2): Dropout(p=0.1, inplace=False)
                    )
                    (1): Linear(in_features=2048, out_features=256, bias=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (dropout_layer): Identity()
                )
              )
              (norms): ModuleList(
                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (img_backbone): 
    init_cfg={'type': 'Pretrained', 'checkpoint': 'ckpts/resnet50_msra-5891d200.pth'}
    (img_neck): CPFPN(
      (lateral_convs): ModuleList(
        (0): ConvModule(
          (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (1): ConvModule(
          (conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (fpn_convs): ModuleList(
        (0): ConvModule(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
    )
    init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
    (grid_mask): GridMask()
  )
)
