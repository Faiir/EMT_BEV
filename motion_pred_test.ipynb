{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "\n",
    "import torch.nn.functional as F \n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from torchvision.ops.boxes import box_area\n",
    "import numpy as np \n",
    "\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(-1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=-1)\n",
    "\n",
    "# modified from torchvision to also return the union\n",
    "\n",
    "\n",
    "def box_iou(boxes1, boxes2):\n",
    "    area1 = box_area(boxes1)\n",
    "    area2 = box_area(boxes2)\n",
    "\n",
    "    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n",
    "    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n",
    "\n",
    "    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
    "    inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n",
    "\n",
    "    union = area1[:, None] + area2 - inter\n",
    "\n",
    "    iou = inter / union\n",
    "    return iou, union\n",
    "\n",
    "\n",
    "def generalized_box_iou(boxes1, boxes2):\n",
    "    \"\"\"\n",
    "    Generalized IoU from https://giou.stanford.edu/\n",
    "\n",
    "    The boxes should be in [x0, y0, x1, y1] format\n",
    "\n",
    "    Returns a [N, M] pairwise matrix, where N = len(boxes1)\n",
    "    and M = len(boxes2)\n",
    "    \"\"\"\n",
    "    # degenerate boxes gives inf / nan results\n",
    "    # so do an early check\n",
    "    assert (boxes1[:, 2:] >= boxes1[:, :2]).all()\n",
    "    assert (boxes2[:, 2:] >= boxes2[:, :2]).all()\n",
    "    iou, union = box_iou(boxes1, boxes2)\n",
    "\n",
    "    lt = torch.min(boxes1[:, None, :2], boxes2[:, :2])\n",
    "    rb = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])\n",
    "\n",
    "    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
    "    area = wh[:, :, 0] * wh[:, :, 1]\n",
    "\n",
    "    return iou - (area - union) / area\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def dice_coef(inputs, targets):\n",
    "    inputs = inputs.sigmoid()\n",
    "    inputs = inputs.flatten(1).unsqueeze(1)\n",
    "    targets = targets.flatten(1).unsqueeze(0)\n",
    "    numerator = 2 * (inputs * targets).sum(2)\n",
    "    denominator = inputs.sum(-1) + targets.sum(-1)\n",
    "\n",
    "    # NOTE coef doesn't be subtracted to 1 as it is not necessary for computing costs\n",
    "    coef = (numerator + 1) / (denominator + 1)\n",
    "    return coef\n",
    "\n",
    "\n",
    "def dice_loss(inputs, targets, num_boxes):\n",
    "    \"\"\"\n",
    "    Compute the DICE loss, similar to generalized IOU for masks\n",
    "    Args:\n",
    "        inputs: A float tensor of arbitrary shape.\n",
    "                The predictions for each example.\n",
    "        targets: A float tensor with the same shape as inputs. Stores the binary\n",
    "                 classification label for each element in inputs\n",
    "                (0 for the negative class and 1 for the positive class).\n",
    "    \"\"\"\n",
    "    inputs = inputs.sigmoid()\n",
    "    inputs = inputs.flatten(1)\n",
    "    numerator = 2 * (inputs * targets).sum(1)\n",
    "    denominator = inputs.sum(-1) + targets.sum(-1)\n",
    "    loss = 1 - (numerator + 1) / (denominator + 1)\n",
    "    return loss.sum() / num_boxes\n",
    "\n",
    "\n",
    "def sigmoid_focal_coef(inputs, targets, alpha: float = 0.25, gamma: float = 2):\n",
    "    N, M = len(inputs), len(targets)\n",
    "    inputs = inputs.flatten(1).unsqueeze(1).expand(-1, M, -1)\n",
    "    targets = targets.flatten(1).unsqueeze(0).expand(N, -1, -1)\n",
    "\n",
    "    prob = inputs.sigmoid()\n",
    "    ce_loss = F.binary_cross_entropy_with_logits(\n",
    "        inputs, targets, reduction=\"none\")\n",
    "    p_t = prob * targets + (1 - prob) * (1 - targets)\n",
    "    coef = ce_loss * ((1 - p_t) ** gamma)\n",
    "\n",
    "    if alpha >= 0:\n",
    "        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
    "        coef = alpha_t * coef\n",
    "\n",
    "    return coef.mean(2)\n",
    "\n",
    "\n",
    "def sigmoid_focal_loss(inputs, targets, num_boxes, alpha: float = 0.25, gamma: float = 2):\n",
    "    \"\"\"\n",
    "    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n",
    "    Args:\n",
    "        inputs: A float tensor of arbitrary shape.\n",
    "                The predictions for each example.\n",
    "        targets: A float tensor with the same shape as inputs. Stores the binary\n",
    "                 classification label for each element in inputs\n",
    "                (0 for the negative class and 1 for the positive class).\n",
    "        alpha: (optional) Weighting factor in range (0,1) to balance\n",
    "                positive vs negative examples. Default = -1 (no weighting).\n",
    "        gamma: Exponent of the modulating factor (1 - p_t) to\n",
    "               balance easy vs hard examples.\n",
    "    Returns:\n",
    "        Loss tensor\n",
    "    \"\"\"\n",
    "    prob = inputs.sigmoid()\n",
    "    ce_loss = F.binary_cross_entropy_with_logits(\n",
    "        inputs, targets, reduction=\"none\")\n",
    "    p_t = prob * targets + (1 - prob) * (1 - targets)\n",
    "    loss = ce_loss * ((1 - p_t) ** gamma)\n",
    "\n",
    "    if alpha >= 0:\n",
    "        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
    "        loss = alpha_t * loss\n",
    "\n",
    "    return loss.mean(1).sum() / num_boxes\n",
    "\n",
    "\n",
    "\n",
    "class HungarianMatcherIFC(nn.Module):\n",
    "    \"\"\"This class computes an assignment between the targets and the predictions of the network\n",
    "\n",
    "    For efficiency reasons, the targets don't include the no_object. Because of this, in general,\n",
    "    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,\n",
    "    while the others are un-matched (and thus treated as non-objects).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cost_class: float = 1,\n",
    "        cost_dice: float = 1,\n",
    "        num_classes: int = 80,\n",
    "    ):\n",
    "        \"\"\"Creates the matcher\n",
    "\n",
    "        Params:\n",
    "            cost_class: This is the relative weight of the classification error in the matching cost\n",
    "            cost_mask: This is the relative weight of the sigmoid_focal error of the masks in the matching cost\n",
    "            cost_dice: This is the relative weight of the dice loss of the masks in the matching cost\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.cost_class = cost_class\n",
    "        self.cost_dice = cost_dice\n",
    "        assert cost_class != 0 or cost_dice != 0, \"all costs cant be 0\"\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_cum_classes = [0] + \\\n",
    "            np.cumsum(np.array(num_classes) + 1).tolist()\n",
    "        self.n_future = 4\n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs, targets):\n",
    "        # We flatten to compute the cost matrices in a batch\n",
    "        out_prob = outputs[\"pred_logits\"].softmax(-1)\n",
    "        out_mask = outputs[\"pred_masks\"]\n",
    "        B, Q, T, s_h, s_w = out_mask.shape\n",
    "        t_h, t_w = targets[0][\"match_masks\"].shape[-2:]\n",
    "\n",
    "        if (s_h, s_w) != (t_h, t_w):\n",
    "            out_mask = out_mask.reshape(B, Q*T, s_h, s_w)\n",
    "            out_mask = torch.nn.F.interpolate(out_mask, size=(\n",
    "                t_h, t_w), mode=\"bilinear\", align_corners=False)\n",
    "            out_mask = out_mask.view(B, Q, T, t_h, t_w)\n",
    "\n",
    "        indices = []\n",
    "        for b_i in range(B):\n",
    "            b_tgt_ids = targets[b_i][\"labels\"]\n",
    "            b_out_prob = out_prob[b_i]\n",
    "\n",
    "            cost_class = b_out_prob[:, b_tgt_ids]\n",
    "\n",
    "            b_tgt_mask = targets[b_i][\"match_masks\"].unsqueeze(0)\n",
    "            b_out_mask = out_mask[b_i]\n",
    "\n",
    "            # Compute the dice coefficient cost between masks\n",
    "            # The 1 is a constant that doesn't change the matching as cost_class, thus omitted.\n",
    "            \n",
    "            cost_dice = dice_coef(\n",
    "                b_out_mask, b_tgt_mask\n",
    "            ).to(cost_class)\n",
    "\n",
    "            # Final cost matrix\n",
    "            C = self.cost_dice * cost_dice + self.cost_class * cost_class\n",
    "\n",
    "            indices.append(linear_sum_assignment(C.cpu(), maximize=True))\n",
    "\n",
    "        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    if target.numel() == 0:\n",
    "        return [torch.zeros([], device=output.device)]\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "class SetCriterion(nn.Module):\n",
    "    \"\"\" This class computes the loss for IFC.\n",
    "    The process happens in two steps:\n",
    "        1) we compute hungarian assignment between ground truth masks and the outputs of the model\n",
    "        2) we supervise each pair of matched ground-truth / prediction (supervise class and mask)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, matcher, weight_dict, eos_coef, losses, num_frames):\n",
    "        \"\"\" Create the criterion.\n",
    "        Parameters:\n",
    "            num_classes: number of object categories, omitting the special no-object category\n",
    "            matcher: module able to compute a matching between targets and proposals\n",
    "            weight_dict: dict containing as key the names of the losses and as values their relative weight.\n",
    "            eos_coef: relative classification weight applied to the no-object category\n",
    "            losses: list of all the losses to be applied. See get_loss for list of available losses.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.matcher = matcher\n",
    "        self.weight_dict = weight_dict\n",
    "        self.eos_coef = eos_coef\n",
    "        self.losses = losses\n",
    "        self.num_frames = num_frames\n",
    "        empty_weight = torch.ones(num_classes + 1)\n",
    "        empty_weight[-1] = self.eos_coef\n",
    "        self.register_buffer('empty_weight', empty_weight)\n",
    "\n",
    "    def loss_labels(self, outputs, targets, indices, num_masks, log=True):\n",
    "        \"\"\"Classification loss (NLL)\n",
    "        targets dicts must contain the key \"labels\" containing a tensor of dim [nb_target_masks]\n",
    "        \"\"\"\n",
    "        src_logits = outputs['pred_logits']\n",
    "\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        target_classes_o = torch.cat([t[\"labels\"][J]\n",
    "                                     for t, (_, J) in zip(targets, indices)])\n",
    "        target_classes = torch.full(src_logits.shape[:2], self.num_classes,\n",
    "                                    dtype=torch.int64, device=src_logits.device)\n",
    "        target_classes[idx] = target_classes_o\n",
    "\n",
    "        loss_ce = F.cross_entropy(src_logits.transpose(\n",
    "            1, 2), target_classes, self.empty_weight)\n",
    "        losses = {'loss_ce': loss_ce}\n",
    "\n",
    "        if log:\n",
    "            # TODO this should probably be a separate loss, not hacked in this one here\n",
    "            losses['class_error'] = 100 - \\\n",
    "                accuracy(src_logits[idx], target_classes_o)[0]\n",
    "        return losses\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def loss_cardinality(self, outputs, targets, indices, num_masks):\n",
    "        \"\"\" Compute the cardinality error, ie the absolute error in the number of predicted non-empty boxes\n",
    "        This is not really a loss, it is intended for logging purposes only. It doesn't propagate gradients\n",
    "        \"\"\"\n",
    "        pred_logits = outputs['pred_logits']\n",
    "        device = pred_logits.device\n",
    "        tgt_lengths = torch.as_tensor(\n",
    "            [len(v[\"labels\"]) for v in targets], device=device)\n",
    "        # Count the number of predictions that are NOT \"no-object\" (which is the last class)\n",
    "        card_pred = (pred_logits.argmax(-1) !=\n",
    "                     pred_logits.shape[-1] - 1).sum(1)\n",
    "        card_err = F.l1_loss(card_pred.float(), tgt_lengths.float())\n",
    "        losses = {'cardinality_error': card_err}\n",
    "        return losses\n",
    "\n",
    "    def loss_masks(self, outputs, targets, indices, num_masks):\n",
    "        \"\"\"Compute the losses related to the masks: the focal loss and the dice loss.\n",
    "           targets dicts must contain the key \"masks\" containing a tensor of dim [nb_target_masks, h, w]\n",
    "        \"\"\"\n",
    "        assert \"pred_masks\" in outputs\n",
    "\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        src_masks = outputs[\"pred_masks\"][idx]\n",
    "        target_masks = torch.cat(\n",
    "            [t['masks'][i] for t, (_, i) in zip(targets, indices)]).to(src_masks)\n",
    "\n",
    "        n, t = src_masks.shape[:2]\n",
    "        t_h, t_w = target_masks.shape[-2:]\n",
    "\n",
    "        src_masks = F.interpolate(src_masks, size=(\n",
    "            t_h, t_w), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        src_masks = src_masks.flatten(1)\n",
    "        target_masks = target_masks.flatten(1)\n",
    "\n",
    "        losses = {\n",
    "            \"loss_mask\": sigmoid_focal_loss(src_masks, target_masks, num_masks),\n",
    "            \"loss_dice\": dice_loss(src_masks, target_masks, num_masks),\n",
    "        }\n",
    "        return losses\n",
    "\n",
    "    def _get_src_permutation_idx(self, indices):\n",
    "        # permute predictions following indices\n",
    "        batch_idx = torch.cat([torch.full_like(src, i)\n",
    "                              for i, (src, _) in enumerate(indices)])\n",
    "        src_idx = torch.cat([src for (src, _) in indices])\n",
    "        return batch_idx, src_idx\n",
    "\n",
    "    def _get_tgt_permutation_idx(self, indices):\n",
    "        # permute targets following indices\n",
    "        batch_idx = torch.cat([torch.full_like(tgt, i)\n",
    "                              for i, (_, tgt) in enumerate(indices)])\n",
    "        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n",
    "        return batch_idx, tgt_idx\n",
    "\n",
    "    def get_loss(self, loss, outputs, targets, indices, num_masks, **kwargs):\n",
    "        loss_map = {\n",
    "            'labels': self.loss_labels,\n",
    "            'cardinality': self.loss_cardinality,\n",
    "            'masks': self.loss_masks\n",
    "        }\n",
    "        assert loss in loss_map, f'do you really want to compute {loss} loss?'\n",
    "        return loss_map[loss](outputs, targets, indices, num_masks, **kwargs)\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\" This performs the loss computation.\n",
    "        Parameters:\n",
    "             outputs: dict of tensors, see the output specification of the model for the format\n",
    "             targets: list of dicts, such that len(targets) == batch_size.\n",
    "                      The expected keys in each dict depends on the losses applied, see each loss' doc\n",
    "        \"\"\"\n",
    "        outputs_without_aux = {k: v for k,\n",
    "                               v in outputs.items() if k != 'aux_outputs'}\n",
    "\n",
    "        # Retrieve the matching between the outputs of the last layer and the targets\n",
    "        indices = self.matcher(outputs_without_aux, targets)\n",
    "\n",
    "        # Compute the average number of target masks accross all nodes, for normalization purposes\n",
    "        num_masks = sum(len(t[\"labels\"]) for t in targets)\n",
    "        num_masks = torch.as_tensor(\n",
    "            [num_masks], dtype=torch.float, device=next(iter(outputs.values())).device)\n",
    "        # if is_dist_avail_and_initialized():\n",
    "        #     torch.distributed.all_reduce(num_masks)\n",
    "        num_masks = torch.clamp(num_masks / 1, min=1).item()\n",
    "\n",
    "        # Compute all the requested losses\n",
    "        losses = {}\n",
    "        for loss in self.losses:\n",
    "            losses.update(self.get_loss(\n",
    "                loss, outputs_without_aux, targets, indices, num_masks))\n",
    "\n",
    "        # In case of auxiliary losses, we repeat this process with the output of each intermediate layer.\n",
    "        if 'aux_outputs' in outputs:\n",
    "            for i, aux_outputs in enumerate(outputs['aux_outputs']):\n",
    "                indices = self.matcher(aux_outputs, targets)\n",
    "                for loss in self.losses:\n",
    "                    kwargs = {}\n",
    "                    if loss == 'labels':\n",
    "                        # Logging is enabled only for the last layer\n",
    "                        kwargs = {'log': False}\n",
    "                    l_dict = self.get_loss(\n",
    "                        loss, aux_outputs, targets, indices, num_masks, **kwargs)\n",
    "                    l_dict = {k + f'_{i}': v for k, v in l_dict.items()}\n",
    "                    losses.update(l_dict)\n",
    "\n",
    "        return losses\n",
    "\n",
    "\n",
    "\n",
    "class MaskHeadSmallConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple convolutional head, using group norm.\n",
    "    Upsampling is done using a FPN approach\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, fpn_dims, context_dim, output_dict=None):\n",
    "        super().__init__()\n",
    "\n",
    "        inter_dims = [dim, context_dim // 2, context_dim // 4,\n",
    "                      context_dim // 8, context_dim // 16, context_dim // 64, context_dim // 128]\n",
    "\n",
    "        gn = 8\n",
    "\n",
    "        self.lay1 = torch.nn.Conv2d(dim, dim, 3, padding=1)\n",
    "        self.gn1 = torch.nn.GroupNorm(gn, dim)\n",
    "        self.lay2 = torch.nn.Conv2d(dim, inter_dims[1], 3, padding=1)\n",
    "        self.gn2 = torch.nn.GroupNorm(gn, inter_dims[1])\n",
    "        self.lay3 = torch.nn.Conv2d(inter_dims[1], inter_dims[2], 3, padding=1)\n",
    "        self.gn3 = torch.nn.GroupNorm(gn, inter_dims[2])\n",
    "        self.lay4 = torch.nn.Conv2d(inter_dims[2], inter_dims[3], 3, padding=1)\n",
    "        self.gn4 = torch.nn.GroupNorm(gn, inter_dims[3])\n",
    "        self.lay5 = torch.nn.Conv2d(inter_dims[3], inter_dims[4], 3, padding=1)\n",
    "        self.gn5 = torch.nn.GroupNorm(gn, inter_dims[4])\n",
    "\n",
    "        self.lay6 = torch.nn.Conv2d(inter_dims[4], inter_dims[4], 3, padding=1)\n",
    "        self.gn6 = torch.nn.GroupNorm(gn, inter_dims[4])\n",
    "\n",
    "        self.out_lay = torch.nn.Conv2d(\n",
    "            inter_dims[4], 1, 3, padding=1)  # <- This would be differen\n",
    "\n",
    "        # if output_dict is not None:\n",
    "        #     self.future_pred_layers = build_output_convs(\n",
    "        #         inter_dims[4], output_dict)\n",
    "        \"\"\" \n",
    "        outheads_\n",
    "            - motion_segmentation: 1x5x200x200   - BxFx1xHxW\n",
    "        \"\"\"\n",
    "\n",
    "        self.dim = dim\n",
    "\n",
    "        self.adapter1 = torch.nn.Conv2d(fpn_dims[0], inter_dims[1], 1)\n",
    "        self.adapter2 = torch.nn.Conv2d(fpn_dims[1], inter_dims[2], 1)\n",
    "        self.adapter3 = torch.nn.Conv2d(fpn_dims[2], inter_dims[3], 1)\n",
    "        self.adapter4 = torch.nn.Conv2d(fpn_dims[3], inter_dims[4], 1)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_uniform_(m.weight, a=1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, bbox_mask, fpns):\n",
    "\n",
    "        def expand(tensor, length):\n",
    "            return tensor.unsqueeze(1).repeat(1, int(length), 1, 1, 1).flatten(0, 1)\n",
    "        print(f\"Input: {x.shape = } {bbox_mask.shape = }\")\n",
    "        x = torch.cat([expand(x, bbox_mask.shape[1]),\n",
    "                      bbox_mask.flatten(0, 1)], 1)\n",
    "        print(f\"First Expand: {x.shape = }\")\n",
    "        x = self.lay1(x)\n",
    "        x = self.gn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.lay2(x)\n",
    "        x = self.gn2(x)\n",
    "        x = F.relu(x)\n",
    "        #print(f\"Before adapter1: {x.shape = }\")\n",
    "        cur_fpn = self.adapter1(fpns[0])\n",
    "        #print(f\"First cur_fpn: {cur_fpn.shape = }\")\n",
    "        if cur_fpn.size(0) != x.size(0):\n",
    "            cur_fpn = expand(cur_fpn, x.size(0) / cur_fpn.size(0))\n",
    "            #print(f\"cur_fpn.size(0) != x.size(0): {cur_fpn.shape = }\")\n",
    "        x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "        #print(f\"Interpolutaion with expan: {x.shape = }\")\n",
    "        x = self.lay3(x)\n",
    "        x = self.gn3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        #print(f\"Before adapter2: {x.shape = }\")\n",
    "        cur_fpn = self.adapter2(fpns[1])\n",
    "        #print(f\"2 adapter2: {cur_fpn.shape = }\")\n",
    "        if cur_fpn.size(0) != x.size(0):\n",
    "            cur_fpn = expand(cur_fpn, x.size(0) / cur_fpn.size(0))\n",
    "            #print(f\"cur_fpn.size(0) != x.size(0): {cur_fpn.shape = }\")\n",
    "\n",
    "        x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "        #print(f\"Interpolutaion with expan: {x.shape = }\")\n",
    "        x = self.lay4(x)\n",
    "        x = self.gn4(x)\n",
    "        x = F.relu(x)\n",
    "        #print(f\"TBefore adapter3: {x.shape = }\")\n",
    "\n",
    "        cur_fpn = self.adapter3(fpns[2])\n",
    "        #print(f\"after adapter3: {cur_fpn.shape = }\")\n",
    "        if cur_fpn.size(0) != x.size(0):\n",
    "            cur_fpn = expand(cur_fpn, x.size(0) / cur_fpn.size(0))\n",
    "            #print(f\"cur_fpn.size(0) != x.size(0): {cur_fpn.shape = }\")\n",
    "        x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "        #print(f\"Interpolutaion with expan: {x.shape = }\")\n",
    "        x = self.lay5(x)\n",
    "        x = self.gn5(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        #print(f\"Fourth Expand: {x.shape = }\")\n",
    "        cur_fpn = self.adapter4(fpns[3])\n",
    "        #print(f\"after adapter4: {cur_fpn.shape = }\")\n",
    "        if cur_fpn.size(0) != x.size(0):\n",
    "            cur_fpn = expand(cur_fpn, x.size(0) / cur_fpn.size(0))\n",
    "            #print(f\"cur_fpn.size(0) != x.size(0): {cur_fpn.shape = }\")\n",
    "        x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "        #print(f\"Interpolutaion with expan: {x.shape = }\")\n",
    "\n",
    "        #print(f\"Interpolutaion with expan: {x.shape = }\")\n",
    "        x = self.lay6(x)\n",
    "        x = self.gn6(x)\n",
    "        x = F.relu(x)\n",
    "        print(f\"Fourth Expand: {x.shape = }\")\n",
    "        #x = F.interpolate(x, size=200, mode=\"nearest\")\n",
    "        x = self.out_lay(x)\n",
    "        print(f\"out {x.shape}\")\n",
    "        return x\n",
    "\n",
    "\n",
    "class MHAttentionMap(nn.Module):\n",
    "    \"\"\"This is a 2D attention module, which only returns the attention softmax (no multiplication by value)\"\"\"\n",
    "\n",
    "    def __init__(self, query_dim, hidden_dim, num_heads, dropout=0, bias=True):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.q_linear = nn.Linear(query_dim, hidden_dim, bias=bias)\n",
    "        self.k_linear = nn.Linear(query_dim, hidden_dim, bias=bias)\n",
    "\n",
    "        nn.init.zeros_(self.k_linear.bias)\n",
    "        nn.init.zeros_(self.q_linear.bias)\n",
    "        nn.init.xavier_uniform_(self.k_linear.weight)\n",
    "        nn.init.xavier_uniform_(self.q_linear.weight)\n",
    "        self.normalize_fact = float(hidden_dim / self.num_heads) ** -0.5\n",
    "\n",
    "    def forward(self, q, k, mask=None):\n",
    "        q = self.q_linear(q)\n",
    "        k = F.conv2d(\n",
    "            k, self.k_linear.weight.unsqueeze(-1).unsqueeze(-1), self.k_linear.bias)\n",
    "        qh = q.view(q.shape[0], q.shape[1], self.num_heads,\n",
    "                    self.hidden_dim // self.num_heads)\n",
    "        kh = k.view(k.shape[0], self.num_heads, self.hidden_dim //\n",
    "                    self.num_heads, k.shape[-2], k.shape[-1])\n",
    "        weights = torch.einsum(\"bqnc,bnchw->bqnhw\",\n",
    "                               qh * self.normalize_fact, kh)\n",
    "\n",
    "        if mask is not None:\n",
    "            weights.masked_fill_(mask.unsqueeze(1).unsqueeze(1), float(\"-inf\"))\n",
    "        weights = F.softmax(weights.flatten(2), dim=-1).view_as(weights)\n",
    "        weights = self.dropout(weights)\n",
    "        #print(f\"MH AttentionMap Shape {weights.shape = }\")\n",
    "        return weights\n",
    "\n",
    "\n",
    "class PostProcessSegm(nn.Module):\n",
    "    def __init__(self, threshold=0.5):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, results, outputs, orig_target_sizes, max_target_sizes):\n",
    "        assert len(orig_target_sizes) == len(max_target_sizes)\n",
    "        max_h, max_w = max_target_sizes.max(0)[0].tolist()\n",
    "        #print(f\"{max_h = }, {max_w = }\")\n",
    "        out_logits, out_bbox = outputs['pred_logits'], outputs['pred_boxes']\n",
    "\n",
    "        assert len(out_logits) == len(orig_target_sizes)\n",
    "        assert orig_target_sizes.shape[1] == 2\n",
    "\n",
    "        prob = out_logits.sigmoid()\n",
    "        topk_values, topk_indexes = torch.topk(\n",
    "            prob.view(out_logits.shape[0], -1), 100, dim=1)\n",
    "        scores = topk_values\n",
    "        topk_boxes = topk_indexes // out_logits.shape[2]\n",
    "        labels = topk_indexes % out_logits.shape[2]\n",
    "        boxes = box_cxcywh_to_xyxy(out_bbox)\n",
    "        boxes = torch.gather(\n",
    "            boxes, 1, topk_boxes.unsqueeze(-1).repeat(1, 1, 4))\n",
    "\n",
    "        # and from relative [0, 1] to absolute [0, height] coordinates\n",
    "        img_h, img_w = orig_target_sizes.unbind(1)\n",
    "\n",
    "        #print(f\"{img_h = }, {img_w = }\")\n",
    "        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n",
    "        #print(f\" {scale_fct.shape = }\")\n",
    "        boxes = boxes * scale_fct[:, None, :]\n",
    "        #print(f\" {boxes.shape = }\")\n",
    "        out_mask = outputs[\"pred_masks\"]\n",
    "        #print(f\" {out_mask.shape = }\")\n",
    "        B, R, H, W = out_mask.shape\n",
    "        out_mask = out_mask.view(B, R, H * W)\n",
    "        #print(f\" {out_mask.shape = }\")\n",
    "        out_mask = torch.gather(\n",
    "            out_mask, 1, topk_boxes.unsqueeze(-1).repeat(1, 1, H * W))\n",
    "        #print(f\"After gather {out_mask.shape = }\")\n",
    "        outputs_masks = out_mask.view(B, 100, H, W).squeeze(2)\n",
    "\n",
    "        outputs_masks = F.interpolate(outputs_masks, size=(\n",
    "            max_h, max_w), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        outputs_masks = (outputs_masks.sigmoid() > self.threshold).cpu()\n",
    "\n",
    "        for i, (cur_mask, t, tt) in enumerate(zip(outputs_masks, max_target_sizes, orig_target_sizes)):\n",
    "            img_h, img_w = t[0], t[1]\n",
    "            results[i][\"masks\"] = cur_mask[:, :img_h, :img_w].unsqueeze(1)\n",
    "            interpol_tmp = F.interpolate(\n",
    "                results[i][\"masks\"].float(), size=tuple(tt.tolist()), mode=\"nearest\"\n",
    "            )\n",
    "\n",
    "            results[i][\"masks\"] = interpol_tmp.byte()\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "# \"\"\" \n",
    "# 1. Get Code and shapes of in-/output  -- \n",
    "# 2. Get Matcher for the masks as well as postprocessing & loss function\n",
    "# 3. Test based on real GTs \n",
    "# \"\"\"\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\" Very simple multi-layer perceptron (also called FFN)\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        h = [hidden_dim] * (num_layers - 1)\n",
    "        self.layers = nn.ModuleList(nn.Linear(n, k)\n",
    "                                    for n, k in zip([input_dim] + h, h + [output_dim]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class depthwise_separable_conv(nn.Module):\n",
    "    def __init__(self, nin, nout,padding=1,kernel_size=5, activation1= None,activation2=None):\n",
    "        super(depthwise_separable_conv, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(nin, nin , kernel_size=kernel_size, padding=padding, groups=nin)\n",
    "        self.pointwise = nn.Conv2d(nin , nout, kernel_size=1)\n",
    "        self.activation1 = activation1\n",
    "        self.activation2 = activation2\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        if self.activation1 is not None:\n",
    "            out = self.activation1(out)\n",
    "        out = self.pointwise(out)\n",
    "        if self.activation1 is not None:\n",
    "            out = self.activation2(out)\n",
    "        return out\n",
    "\n",
    "class MaskHeadSmallConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple convolutional head, using group norm.\n",
    "    Upsampling is done using a FPN approach\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, fpn_dims, output_dict=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # inter_dims = [dim, context_dim // 2, context_dim // 4,\n",
    "        #               context_dim // 8, context_dim // 16, context_dim // 64, context_dim // 128]\n",
    "        self.n_future = 4 \n",
    "        gn = 8\n",
    "         \n",
    "        self.lay1 = torch.nn.Conv2d(dim, dim, 3, padding=1)\n",
    "        self.gn1 = torch.nn.GroupNorm(gn, dim)\n",
    "        self.lay2 = torch.nn.Conv2d(dim, dim, 3, padding=1)\n",
    "        self.gn2 = torch.nn.GroupNorm(gn, dim)\n",
    "        self.lay3 = torch.nn.Conv2d(dim, dim, 3, padding=1)\n",
    "        self.gn3 = torch.nn.GroupNorm(gn, dim)\n",
    "        self.lay4 = torch.nn.Conv2d(dim, dim, 3, padding=1)\n",
    "        self.gn4 = torch.nn.GroupNorm(gn, dim)\n",
    "        self.lay5 = torch.nn.Conv2d(dim, dim, 3, padding=1)\n",
    "        self.gn5 = torch.nn.GroupNorm(gn, dim)\n",
    "        \n",
    "        self.lay6 = torch.nn.Conv2d(dim, dim, 3, padding=1)\n",
    "        self.gn6 = torch.nn.GroupNorm(gn, dim)\n",
    "\n",
    "        self.depth_sep_conv2d =  depthwise_separable_conv(dim,dim,kernel_size=5,padding=2, activation1= F.relu,activation2= F.relu)\n",
    "\n",
    "        # half_dim = dim/2     \n",
    "        # self.out_lay_1 = torch.nn.Conv2d(\n",
    "        #     dim, half_dim, 3, padding=1)\n",
    "        # self.out_lay_2 = torch.nn.Conv2d(\n",
    "        #     half_dim, 1, 3, padding=1)  # <- This would be differen\n",
    "        \n",
    "        self.convert_to_weight = MLP(dim, dim, dim, 3)\n",
    "        # if output_dict is not None:\n",
    "        #     self.future_pred_layers = build_output_convs(\n",
    "        #         inter_dims[4], output_dict)\n",
    "        \"\"\" \n",
    "        outheads_\n",
    "            - motion_segmentation: 1x5x200x200   - BxFx1xHxW\n",
    "        \"\"\"\n",
    "\n",
    "        self.dim = dim\n",
    "\n",
    "        self.adapter1 = torch.nn.Conv2d(fpn_dims[0], dim, 1)\n",
    "        self.adapter2 = torch.nn.Conv2d(fpn_dims[1], dim, 1)\n",
    "        self.adapter3 = torch.nn.Conv2d(fpn_dims[2], dim, 1)\n",
    "        self.adapter4 = torch.nn.Conv2d(fpn_dims[3], dim, 1)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_uniform_(m.weight, a=1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, src, seg_memory, fpns, hs ):\n",
    "        x = src + seg_memory\n",
    "        x = self.lay1(x)\n",
    "        x = self.gn1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        cur_fpn = self.adapter1(fpns[0])\n",
    "        x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "        x = self.lay2(x)\n",
    "        x = self.gn2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        cur_fpn = self.adapter2(fpns[1])\n",
    "        x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "        x = self.lay3(x)\n",
    "        x = self.gn3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        cur_fpn = self.adapter3(fpns[2])\n",
    "        x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "        #print(f\"Interpolutaion with expan: {x.shape = }\")\n",
    "        x = self.lay4(x)\n",
    "        x = self.gn4(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        cur_fpn = self.adapter4(fpns[3])\n",
    "        x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "        x = self.lay5(x)\n",
    "        x = self.gn5(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        T = self.n_future\n",
    "\n",
    "        x = x.unsqueeze(1).repeat(1,T,1,1,1)\n",
    "        B, BT, C, H, W = x.shape\n",
    "        L, B, N, C = hs.shape\n",
    "        x = self.depth_sep_conv2d(x.view(B*BT, C , H,W)).view(B,BT,C,H,W)\n",
    "\n",
    "        w = self.convert_to_weight(hs).permute(1,0,2,3)\n",
    "        w = w.unsqueeze(1).repeat(1,4,1,1,1)\n",
    "\n",
    "        mask_logits = F.conv2d(x.view(1, BT*C, H, W), w.reshape(B*T*L*N, C, 1, 1), groups=BT)\n",
    "        mask_logits = mask_logits.view(B, T, L, N, H, W).permute(2, 0, 3, 1, 4, 5)\n",
    "        return mask_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_future = 4\n",
    "hidden_dim = 256\n",
    "nheads = 8\n",
    "num_queries = 100\n",
    "\n",
    "gt_instance = torch.randint(low=0,high=2, size=(1, n_future, 200, 200)).to(torch.float32)\n",
    "seg_memory = torch.rand((1, hidden_dim, 13, 13))\n",
    "seg_mask = torch.randint(low=0, high=1, size=(1, 13, 13))\n",
    "hs = torch.rand([3, 1, num_queries, hidden_dim]) # N x B X Q x H <. N layers , B batchsize, query dim , hidden \n",
    "init_reference = torch.rand([1, num_queries, 2])\n",
    "srcs = torch.rand([1, hidden_dim, 13, 13])\n",
    "\n",
    "features = [\n",
    "    torch.rand((1, 64, 100, 100)),\n",
    "    torch.rand((1, 128, 50, 50)),\n",
    "    torch.rand((1, 256, 25, 25)),\n",
    "    torch.rand((1, 512, 13, 13)),\n",
    "]\n",
    "\n",
    "input_projections = [(features[-1]),\n",
    "                     (features[-2]), (features[-3]), features[-4]]\n",
    "\n",
    "\n",
    "\n",
    "bbox_attention = (MHAttentionMap(\n",
    "    hidden_dim, hidden_dim, nheads, dropout=0))\n",
    "\n",
    "fpn_dims = [512, 256, 128, 64]\n",
    "\n",
    "\n",
    "\n",
    "class_mlp = MLP(hidden_dim, hidden_dim, output_dim=num_queries, num_layers=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 100, 100])\n"
     ]
    }
   ],
   "source": [
    "features = [ # with input projection\n",
    "    torch.rand((1, 256, 100, 100)),\n",
    "    torch.rand((1, 256, 50, 50)),\n",
    "    torch.rand((1, 256, 25, 25)),\n",
    "    torch.rand((1, 256, 13, 13)),\n",
    "]\n",
    "input_projections = [(features[-1]),\n",
    "                     (features[-2]), (features[-3]), features[-4]]\n",
    "#\n",
    "fpn_dims = [256, 256, 256, 256]\n",
    "\n",
    "\n",
    "def _set_aux_loss( outputs_class, outputs_masks):\n",
    "    # this is a workaround to make torchscript happy, as torchscript\n",
    "    # doesn't support dictionary with non-homogeneous values, such\n",
    "    # as a dict having both a Tensor and a list.\n",
    "    return [{'pred_logits': a, 'pred_masks': b}\n",
    "            for a, b in zip(outputs_class[:-1], outputs_masks[:-1])]\n",
    "aux_loss = True \n",
    "class_logits_list = []\n",
    "for i in range(n_future):\n",
    "    class_logits_list.append( class_mlp(hs[-1]))\n",
    "\n",
    "outputs_class = torch.stack(class_logits_list)\n",
    "print(outputs_class.shape)\n",
    "mask_head = MaskHeadSmallConv(hidden_dim,fpn_dims)\n",
    "\n",
    "outputs_masks = mask_head(\n",
    "        srcs, seg_memory, input_projections,hs)\n",
    "\n",
    "\n",
    "out = {'pred_logits': outputs_class[-1]}\n",
    "out.update({'pred_masks': outputs_masks[-1]})\n",
    "\n",
    "if aux_loss:\n",
    "    out['aux_outputs'] = _set_aux_loss(outputs_class, outputs_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 1, 1, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [49], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m temp \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(MaxID)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(temp\u001b[39m.\u001b[39mshape)\n\u001b[0;32m----> 6\u001b[0m Bool_masks  \u001b[39m=\u001b[39m (temp\u001b[39m==\u001b[39;49m Target\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m1\u001b[39;49m))\u001b[39m.\u001b[39mto(\u001b[39mint\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "MaxID = 10 \n",
    "B = 2\n",
    "Target =torch.randint(low=0, high=high,size=(B,3,5,5)).to(torch.float32) \n",
    "temp = torch.arange(MaxID).unsqueeze(0).repeat(B, 1).unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "Bool_masks  = (temp== Target.unsqueeze(1)).float() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue GN does not work with 192 Hidden Dim -> maybe just lin reprojectoin layer \n",
    "IFC skips BBox Attn and predicts class -> for me kinda irrelevant since only one class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 50 \n",
    "num_frames = 4\n",
    "dice_weight=3.0\n",
    "mask_weight=3.0\n",
    "no_object_weight = 0.1 \n",
    "deep_supervision = True\n",
    "dec_layers = 3\n",
    "\n",
    "\n",
    "matcher = HungarianMatcherIFC(\n",
    "    cost_class=1,\n",
    "    cost_dice=dice_weight,\n",
    "    num_classes=num_classes,\n",
    "    )\n",
    "weight_dict = {\"loss_ce\": 1, \"loss_mask\": mask_weight,\n",
    "                \"loss_dice\": dice_weight}\n",
    "if deep_supervision:\n",
    "    aux_weight_dict = {}\n",
    "    for i in range(dec_layers - 1):\n",
    "        aux_weight_dict.update(\n",
    "            {k + f\"_{i}\": v for k, v in weight_dict.items()})\n",
    "    weight_dict.update(aux_weight_dict)\n",
    "losses = [\"labels\", \"masks\", \"cardinality\"]\n",
    "criterion = SetCriterion(\n",
    "    num_classes, matcher=matcher, weight_dict=weight_dict, eos_coef=no_object_weight, losses=losses,\n",
    "    num_frames=num_frames\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects.mmdet3d_plugin\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch.nn.functional as F \n",
    "\n",
    "from projects.mmdet3d_plugin.datasets.utils.warper import FeatureWarper\n",
    "import os\n",
    "from mmdet.datasets import (build_dataloader, build_dataset,\n",
    "                            replace_ImageToTensor)\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from mmcv import Config\n",
    "\n",
    "\n",
    "def import_modules_load_config(cfg_file=\"beverse_tiny.py\", samples_per_gpu=1):\n",
    "    cfg_path = r\"/home/niklas/ETM_BEV/BEVerse/projects/configs\"\n",
    "    cfg_path = os.path.join(cfg_path, cfg_file)\n",
    "\n",
    "    cfg = Config.fromfile(cfg_path)\n",
    "\n",
    "    # if args.cfg_options is not None:\n",
    "    #     cfg.merge_from_dict(args.cfg_options)\n",
    "    # import modules from string list.\n",
    "    if cfg.get(\"custom_imports\", None):\n",
    "        from mmcv.utils import import_modules_from_strings\n",
    "\n",
    "        import_modules_from_strings(**cfg[\"custom_imports\"])\n",
    "\n",
    "    # import modules from plguin/xx, registry will be updated\n",
    "    if hasattr(cfg, \"plugin\"):\n",
    "        if cfg.plugin:\n",
    "            import importlib\n",
    "\n",
    "            if hasattr(cfg, \"plugin_dir\"):\n",
    "                plugin_dir = cfg.plugin_dir\n",
    "                _module_dir = os.path.dirname(plugin_dir)\n",
    "                _module_dir = _module_dir.split(\"/\")\n",
    "                _module_path = _module_dir[0]\n",
    "\n",
    "                for m in _module_dir[1:]:\n",
    "                    _module_path = _module_path + \".\" + m\n",
    "                print(_module_path)\n",
    "                plg_lib = importlib.import_module(_module_path)\n",
    "            else:\n",
    "                # import dir is the dirpath for the config file\n",
    "                _module_dir = cfg_path\n",
    "                _module_dir = _module_dir.split(\"/\")\n",
    "                _module_path = _module_dir[0]\n",
    "                for m in _module_dir[1:]:\n",
    "                    _module_path = _module_path + \".\" + m\n",
    "                print(_module_path)\n",
    "                plg_lib = importlib.import_module(_module_path)\n",
    "\n",
    "    samples_per_gpu = 1\n",
    "    if isinstance(cfg.data.test, dict):\n",
    "        cfg.data.test.test_mode = True\n",
    "        samples_per_gpu = cfg.data.test.pop(\"samples_per_gpu\", 1)\n",
    "        if samples_per_gpu > 1:\n",
    "            # Replace 'ImageToTensor' to 'DefaultFormatBundle'\n",
    "            cfg.data.test.pipeline = replace_ImageToTensor(\n",
    "                cfg.data.test.pipeline)\n",
    "    elif isinstance(cfg.data.test, list):\n",
    "        for ds_cfg in cfg.data.test:\n",
    "            ds_cfg.test_mode = True\n",
    "        samples_per_gpu = max(\n",
    "            [ds_cfg.pop(\"samples_per_gpu\", 1) for ds_cfg in cfg.data.test]\n",
    "        )\n",
    "        if samples_per_gpu > 1:\n",
    "            for ds_cfg in cfg.data.test:\n",
    "                ds_cfg.pipeline = replace_ImageToTensor(ds_cfg.pipeline)\n",
    "\n",
    "    return cfg\n",
    "\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "cfg = import_modules_load_config(\n",
    "    cfg_file=r\"beverse_tiny_org.py\")\n",
    "\n",
    "dataset = build_dataset(cfg.data.test)\n",
    "data_loader = build_dataloader(\n",
    "    dataset,\n",
    "    samples_per_gpu=2,\n",
    "    workers_per_gpu=cfg.data.workers_per_gpu,\n",
    "    dist=False,\n",
    "    shuffle=False)\n",
    "\n",
    "\n",
    "grid_conf = {\n",
    "    \"xbound\": [-50.0, 50.0, 0.5],\n",
    "    \"ybound\": [-50.0, 50.0, 0.5],\n",
    "    \"zbound\": [-10.0, 10.0, 20.0],\n",
    "    \"dbound\": [1.0, 60.0, 1.0],\n",
    "}\n",
    "\n",
    "warper = FeatureWarper(grid_conf=grid_conf)\n",
    "\n",
    "\n",
    "class pseud_class:\n",
    "    def __init__(self) -> None:\n",
    "        \n",
    "        self.receptive_field = 4\n",
    "        self.warper = FeatureWarper(grid_conf=grid_conf)\n",
    "        \n",
    "    def prepare_targets(self, batch,bev_size = (200,200), mask_stride=2,match_stride=2):\n",
    "        segmentation_labels = batch[\"motion_segmentation\"][0]\n",
    "        gt_instance = batch[\"motion_instance\"][0]\n",
    "        future_egomotion = batch[\"future_egomotions\"][0]\n",
    "        batch_size = len(segmentation_labels)\n",
    "        labels = {}\n",
    "\n",
    "        bev_transform = batch.get(\"aug_transform\", None)\n",
    "        labels[\"img_is_valid\"] = batch.get(\"img_is_valid\", None)\n",
    "\n",
    "        if bev_transform is not None:\n",
    "            bev_transform = bev_transform.float()\n",
    "        #warping so all segmentation labels are inside the current BEV frame FIERY reports better convergence / performance if you do this \n",
    "        # segmentation_labels = (\n",
    "        #     self.warper.cumulative_warp_features_reverse(\n",
    "        #         segmentation_labels.float().unsqueeze(2),\n",
    "        #         future_egomotion[:, (self.receptive_field - 1) :],\n",
    "        #         mode=\"nearest\",\n",
    "        #         bev_transform=bev_transform,\n",
    "        #     )\n",
    "        #     .long()\n",
    "        #     .contiguous()\n",
    "        # ).squeeze().to(torch.float32)\n",
    "        #print(f\"Seg labels shape: {segmentation_labels.shape =}\")\n",
    "\n",
    "        \n",
    "        # Warp instance labels to present's reference frame\n",
    "        gt_instance = (\n",
    "            self.warper.cumulative_warp_features_reverse(\n",
    "                gt_instance.float().unsqueeze(2),\n",
    "                future_egomotion[:, (self.receptive_field - 1) :],\n",
    "                mode=\"nearest\",\n",
    "                bev_transform=bev_transform,\n",
    "            )\n",
    "            .long()\n",
    "            .contiguous()[:, :, 0]\n",
    "        )\n",
    "        # better solution by abdur but unsure how to make it work with the rest of the code specifcally maxID since it can be diffferent for batches\n",
    "        # temp = torch.arange(MaxID).unsqueeze(0).repeat(B, 1).unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        # gt_masks_ifc_dim  = (temp== Target.unsqueeze(1)).float() \n",
    "        target_list = []\n",
    "        for b in range(batch_size):\n",
    "            gt_list = []\n",
    "            ids = len(gt_instance[b].unique())\n",
    "            for _id in range(ids):\n",
    "                test_bool = torch.where(gt_instance[b] == _id,1.,0.)\n",
    "                gt_list.append(test_bool)\n",
    "\n",
    "            segmentation_labels = torch.stack(gt_list,dim=0)\n",
    "            \n",
    "            #segmentation_labels = torch.stack(gt_batch_instances_list,dim=0)\n",
    "            o_h, o_w = bev_size\n",
    "            l_h, l_w = math.ceil(o_h/mask_stride), math.ceil(o_w/mask_stride)\n",
    "            m_h, m_w = math.ceil(o_h/match_stride), math.ceil(o_w/match_stride)\n",
    "\n",
    "            gt_masks_for_loss  = F.interpolate(segmentation_labels, size=(l_h, l_w), mode=\"bilinear\", align_corners=False)\n",
    "            gt_masks_for_match = F.interpolate(segmentation_labels, size=(m_h, m_w), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "            ids = gt_instance[b].unique() # labels only continous for clip - this is much more of an tracking id as every class is a vehicle anyways # TODO make work with other types of superclasses other then vehicle\n",
    "            target_list.append({\"labels\": ids,\"masks\": gt_masks_for_loss[b], \"match_masks\": gt_masks_for_match[b], \"gt_motion_instance\":gt_instance[b] })\n",
    "        \n",
    "        return target_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=pseud_class()\n",
    "target_list = p.prepare_targets(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['labels', 'masks', 'match_masks', 'gt_motion_instance'])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_list[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "weight tensor should be defined either for all or no classes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [73], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loss_dict \u001b[39m=\u001b[39m criterion(out, target_list)\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [71], line 364\u001b[0m, in \u001b[0;36mSetCriterion.forward\u001b[0;34m(self, outputs, targets)\u001b[0m\n\u001b[1;32m    362\u001b[0m losses \u001b[39m=\u001b[39m {}\n\u001b[1;32m    363\u001b[0m \u001b[39mfor\u001b[39;00m loss \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlosses:\n\u001b[0;32m--> 364\u001b[0m     losses\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_loss(\n\u001b[1;32m    365\u001b[0m         loss, outputs_without_aux, targets, indices, num_masks))\n\u001b[1;32m    367\u001b[0m \u001b[39m# In case of auxiliary losses, we repeat this process with the output of each intermediate layer.\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39maux_outputs\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m outputs:\n",
      "Cell \u001b[0;32mIn [71], line 338\u001b[0m, in \u001b[0;36mSetCriterion.get_loss\u001b[0;34m(self, loss, outputs, targets, indices, num_masks, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m loss_map \u001b[39m=\u001b[39m {\n\u001b[1;32m    333\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_labels,\n\u001b[1;32m    334\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mcardinality\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_cardinality,\n\u001b[1;32m    335\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmasks\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_masks\n\u001b[1;32m    336\u001b[0m }\n\u001b[1;32m    337\u001b[0m \u001b[39massert\u001b[39;00m loss \u001b[39min\u001b[39;00m loss_map, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdo you really want to compute \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m}\u001b[39;00m\u001b[39m loss?\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 338\u001b[0m \u001b[39mreturn\u001b[39;00m loss_map[loss](outputs, targets, indices, num_masks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn [71], line 265\u001b[0m, in \u001b[0;36mSetCriterion.loss_labels\u001b[0;34m(self, outputs, targets, indices, num_masks, log)\u001b[0m\n\u001b[1;32m    261\u001b[0m target_classes \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfull(src_logits\u001b[39m.\u001b[39mshape[:\u001b[39m2\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_classes,\n\u001b[1;32m    262\u001b[0m                             dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint64, device\u001b[39m=\u001b[39msrc_logits\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    263\u001b[0m target_classes[idx] \u001b[39m=\u001b[39m target_classes_o\n\u001b[0;32m--> 265\u001b[0m loss_ce \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mcross_entropy(src_logits\u001b[39m.\u001b[39;49mtranspose(\n\u001b[1;32m    266\u001b[0m     \u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m), target_classes, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mempty_weight)\n\u001b[1;32m    267\u001b[0m losses \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mloss_ce\u001b[39m\u001b[39m'\u001b[39m: loss_ce}\n\u001b[1;32m    269\u001b[0m \u001b[39mif\u001b[39;00m log:\n\u001b[1;32m    270\u001b[0m     \u001b[39m# TODO this should probably be a separate loss, not hacked in this one here\u001b[39;00m\n",
      "File \u001b[0;32m~/ETM_BEV/venv/lib/python3.8/site-packages/torch/nn/functional.py:2846\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2844\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2845\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 2846\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: weight tensor should be defined either for all or no classes"
     ]
    }
   ],
   "source": [
    "loss_dict = criterion(out, target_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 100, 100, 100])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_masks_stacked = torch.stack(out[\"pred_masks\"]).transpose_(1,0)\n",
    "# list of BxQxHxW\n",
    "pred_masks_stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 100, 100])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_instances[0][\"match_masks\"].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_prob = torch.stack(out[\"pred_logits\"]).transpose(1, 0).softmax(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'softmax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out_prob \u001b[39m=\u001b[39m out[\u001b[39m\"\u001b[39;49m\u001b[39mpred_logits\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49msoftmax(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'softmax'"
     ]
    }
   ],
   "source": [
    "out_prob = out[\"pred_logits\"].softmax(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 100, 100, 100])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_mask = torch.stack(out[\"pred_masks\"]).transpose(1, 0)\n",
    "out_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_prob = torch.stack(out[\"pred_logits\"]).transpose(\n",
    "    1, 0).softmax(-1)  # BxNxCxQ\n",
    "out_mask = torch.stack(out[\"pred_masks\"]).transpose(1, 0)\n",
    "B, T, Q, s_h, s_w = out_mask.shape\n",
    "t_h, t_w = gt_instances[0][\"match_masks\"].shape[-2:]\n",
    "\n",
    "if (s_h, s_w) != (t_h, t_w):\n",
    "    out_mask = out_mask.reshape(B, Q*T, s_h, s_w)\n",
    "    out_mask = torch.nn.F.interpolate(out_mask, size=(\n",
    "        t_h, t_w), mode=\"bilinear\", align_corners=False)\n",
    "    out_mask = out_mask.view(B, Q, T, t_h, t_w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1000000) must match the size of tensor b (40000) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [61], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m b_out_mask \u001b[39m=\u001b[39m out_mask[b_i]\n\u001b[1;32m     25\u001b[0m \u001b[39m# Compute the dice coefficient cost between masks\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39m# The 1 is a constant that doesn't change the matching as cost_class, thus omitted.\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m cost_dice \u001b[39m=\u001b[39m dice_coef(\n\u001b[1;32m     28\u001b[0m     b_out_mask, b_tgt_mask\n\u001b[1;32m     29\u001b[0m )\u001b[39m.\u001b[39mto(cost_class)\n\u001b[1;32m     31\u001b[0m \u001b[39m# Final cost matrix\u001b[39;00m\n\u001b[1;32m     32\u001b[0m C \u001b[39m=\u001b[39m cost_dice \u001b[39m*\u001b[39m cost_dice \u001b[39m+\u001b[39m cost_class \u001b[39m*\u001b[39m cost_class\n",
      "Cell \u001b[0;32mIn [1], line 147\u001b[0m, in \u001b[0;36mdice_coef\u001b[0;34m(inputs, targets)\u001b[0m\n\u001b[1;32m    145\u001b[0m inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mflatten(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m    146\u001b[0m targets \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39mflatten(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m--> 147\u001b[0m numerator \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m (inputs \u001b[39m*\u001b[39;49m targets)\u001b[39m.\u001b[39msum(\u001b[39m2\u001b[39m)\n\u001b[1;32m    148\u001b[0m denominator \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39msum(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m+\u001b[39m targets\u001b[39m.\u001b[39msum(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    150\u001b[0m \u001b[39m# NOTE coef doesn't be subtracted to 1 as it is not necessary for computing costs\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (1000000) must match the size of tensor b (40000) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "match_list = []\n",
    "\n",
    "#out_prob = out[\"pred_logits\"].softmax(-1)\n",
    "out_prob = torch.stack(out[\"pred_logits\"]).transpose(1, 0).softmax(-1) # BxNxCxQ \n",
    "out_mask = torch.stack(out[\"pred_masks\"]).transpose(1, 0)\n",
    "B, T, Q, s_h, s_w = out_mask.shape\n",
    "t_h, t_w = gt_instances[0][\"match_masks\"].shape[-2:]\n",
    "\n",
    "if (s_h, s_w) != (t_h, t_w):\n",
    "    out_mask = out_mask.reshape(B, Q*T, s_h, s_w)\n",
    "    out_mask = torch.nn.F.interpolate(out_mask, size=(\n",
    "        t_h, t_w), mode=\"bilinear\", align_corners=False)\n",
    "    out_mask = out_mask.view(B, Q, T, t_h, t_w)\n",
    "\n",
    "indices = []\n",
    "for b_i in range(B):\n",
    "    b_tgt_ids = gt_instances[b_i][\"labels\"]\n",
    "    b_out_prob = out_prob[b_i]\n",
    "\n",
    "    cost_class = b_out_prob[:, b_tgt_ids]\n",
    "\n",
    "    b_tgt_mask = gt_instances[b_i][\"match_masks\"]\n",
    "    b_out_mask = out_mask[b_i]\n",
    "\n",
    "    # Compute the dice coefficient cost between masks\n",
    "    # The 1 is a constant that doesn't change the matching as cost_class, thus omitted.\n",
    "    cost_dice = dice_coef(\n",
    "        b_out_mask, b_tgt_mask\n",
    "    ).to(cost_class)\n",
    "\n",
    "    # Final cost matrix\n",
    "    C = cost_dice * cost_dice + cost_class * cost_class\n",
    "\n",
    "    indices.append(linear_sum_assignment(C.cpu(), maximize=True))\n",
    "\n",
    "matches = [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(\n",
    "    j, dtype=torch.int64)) for i, j in indices]\n",
    "match_list.append(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 100, 100])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_instances[0][\"masks\"].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 100, 100]) torch.Size([4, 100, 100, 100])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1000000) must match the size of tensor b (10000) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [65], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mflatten(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m targets \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39mflatten(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m numerator \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m (inputs \u001b[39m*\u001b[39;49m targets)\u001b[39m.\u001b[39msum(\u001b[39m2\u001b[39m)\n\u001b[1;32m     17\u001b[0m denominator \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39msum(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m+\u001b[39m targets\u001b[39m.\u001b[39msum(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[39m# NOTE coef doesn't be subtracted to 1 as it is not necessary for computing costs\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (1000000) must match the size of tensor b (10000) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "out_prob = torch.stack(out[\"pred_logits\"]).transpose(\n",
    "    1, 0).softmax(-1)  # BxNxCxQ\n",
    "out_mask = torch.stack(out[\"pred_masks\"]).transpose(1, 0)\n",
    "B, T, Q, s_h, s_w = out_mask.shape\n",
    "t_h, t_w = gt_instances[0][\"match_masks\"].shape[-2:]\n",
    "\n",
    "\n",
    "targets = gt_instances[0][\"match_masks\"][0]\n",
    "inputs = out_mask[0]\n",
    "\n",
    "print(targets.shape, inputs.shape)\n",
    "\n",
    "inputs = inputs.sigmoid()\n",
    "inputs = inputs.flatten(1).unsqueeze(1)\n",
    "targets = targets.flatten(1).unsqueeze(0)\n",
    "numerator = 2 * (inputs * targets).sum(2)\n",
    "denominator = inputs.sum(-1) + targets.sum(-1)\n",
    "\n",
    "# NOTE coef doesn't be subtracted to 1 as it is not necessary for computing costs\n",
    "coef = (numerator + 1) / (denominator + 1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49a6cb26e152f15aca94d1d3fa9630fb57fb8fd83a336982cd2ebf9e9635e69c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
