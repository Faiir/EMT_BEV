DETR For Future Prediction


Backbone Check where impact and what output: ----

Relevant how features are transformed 


Relevant hyper parameter: 
    ResNet Channels 
        Affects 
            InputProjections , masked_convs , 

    Mask -> If resize?  just mask = torch.ones((b, h, w), dtype=torch.bool, device=device) for BEV? 
    hidden dim 
    object queries -> 50 / 100 
    dim_feed_forward 
    positional embedding 
    points 
    input size 


Integration plan DETR > BEVerse 
    1. Builder Functions for the different parts 
    2. Builder for output heads 
    3. Get the output sizes
    4. 3D object detection based on DETR 
    5. as small as possible 


Deformable DETR [58] associates the object queries with 2D reference points and proposes deformable cross-attention to perform sparse interaction


Args:
    Coder _> NMSFreeCoder 
    
    DeformableDETR 
        query_decoder -> output muss 10 haben 
        num_queries 
        transformer
            hidden 
            heads 
            num_encoder_layers
            num_decoder_layers
            fim_feedforward
            dropout 
            activation 
            return_intermediate_dec
            num_feature_levels
            dec_n_points 
            enc_n_points
        hidden_dim (transformer)
        num_classes 
        past_query_embed
        tqa
        num_feature_levels
        backbone

        forward: 
            inputs, mask_inputs  -> (GridMask?)


    DeformableDETRSegm 
        MaskHeadSmallConv 
            Dim 
            fpn_dims 
            contrext_dim 

    PositionEmbeddingSine:
        num_pos_feats
        temperature
        normalize 
        scale 

        forward;
            inputs, masks 

        learned:
            num_pos_feats 

    Assigner
        HungarianAssigner3D ? 
        HungarianMatcher  ? <- ORNG


    Backbone:
        resnet18 
            layers=[2,2,2,2]

        return_interm_layers 

        build_backbone(
            build_position_encoding(position_embedding="sine" / "learned")
            return_feature_layers
            backbone("resnet18", )
        )

    Joiner 


TODO #
    Temporal Object Queries  -> https://github.com/megvii-research/MOTR/blob/main/models/qim.py
    Output Heads 
    GetDeformableAttention from MMCV 

    Need for transformation from BEV -> 3D? 
        Reference Points PETR 3 -> 3D coordiantes Deformable DETR only 2 -> could lead to issues 

    PETR
        3D Anchor Points in Multiview space best performance 
        tried BEV space, yielded worse performance 

    If autoregessive
        Warp the hieracical features for generation 


PETR_HEAD FORWARD Input: x.shape =torch.Size([1, 6, 256, 32, 88])
PETR_HEAD FORWARD Input Proj: x.shape =torch.Size([6, 256, 32, 88])
PETR_HEAD FORWARD Mask: torch.Size([1, 6, 32, 88])
PETR_HEAD FORWARD posembed: torch.Size([1, 6, 256, 32, 88])
POS SINE: pos_n.shape =torch.Size([1, 6, 32, 88, 128]) pos_x.shape =torch.Size([1, 6, 32, 88, 128]) pos_y.shape =torch.Size([1, 6, 32, 88, 128]), pos.shape =torch.Size([1, 6, 384, 32, 88])  mask.shape =torch.Size([1, 6, 32, 88])
PETR_HEAD FORWARD reference_points: torch.Size([900, 3])
pos2posemb3d input:  torch.Size([900, 3])
pos_x.shape = torch.Size([900, 128]), pos_y.shape = torch.Size([900, 128]), pos_z.shape = torch.Size([900, 128]), posemb.shape = torch.Size([900, 384])
PETR_HEAD FORWARD query_embeds: torch.Size([900, 256])
PETR_HEAD FORWARD reference_points: torch.Size([1, 900, 3])
memory.shape = torch.Size([16896, 1, 256])
pos_embed.shape = torch.Size([16896, 1, 256])
query_embed.shape = torch.Size([900, 1, 256])
mask.shape = torch.Size([1, 16896])
target.shape = torch.Size([900, 1, 256])
out_dec.shape = torch.Size([1, 900, 1, 256])
out: out_dec.shape = torch.Size([1, 1, 900, 256]) memory.shape = torch.Size([1, 6, 256, 32, 88])
PETR_HEAD FORWARD outs_dec: torch.Size([1, 1, 900, 256])
PETR_HEAD FORWARD reference: torch.Size([1, 900, 3])
PETR_HEAD FORWARD outputs_class: torch.Size([1, 900, 10])
PETR_HEAD FORWARD outs_dec: torch.Size([1, 900, 256])
PETR_HEAD FORWARD tmp: torch.Size([1, 900, 10])
PETR_HEAD FORWARD tmp ref: torch.Size([1, 900, 2]), torch.Size([1, 900, 2])
PETR_HEAD FORWARD tmp ref: torch.Size([1, 900, 1]) , torch.Size([1, 900, 1])
PETR_HEAD FORWARD tmptmp: torch.Size([1, 900, 10])
PETR_HEAD FORWARD all_cls_scores: torch.Size([1, 1, 900, 10])
PETR_HEAD FORWARD all_bbox_preds: torch.Size([1, 1, 900, 10])
PETR_HEAD FORWARD all_bbox_preds: torch.Size([1, 1, 900, 10])
NMS_FREE DECODE SINGLE bbox_preds torch.Size([300, 10])
NMS_FREE DECODE SINGLE FINAL BOX PREDStorch.Size([300, 9])
NMS_FREE DECODE SINGLE MASK torch.Size([300])
NMS_FREE DECODE SINGLE boxes3d torch.Size([300, 9])


Semantic Segmentation 


decoder: hs, inter_references 
    - hs hs.shape = torch.Size([1, 1, 300, 256])
    - init_reference  init_reference.shape = torch.Size([1, 300, 2])  
Encoder -> Memory 
    memory.shape =torch.Size([1, 23650, 256])
Memory -< seg_memory, seg_mask 
    seg_memory.shape =torch.Size([1, 950, 256]) seg_mask.shape =torch.Size([1, 950])
    after Permute/View seg_memory.shape =torch.Size([1, 256, 25, 38]) seg_mask.shape =torch.Size([1, 25, 38])

**Decoder Output**
- hs hs.shape = torch.Size([1, 1, 300, 256]) -> Object Queries After Decoder 
- init_reference  init_reference.shape = torch.Size([1, 300, 2])   > X,Y References 
- enc_outputs_class  enc_outputs_class = None 
- enc_outputs_coord_unact  enc_outputs_coord_unact = None 
- seg_memory  seg_memory.shape = torch.Size([1, 256, 25, 38])  > Encoder Segmentation Memory H/32, W/32
- seg_mask  seg_mask.shape = torch.Size([1, 25, 38]) > Encoder Segmentation Mask 



**Mask Head**
- output_class shape outputs_class.shape = torch.Size([1, 1, 300, 250]),
-  outputs_coord outputs_coord.shape = torch.Size([1, 1, 300, 4])
-  MH AttentionMap Shape weights.shape = torch.Size([1, 300, 8, 25, 38])
-  bbox_mask shape bbox_mask.shape = torch.Size([1, 300, 8, 25, 38])


**ConvBlockt**
- Input MHead SegConv Shape x.shape = torch.Size([1, 256, 25, 38]), 
- bbox_mask bbox_mask.shape = torch.Size([1, 300, 8, 25, 38]), B x OQ x Head x SegMem(H/32 W/32)
- features from image torch.Size([1, 1024, 25, 38])
- features from image torch.Size([1, 512, 25, 38])
- features from image torch.Size([1, 256, 50, 76])
- First Expand: x.shape = torch.Size([300, 264, 25, 38])
- Sec Expand: x.shape = torch.Size([300, 64, 25, 38])
- Third Expand: x.shape = torch.Size([300, 32, 25, 38])
- Fourth Expand: x.shape = torch.Size([300, 16, 50, 76])
- Out MHead SegConv Shape x.shape = torch.Size([300, 1, 50, 76])
- seg_masks shape seg_masks.shape = torch.Size([300, 1, 50, 76])
- outputs_seg_masks shape outputs_seg_masks.shape = torch.Size([1, 300, 50, 76])


C*Nq 
300x256 
    76,800

Wq= W/32 
Nq = Wq Hq = 950

800
400 2
200 3
100 4
50 5 
25 6


Intermediate LayerGetter used to define input projection 
Must be fitting with Backbone Construction -> Setting Introduction

resnet 18 # test with both 
    torch.Size([1, 64, 200, 301])
    torch.Size([1, 128, 100, 151])
    torch.Size([1, 256, 50, 76])
    torch.Size([1, 512, 25, 38])

resnet50 
    torch.Size([1, 256, 200, 301])
    torch.Size([1, 512, 100, 151])
    torch.Size([1, 1024, 50, 76])
    torch.Size([1, 2048, 50, 76])

torch.Size([1, 64, 200, 301])
torch.Size([1, 128, 100, 151])
torch.Size([1, 256, 50, 76])
torch.Size([1, 512, 25, 38])
ModuleList(
  (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (2): Conv2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (3): Conv2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
)





bbox_mask.shape = torch.Size([1, 300, 8, 25, 38])
torch.Size([1, 512, 100, 151])
torch.Size([1, 1024, 50, 76])
torch.Size([1, 2048, 50, 76])
ModuleList(
  (0): Conv2d(2048, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (2): Conv2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
)
Input MHead SegConv Shape x.shape = torch.Size([1, 256, 25, 38]), bbox_mask bbox_mask.shape = torch.Size([1, 300, 8, 25, 38]),
features from image torch.Size([1, 1024, 25, 38])
features from image torch.Size([1, 512, 25, 38])
features from image torch.Size([1, 256, 50, 76])
First Expand: x.shape = torch.Size([300, 264, 25, 38])
Sec Expand: x.shape = torch.Size([300, 64, 25, 38])
Third Expand: x.shape = torch.Size([300, 32, 25, 38])
Fourth Expand: x.shape = torch.Size([300, 16, 50, 76])
Out MHead SegConv Shape x.shape = torch.Size([300, 1, 50, 76])
seg_masks.shape = torch.Size([300, 1, 50, 76])
outputs_seg_masks.shape = torch.Size([1, 300, 50, 76]) <-- 1/16








torch.Size([1, 64, 200, 301])
torch.Size([1, 128, 100, 151])
torch.Size([1, 256, 50, 76])
torch.Size([1, 512, 25, 38])
ModuleList(
  (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
)
in maskhead
Input MHead SegConv Shape x.shape = torch.Size([1, 256, 25, 38]), bbox_mask bbox_mask.shape = torch.Size([1, 300, 8, 25, 38]),
features from image torch.Size([1, 256, 13, 19])
features from image torch.Size([1, 128, 25, 38])
features from image torch.Size([1, 64, 50, 76])
First Expand: x.shape = torch.Size([300, 264, 25, 38])
First cur_fpn: cur_fpn.shape = torch.Size([1, 128, 13, 19])
cur_fpn.size(0) != x.size(0): cur_fpn.shape = torch.Size([300, 128, 13, 19])
Interpolutaion with expan: x.shape = torch.Size([300, 128, 13, 19])
Sec Expand: x.shape = torch.Size([300, 64, 13, 19])
2 adapter2: cur_fpn.shape = torch.Size([1, 64, 25, 38])
cur_fpn.size(0) != x.size(0): cur_fpn.shape = torch.Size([300, 64, 25, 38])
Interpolutaion with expan: x.shape = torch.Size([300, 64, 25, 38])
Third Expand: x.shape = torch.Size([300, 32, 25, 38])
3 adapter3: cur_fpn.shape = torch.Size([1, 32, 50, 76])
cur_fpn.size(0) != x.size(0): cur_fpn.shape = torch.Size([300, 32, 50, 76])
Interpolutaion with expan: x.shape = torch.Size([300, 32, 50, 76])
Fourth Expand: x.shape = torch.Size([300, 16, 50, 76])
Out MHead SegConv Shape x.shape = torch.Size([300, 1, 50, 76])
seg_masks shape seg_masks.shape = torch.Size([300, 1, 50, 76])
outputs_seg_masks shape outputs_seg_masks.shape = torch.Size([1, 300, 50, 76])





ORIGINAL DETR 

inputs: torch.Size([1, 3, 800, 1066])
input_layer1: torch.Size([1, 64, 200, 267])
input_layer2: torch.Size([1, 256, 200, 267])
input_layer3: torch.Size([1, 512, 100, 134])
input_layer4: torch.Size([1, 1024, 50, 67])
output_layer4: torch.Size([1, 2048, 25, 34])
h torch.Size([1, 256, 25, 34])
25 34
pos torch.Size([850, 1, 256])
torch.Size([850, 1, 256])
mem torch.Size([850, 1, 256])
h torch.Size([1, 100, 256])



print(dim, fpn_dims, context_dim, inter_dims)
264 [1024, 512, 256] 256 [264, 128, 64, 32, 16, 4]


src: torch.Size([1, 2048, 25, 38])
src_proj: torch.Size([1, 256, 25, 38])
hs: torch.Size([6, 1, 100, 256]), memory: torch.Size([1, 256, 25, 38])
bbox_mask: torch.Size([1, 100, 8, 25, 38])
Inputs x: torch.Size([1, 256, 25, 38]) bbox_mask: torch.Size([1, 100, 8, 25, 38])
 fpn: torch.Size([1, 1024, 50, 75])
 fpn: torch.Size([1, 512, 100, 150])
 fpn: torch.Size([1, 256, 200, 300])
Inputs x: torch.Size([100, 264, 25, 38])
Before adapter1: torch.Size([100, 128, 25, 38])
after adapter1: torch.Size([1, 128, 50, 75])
size !xsize  adapter1: torch.Size([100, 128, 50, 75])
after interpolation: torch.Size([100, 128, 50, 75])
Before adapter1: torch.Size([100, 64, 50, 75])
after adapter2: torch.Size([1, 64, 100, 150])
size !xsize  adapter2: torch.Size([100, 64, 100, 150])
after interpolation2: torch.Size([100, 64, 100, 150])
Before adapter3: torch.Size([100, 32, 100, 150])
after adapter3: torch.Size([1, 32, 200, 300])
size !xsize  adapter3: torch.Size([100, 32, 200, 300])
after interpolation3: torch.Size([100, 32, 200, 300])
Before out: torch.Size([100, 1, 200, 300])



3x800x1600 <--- 
400x800
200x400
...

1 x F

35x75
70x150
...

200x400  1/4 <- 
 


